{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import hdmpy\n",
    "import statsmodels.api as sm\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sys\n",
    "sys.path.insert(1, \"./hdmpy\")\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c.) \n",
    "\n",
    "class RLasso(BaseEstimator):\n",
    "\n",
    "    def __init__(self, *, post=True):\n",
    "        self.post = post\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.rlasso_ = hdmpy.rlasso(X, y, post=self.post)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array(X) @ np.array(self.rlasso_.est[\"beta\"]).flatten() + np.array(self.rlasso_.est[\"intercept\"])\n",
    "\n",
    "\n",
    "def lasso_model():\n",
    "    return RLasso(post=False)\n",
    "\n",
    "\n",
    "def gen_data(n, d, p, delta, base):\n",
    "    \"\"\"\n",
    "    n: sample size\n",
    "    d: number of covariates\n",
    "    p: probability of treatment\n",
    "    delta: true treatment effect\n",
    "    base: baseline response\n",
    "    \n",
    "    Returns:\n",
    "        y (n-vector): outcome\n",
    "        D (n-vector): treatment indicator\n",
    "        X (n x d): covariates\n",
    "    \"\"\"\n",
    "    D = np.random.binomial(1, p, size=n)\n",
    "    X = np.random.normal(0, 1, size=(n, d))\n",
    "    \n",
    "    # Potential outcomes:\n",
    "    #  Y(0) = base - X[:,0] + noise\n",
    "    #  Y(1) = delta + base + X[:,0] + noise\n",
    "    y0 = base - X[:, 0] + np.random.normal(0, 1, size=n)\n",
    "    y1 = delta + base + X[:, 0] + np.random.normal(0, 1, size=n)\n",
    "    \n",
    "    # Observed outcome:\n",
    "    y = D * y1 + (1 - D) * y0\n",
    "    return y, D, X\n",
    "\n",
    "n_grid = [50, 150, 250, 350, 450, 550, 650, 750]\n",
    "reps = 1000\n",
    "delta_true = 1.0      # given\n",
    "d = 10                # dimensionality of X\n",
    "base = 0.3            # baseline\n",
    "\n",
    "# double selection procedure\n",
    "def run_experiment(procedure, reps, delta_true, d, base, p, title, interactions=False):\n",
    "    coverage_results = []\n",
    "    tau_hats = []\n",
    "    se_hats = []\n",
    "    for n in n_grid:\n",
    "        covered_count = 0\n",
    "        for i in range(reps):\n",
    "            y, D, X = gen_data(n, d, p, delta_true, base)\n",
    "            \n",
    "            tau_hat, se = procedure(y, D, X, interactions)\n",
    "            tau_hats.append(tau_hat)\n",
    "            se_hats.append(se)\n",
    "            # 95% CI:\n",
    "            ci_lower = tau_hat - 1.96 * se\n",
    "            ci_upper = tau_hat + 1.96 * se\n",
    "            \n",
    "            # Check coverage:\n",
    "            if (delta_true >= ci_lower) and (delta_true <= ci_upper):\n",
    "                covered_count += 1\n",
    "        coverage = covered_count / reps\n",
    "        coverage_results.append(coverage)\n",
    "    # remove inf values and nan values (happens du to instability)\n",
    "    tau_hats = [x for x in tau_hats if x != np.inf]\n",
    "    se_hats = [x for x in se_hats if x != np.inf]\n",
    "    tau_hats = [x for x in tau_hats if not np.isnan(x)]\n",
    "    se_hats = [x for x in se_hats if not np.isnan(x)]\n",
    "    \n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.hist(tau_hats, bins=20, alpha=0.5, label=\"tau_hat\")\n",
    "    plt.axvline(x=np.mean(tau_hats), color=\"r\", linestyle=\"--\", label=f\"Mean: {np.mean(tau_hats):.2f}\")\n",
    "    plt.axvline(x=np.mean(tau_hats) + np.std(tau_hats), color=\"g\", linestyle=\"--\", label=f\"Mean + Std: {np.mean(tau_hats) + np.std(tau_hats):.2f}\")\n",
    "    plt.axvline(x=np.mean(tau_hats) - np.std(tau_hats), color=\"g\", linestyle=\"--\", label=f\"Mean - Std: {np.mean(tau_hats) - np.std(tau_hats):.2f}\")\n",
    "    plt.xlabel(\"tau_hat\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    if interactions:\n",
    "        plt.title(f\"tau_hat distribution for {title} with Interactions (p={p})\")\n",
    "    else:\n",
    "        plt.title(f\"tau_hat distribution for {title} without Interactions (p={p})\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.hist(se_hats, bins=20, alpha=0.5, label=\"se_hat\")\n",
    "    plt.axvline(x=np.mean(se_hats), color=\"r\", linestyle=\"--\", label=f\"Mean: {np.mean(se_hats):.2f}\")\n",
    "    plt.axvline(x=np.mean(se_hats) + np.std(se_hats), color=\"g\", linestyle=\"--\", label=f\"Mean + Std: {np.mean(se_hats) + np.std(se_hats):.2f}\")\n",
    "    plt.axvline(x=np.mean(se_hats) - np.std(se_hats), color=\"g\", linestyle=\"--\", label=f\"Mean - Std: {np.mean(se_hats) - np.std(se_hats):.2f}\")\n",
    "    plt.xlabel(\"se_hat\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    if interactions:\n",
    "        plt.title(f\"se_hat distribution for {title} with Interactions (p={p})\")\n",
    "    else:\n",
    "        plt.title(f\"se_hat distribution for {title} without Interactions (p={p})\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.plot(n_grid, coverage_results, marker=\"o\", label=f\"{title}'s coverage\")\n",
    "    plt.axhline(y=0.95, color=\"r\", linestyle=\"--\", label=\"95% target\")\n",
    "    plt.xlabel(\"Sample size n\")\n",
    "    plt.ylabel(\"Coverage\")\n",
    "    if interactions:\n",
    "        plt.title(f\"Coverage of {title} CI vs. Sample Size with Interactions (p={p})\")\n",
    "    else:\n",
    "        plt.title(f\"Coverage of {title} CI vs. Sample Size without Interactions (p={p})\")\n",
    "    plt.show()\n",
    "\n",
    "def double_selection(y, D, X, interactions=False):\n",
    "    scaler = StandardScaler()\n",
    "    X = X - np.mean(X, axis=0)\n",
    "    X = scaler.fit_transform(X)\n",
    "    if interactions:\n",
    "        X = np.hstack((X, X*D[:, None]))\n",
    "    lasso = lasso_model()\n",
    "    lasso.fit(X, y)\n",
    "    none_zero_idx = np.where(lasso.rlasso_.est[\"beta\"] != 0)[0]\n",
    "    lasso.fit(X, D)\n",
    "    none_zero_idx_D = np.where(lasso.rlasso_.est[\"beta\"] != 0)[0]\n",
    "    non_zero_final = np.union1d(none_zero_idx, none_zero_idx_D)\n",
    "    X_final = X[:, non_zero_final]\n",
    "    D = D[:, None]\n",
    "    X_final = np.hstack((D, X_final))\n",
    "    final_reg = sm.OLS(y, sm.add_constant(X_final)).fit(cov_type='HC3')\n",
    "    tau_hat = final_reg.params[1]\n",
    "    se = final_reg.HC3_se[1]\n",
    "    return tau_hat, se\n",
    "\n",
    "def de_sparsified_lasso(y, D, X, interactions=False):\n",
    "    scaler = StandardScaler()\n",
    "    X = X - np.mean(X, axis=0)\n",
    "    X = scaler.fit_transform(X)\n",
    "    if interactions:\n",
    "        X = np.hstack((X, X*D[:, None]))\n",
    "    DX = np.hstack((D[:, None], X))\n",
    "    lasso_one = lasso_model()\n",
    "    lasso_one.fit(DX, y)\n",
    "    DX_untreated = np.hstack((np.zeros((len(D), 1)), X))\n",
    "    y_hat = y - lasso_one.predict(DX_untreated)\n",
    "    epsilon = y - lasso_one.predict(DX)\n",
    "    lasso_two = lasso_model()\n",
    "    lasso_two.fit(X,D)\n",
    "    D_hat = D - lasso_two.predict(X)\n",
    "    tau_hat = np.abs((1.0/np.mean(D*D_hat)) * np.mean(D_hat*y_hat))\n",
    "    n = len(y)\n",
    "    se = np.sqrt((1.0/n) * ((np.sum(epsilon**2 * D_hat**2)-np.sum(epsilon*D_hat)**2) / np.sum(D*D_hat)**2))\n",
    "    return tau_hat, se\n",
    "\n",
    "run_experiment(double_selection, reps, delta_true, d, base, 0.2, \"Double Selection\")\n",
    "run_experiment(de_sparsified_lasso, reps, delta_true, d, base, 0.2, \"De-sparsified Lasso\")\n",
    "\n",
    "run_experiment(double_selection, reps, delta_true, d, base, 0.1, \"Double Selection\")\n",
    "run_experiment(de_sparsified_lasso, reps, delta_true, d, base, 0.1, \"De-sparsified Lasso\")\n",
    "\n",
    "run_experiment(double_selection, reps, delta_true, d, base, 0.2, \"Double Selection\", interactions=True)\n",
    "run_experiment(de_sparsified_lasso, reps, delta_true, d, base, 0.2, \"De-sparsified Lasso\", interactions=True)\n",
    "\n",
    "run_experiment(double_selection, reps, delta_true, d, base, 0.1, \"Double Selection\", interactions=True)\n",
    "run_experiment(de_sparsified_lasso, reps, delta_true, d, base, 0.1, \"De-sparsified Lasso\", interactions=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Across repeated runs, we see that both double selection and desparsified lasso achieve the desired coverage pretty consistently without interactions. Without interactions, the bias appears very low for both methods, though there is some variance with a stdev of around 30% of the true delta. With interactions, for both methods the variance increases and the bias increases a little (e.g. the mean estimate for double selection with p0.1 and interactions is .95, for desparsified lasso it is 1.24). We see large variance with standard errors not increasing by the same amount, which leads to slightly lower coverage. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 2\n",
    "\n",
    "\n",
    "In various regions across Europe, we see a correlation between stork populations and birth rates (Matthews, 2000). This joke “evidence” that storks bring babies is driven by confounders that affect both the number of storks and the number of births.\n",
    "\n",
    "---\n",
    "\n",
    "Causal Graph:\n",
    "  \n",
    "```\n",
    "+-----------------------------------------------------------------------------------------------------+\n",
    "                                |  Rurality/Environment|\n",
    "                                |  Population density  |\n",
    "                                +----------------------+\n",
    "                                   /            \\\n",
    "                                  /              \\\n",
    "                                 / (influences)   \\ (influences)\n",
    "                                /                  \\\n",
    " +------------------+ | Stork Population |     | Birth Rate | +---------------------------------------+\n",
    "```\n",
    "\n",
    "  \n",
    "\n",
    "- **Stork Population (T)**: The so-called “treatment” (erroneously presumed to cause higher birth rates).\n",
    "- **Birth Rate (O)**: The observed outcome (number of babies per population).\n",
    "- **Rurality / Environment / Population density etc (C)**: A set of key *confounders* that influences both stork nesting (availability of farmland, conservation efforts, nesting sites) and human fertility behavior (cultural norms, family size preferences, population density, etc.).\n",
    "\n",
    "Note that it is incredibly unlikely that there is any actual causal relationship between storks and birth rates, hence we leave out the arrow from stork population to birth rate. The correlation is driven by confounders that influence both stork populations and birth rates.\n",
    "\n",
    "---\n",
    "\n",
    "What variables or domain knowledge would you need to collect in a dataset in order to apply an identification by conditioning or an identification via propensity scores approach to the estimation of the causal effect?\n",
    "\n",
    "This case is admittedly slightly ridiculous, since it is clear that storks do not cause babies. Thus \"domain expertise\" is not really applicable here, although we could of course talk to a avian zoologist specialized in storks, that would likely confirm this. However, if we were to take this seriously from a data perspective, we would collect variables, such as:\n",
    "\n",
    "- **Land Use / Habitat:** Farmland, wetlands, regulations.  \n",
    "- **Demographics:** Rural vs. urban splits, age distributions, cultural norms.  \n",
    "- **Socioeconomics:** Income, education, healthcare access.  \n",
    "- **Geography / Climate:** Seasonal patterns, temperature, precipitation.  \n",
    "- **Policy / Infrastructure:** Zoning laws, quality of healthcare.\n",
    "\n",
    "By conditioning on these variables (e.g., in regression or stratification) or using propensity scores (estimating the likelihood of “high stork population” given habitat/socioeconomic features), we could isolate any direct effect of storks on birth rates (which would extremely unlikely to be non-zero if we do it right).\n",
    "\n",
    "---\n",
    "\n",
    "### References\n",
    "\n",
    "1. **Matthews, R. (2000).** “Storks deliver babies (p = 0.008).” *Teaching Statistics*, 2(2), 70–72.  \n",
    "   [https://www.researchgate.net/publication/227763292_Storks_Deliver_Babies_p_0008](stork paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs288",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
