{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1\n",
    "\n",
    "We start with the PLR part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTHONWARNINGS=ignore\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>estimate</th>\n",
       "      <th>stderr</th>\n",
       "      <th>lower</th>\n",
       "      <th>upper</th>\n",
       "      <th>rmse y</th>\n",
       "      <th>rmse D</th>\n",
       "      <th>accuracy D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>double lasso</th>\n",
       "      <td>9035.120004</td>\n",
       "      <td>1295.135748</td>\n",
       "      <td>6496.653938</td>\n",
       "      <td>11573.586070</td>\n",
       "      <td>54254.468883</td>\n",
       "      <td>0.443406</td>\n",
       "      <td>0.688553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lasso/logistic</th>\n",
       "      <td>9092.508157</td>\n",
       "      <td>1304.398170</td>\n",
       "      <td>6535.887743</td>\n",
       "      <td>11649.128571</td>\n",
       "      <td>54254.468883</td>\n",
       "      <td>0.444043</td>\n",
       "      <td>0.687847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random forest</th>\n",
       "      <td>8777.018768</td>\n",
       "      <td>1358.297151</td>\n",
       "      <td>6114.756352</td>\n",
       "      <td>11439.281185</td>\n",
       "      <td>54929.228824</td>\n",
       "      <td>0.444558</td>\n",
       "      <td>0.688452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>decision tree</th>\n",
       "      <td>9236.195678</td>\n",
       "      <td>1440.551643</td>\n",
       "      <td>6412.714457</td>\n",
       "      <td>12059.676898</td>\n",
       "      <td>59427.392172</td>\n",
       "      <td>0.446437</td>\n",
       "      <td>0.688048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>boosted forest</th>\n",
       "      <td>8538.076365</td>\n",
       "      <td>1367.033514</td>\n",
       "      <td>5858.690678</td>\n",
       "      <td>11217.462051</td>\n",
       "      <td>56844.202841</td>\n",
       "      <td>0.443554</td>\n",
       "      <td>0.690469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>automl (semi-cfit)</th>\n",
       "      <td>8884.424774</td>\n",
       "      <td>1306.176253</td>\n",
       "      <td>6324.319318</td>\n",
       "      <td>11444.530230</td>\n",
       "      <td>53908.841872</td>\n",
       "      <td>0.443595</td>\n",
       "      <td>0.690973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stacked (semi-cfit)</th>\n",
       "      <td>8950.138733</td>\n",
       "      <td>1305.541885</td>\n",
       "      <td>6391.276638</td>\n",
       "      <td>11509.000829</td>\n",
       "      <td>54003.262595</td>\n",
       "      <td>0.442970</td>\n",
       "      <td>0.689965</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        estimate       stderr        lower         upper  \\\n",
       "double lasso         9035.120004  1295.135748  6496.653938  11573.586070   \n",
       "lasso/logistic       9092.508157  1304.398170  6535.887743  11649.128571   \n",
       "random forest        8777.018768  1358.297151  6114.756352  11439.281185   \n",
       "decision tree        9236.195678  1440.551643  6412.714457  12059.676898   \n",
       "boosted forest       8538.076365  1367.033514  5858.690678  11217.462051   \n",
       "automl (semi-cfit)   8884.424774  1306.176253  6324.319318  11444.530230   \n",
       "stacked (semi-cfit)  8950.138733  1305.541885  6391.276638  11509.000829   \n",
       "\n",
       "                           rmse y    rmse D  accuracy D  \n",
       "double lasso         54254.468883  0.443406    0.688553  \n",
       "lasso/logistic       54254.468883  0.444043    0.687847  \n",
       "random forest        54929.228824  0.444558    0.688452  \n",
       "decision tree        59427.392172  0.446437    0.688048  \n",
       "boosted forest       56844.202841  0.443554    0.690469  \n",
       "automl (semi-cfit)   53908.841872  0.443595    0.690973  \n",
       "stacked (semi-cfit)  54003.262595  0.442970    0.689965  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%env PYTHONWARNINGS=ignore\n",
    "\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=ConvergenceWarning)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter('ignore')\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\".*did not converge.*\",\n",
    "    category=ConvergenceWarning\n",
    ")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_predict, KFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LassoCV, LinearRegression, LogisticRegressionCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.base import TransformerMixin, BaseEstimator, clone\n",
    "from formulaic import Formula\n",
    "from flaml.automl import AutoML\n",
    "np.random.seed(1234)\n",
    "# set random seed for all other libraries\n",
    "import random\n",
    "random.seed(1234)\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '1234'\n",
    "\n",
    "\n",
    "file = \"https://raw.githubusercontent.com/CausalAIBook/MetricsMLNotebooks/main/data/401k.csv\"\n",
    "data = pd.read_csv(file)\n",
    "y = data['net_tfa'].values\n",
    "D = data['e401'].values\n",
    "D2 = data['p401'].values\n",
    "D3 = data['a401'].values\n",
    "X = data.drop(['e401', 'p401', 'a401', 'tw', 'tfa', 'net_tfa', 'tfa_he',\n",
    "               'hval', 'hmort', 'hequity',\n",
    "               'nifa', 'net_nifa', 'net_n401', 'ira',\n",
    "               'dum91', 'icat', 'ecat', 'zhat',\n",
    "               'i1', 'i2', 'i3', 'i4', 'i5', 'i6', 'i7',\n",
    "               'a1', 'a2', 'a3', 'a4', 'a5'], axis=1)\n",
    "\n",
    "class FormulaTransformer(TransformerMixin, BaseEstimator):\n",
    "\n",
    "    def __init__(self, formula, array=False):\n",
    "        self.formula = formula\n",
    "        self.array = array\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        df = Formula(self.formula).get_model_matrix(X)\n",
    "        if self.array:\n",
    "            return df.values\n",
    "        return df\n",
    "transformer = FormulaTransformer(\"0 + poly(age, degree=6, raw=True) + poly(inc, degree=8, raw=True) \"\n",
    "                                 \"+ poly(educ, degree=4, raw=True) + poly(fsize, degree=2, raw=True) \"\n",
    "                                 \"+ male + marr + twoearn + db + pira + hown\", array=True)\n",
    "\n",
    "def dml(X, D, y, modely, modeld, *, nfolds, classifier=False):\n",
    "    '''\n",
    "    DML for the Partially Linear Model setting with cross-fitting\n",
    "\n",
    "    Input\n",
    "    -----\n",
    "    X: the controls\n",
    "    D: the treatment\n",
    "    y: the outcome\n",
    "    modely: the ML model for predicting the outcome y\n",
    "    modeld: the ML model for predicting the treatment D\n",
    "    nfolds: the number of folds in cross-fitting\n",
    "    classifier: bool, whether the modeld is a classifier or a regressor\n",
    "\n",
    "    Output\n",
    "    ------\n",
    "    point: the point estimate of the treatment effect of D on y\n",
    "    stderr: the standard error of the treatment effect\n",
    "    yhat: the cross-fitted predictions for the outcome y\n",
    "    Dhat: the cross-fitted predictions for the treatment D\n",
    "    resy: the outcome residuals\n",
    "    resD: the treatment residuals\n",
    "    epsilon: the final residual-on-residual OLS regression residual\n",
    "    '''\n",
    "    cv = KFold(n_splits=nfolds, shuffle=True, random_state=123)  # shuffled k-folds\n",
    "    yhat = cross_val_predict(modely, X, y, cv=cv, n_jobs=-1)  # out-of-fold predictions for y\n",
    "    # out-of-fold predictions for D\n",
    "    # use predict or predict_proba dependent on classifier or regressor for D\n",
    "    if classifier:\n",
    "        Dhat = cross_val_predict(modeld, X, D, cv=cv, method='predict_proba', n_jobs=-1)[:, 1]\n",
    "    else:\n",
    "        Dhat = cross_val_predict(modeld, X, D, cv=cv, n_jobs=-1)\n",
    "    # calculate outcome and treatment residuals\n",
    "    resy = y - yhat\n",
    "    resD = D - Dhat\n",
    "\n",
    "    # final stage ols based point estimate and standard error\n",
    "    point = np.mean(resy * resD) / np.mean(resD**2)\n",
    "    epsilon = resy - point * resD\n",
    "    var = np.mean(epsilon**2 * resD**2) / np.mean(resD**2)**2\n",
    "    stderr = np.sqrt(var / X.shape[0])\n",
    "\n",
    "    return point, stderr, yhat, Dhat, resy, resD, epsilon\n",
    "\n",
    "def summary(point, stderr, yhat, Dhat, resy, resD, epsilon, X, D, y, *, name):\n",
    "    '''\n",
    "    Convenience summary function that takes the results of the DML function\n",
    "    and summarizes several estimation quantities and performance metrics.\n",
    "    '''\n",
    "    return pd.DataFrame({'estimate': point,  # point estimate\n",
    "                         'stderr': stderr,  # standard error\n",
    "                         'lower': point - 1.96 * stderr,  # lower end of 95% confidence interval\n",
    "                         'upper': point + 1.96 * stderr,  # upper end of 95% confidence interval\n",
    "                         'rmse y': np.sqrt(np.mean(resy**2)),  # RMSE of model that predicts outcome y\n",
    "                         'rmse D': np.sqrt(np.mean(resD**2)),  # RMSE of model that predicts treatment D\n",
    "                         'accuracy D': np.mean(np.abs(resD) < .5),  # binary classification accuracy of model for D\n",
    "                         }, index=[name])\n",
    "# double lasso with cross-fitting\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "lassoy = make_pipeline(transformer, StandardScaler(), LassoCV(cv=cv))\n",
    "lassod = make_pipeline(transformer, StandardScaler(), LassoCV(cv=cv))\n",
    "result = dml(X, D, y, lassoy, lassod, nfolds=5)\n",
    "table = summary(*result, X, D, y, name='double lasso')\n",
    "\n",
    "# penalized logreg for D (default is l2 penalty)\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "lassoy = make_pipeline(transformer, StandardScaler(), LassoCV(cv=cv))\n",
    "lgrd = make_pipeline(transformer, StandardScaler(), LogisticRegressionCV(cv=cv))\n",
    "result = dml(X, D, y, lassoy, lgrd, nfolds=5, classifier=True)\n",
    "table = pd.concat([table, summary(*result, X, D, y, name='lasso/logistic')])\n",
    "\n",
    "# random forest\n",
    "rfy = make_pipeline(transformer, RandomForestRegressor(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001))\n",
    "rfd = make_pipeline(transformer, RandomForestClassifier(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001))\n",
    "result = dml(X, D, y, rfy, rfd, nfolds=5, classifier=True)\n",
    "table = pd.concat([table, summary(*result, X, D, y, name='random forest')])\n",
    "\n",
    "# decision tree\n",
    "dtry = make_pipeline(transformer, DecisionTreeRegressor(min_samples_leaf=10, ccp_alpha=.001))\n",
    "dtrd = make_pipeline(transformer, DecisionTreeClassifier(min_samples_leaf=10, ccp_alpha=.001))\n",
    "result = dml(X, D, y, dtry, dtrd, nfolds=5, classifier=True)\n",
    "table = pd.concat([table, summary(*result, X, D, y, name='decision tree')])\n",
    "\n",
    "# boosted trees\n",
    "gbfy = make_pipeline(transformer, GradientBoostingRegressor(max_depth=2, n_iter_no_change=5))\n",
    "gbfd = make_pipeline(transformer, GradientBoostingClassifier(max_depth=2, n_iter_no_change=5))\n",
    "result = dml(X, D, y, gbfy, gbfd, nfolds=5, classifier=True)\n",
    "table = pd.concat([table, summary(*result, X, D, y, name='boosted forest')])\n",
    "\n",
    "# semi cross fitting: To avoid the computational cost of performing model selection within each fold (assuming that we don't select among an exponential set of hyperparameters/models in the number of samples), it is ok to perform model selection using all the data and then perform cross-fitting with the selected model\n",
    "flamly = make_pipeline(transformer, AutoML(time_budget=100, task='regression', early_stop=True,\n",
    "                                           eval_method='cv', n_splits=3, metric='r2', verbose=0))\n",
    "flamld = make_pipeline(transformer, AutoML(time_budget=100, task='classification', early_stop=True,\n",
    "                                           eval_method='cv', n_splits=3, metric='r2', verbose=0))\n",
    "flamly.fit(X, y)\n",
    "besty = make_pipeline(transformer, clone(flamly[-1].best_model_for_estimator(flamly[-1].best_estimator)))\n",
    "flamld.fit(X, D)\n",
    "bestd = make_pipeline(transformer, clone(flamld[-1].best_model_for_estimator(flamld[-1].best_estimator)))\n",
    "result = dml(X, D, y, besty, bestd, nfolds=5, classifier=True)\n",
    "table = pd.concat([table, summary(*result, X, D, y, name='automl (semi-cfit)')])\n",
    "\n",
    "# semi cross fitting with stacking\n",
    "def dml_dirty(X, D, y, modely_list, modeld_list, *,\n",
    "              stacker=LinearRegression(), nfolds, classifier=False):\n",
    "    '''\n",
    "    DML for the Partially Linear Model setting with semi-cross-fitting\n",
    "\n",
    "    Input\n",
    "    -----\n",
    "    X: the controls\n",
    "    D: the treatment\n",
    "    y: the outcome\n",
    "    modely: the ML model for predicting the outcome y\n",
    "    modeld: the ML model for predicting the treatment D\n",
    "    stacker: model used to aggregate predictions of each of the base models\n",
    "    nfolds: the number of folds in cross-fitting\n",
    "    classifier: bool, whether the modeld is a classifier or a regressor\n",
    "\n",
    "    Output\n",
    "    ------\n",
    "    point: the point estimate of the treatment effect of D on y\n",
    "    stderr: the standard error of the treatment effect\n",
    "    yhat: the cross-fitted predictions for the outcome y\n",
    "    Dhat: the cross-fitted predictions for the treatment D\n",
    "    resy: the outcome residuals\n",
    "    resD: the treatment residuals\n",
    "    epsilon: the final residual-on-residual OLS regression residual\n",
    "    '''\n",
    "    # construct out-of-fold predictions for each model\n",
    "    cv = KFold(n_splits=nfolds, shuffle=True, random_state=123)\n",
    "    yhats = np.array([cross_val_predict(modely, X, y, cv=cv, n_jobs=-1) for modely in modely_list]).T\n",
    "    if classifier:\n",
    "        Dhats = np.array([cross_val_predict(modeld, X, D, cv=cv, method='predict_proba', n_jobs=-1)[:, 1]\n",
    "                         for modeld in modeld_list]).T\n",
    "    else:\n",
    "        Dhats = np.array([cross_val_predict(modeld, X, D, cv=cv, n_jobs=-1) for modeld in modeld_list]).T\n",
    "    # calculate stacked residuals by finding optimal coefficients\n",
    "    # and weigthing out-of-sample predictions by these coefficients\n",
    "    yhat = stacker.fit(yhats, y).predict(yhats)\n",
    "    Dhat = stacker.fit(Dhats, D).predict(Dhats)\n",
    "    resy = y - yhat\n",
    "    resD = D - Dhat\n",
    "    # go with the stacked residuals\n",
    "    point = np.mean(resy * resD) / np.mean(resD**2)\n",
    "    epsilon = resy - point * resD\n",
    "    var = np.mean(epsilon**2 * resD**2) / np.mean(resD**2)**2\n",
    "    stderr = np.sqrt(var / X.shape[0])\n",
    "    return point, stderr, yhat, Dhat, resy, resD, epsilon\n",
    "\n",
    "result = dml_dirty(X, D, y, [lassoy, rfy, dtry, gbfy], [lgrd, rfd, dtrd, gbfd],\n",
    "                   nfolds=5, classifier=True)\n",
    "table = pd.concat([table, summary(*result, X, D, y, name='stacked (semi-cfit)')])\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Bottom 25% Income Sample ===\n",
      "                                  estimate       stderr        lower  \\\n",
      "bottom25% double lasso         3769.763582  1092.945664  1627.590080   \n",
      "bottom25% lasso/logistic       3803.759038  1072.578658  1701.504869   \n",
      "bottom25% random forest        4423.113011  1087.472959  2291.666012   \n",
      "bottom25% decision tree        3497.486250  1034.062103  1470.724528   \n",
      "bottom25% boosted forest       3783.151554  1102.129138  1622.978443   \n",
      "bottom25% automl (semi-cfit)   3818.527486  1094.432778  1673.439240   \n",
      "bottom25% stacked (semi-cfit)  4015.379671  1093.743219  1871.642961   \n",
      "\n",
      "                                     upper        rmse y    rmse D  accuracy D  \n",
      "bottom25% double lasso         5911.937083  13400.361810  0.343801    0.846433  \n",
      "bottom25% lasso/logistic       5906.013207  13400.361810  0.354799    0.844015  \n",
      "bottom25% random forest        6554.560010  13506.389373  0.345322    0.844418  \n",
      "bottom25% decision tree        5524.247972  14728.635257  0.380522    0.804917  \n",
      "bottom25% boosted forest       5943.324665  13471.186858  0.344859    0.843611  \n",
      "bottom25% automl (semi-cfit)   5963.615731  13480.512942  0.351340    0.835953  \n",
      "bottom25% stacked (semi-cfit)  6159.116380  13308.411978  0.342433    0.847239  \n",
      "\n",
      "=== Top 25% Income Sample ===\n",
      "                                estimate       stderr         lower  \\\n",
      "top25% double lasso         17505.836955  3902.624431   9856.693070   \n",
      "top25% lasso/logistic       18204.194590  3867.405524  10624.079763   \n",
      "top25% random forest        17171.343190  3992.190999   9346.648832   \n",
      "top25% decision tree        15270.539604  3774.945121   7871.647166   \n",
      "top25% boosted forest       17232.066048  3950.929491   9488.244246   \n",
      "top25% automl (semi-cfit)   17540.829818  3838.965253  10016.457922   \n",
      "top25% stacked (semi-cfit)  17998.039136  3830.863647  10489.546387   \n",
      "\n",
      "                                   upper         rmse y    rmse D  accuracy D  \n",
      "top25% double lasso         25154.980841   91393.039963  0.483189    0.601049  \n",
      "top25% lasso/logistic       25784.309418   91393.039963  0.482708    0.601049  \n",
      "top25% random forest        24996.037548   93884.561559  0.484189    0.610730  \n",
      "top25% decision tree        22669.432041  101948.859766  0.542915    0.538927  \n",
      "top25% boosted forest       24975.887849   94094.648880  0.483780    0.602259  \n",
      "top25% automl (semi-cfit)   25065.201715   95109.874876  0.484254    0.601856  \n",
      "top25% stacked (semi-cfit)  25506.531884   91322.787397  0.481654    0.610730  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_bottom = data.loc[data['inc'] <= data['inc'].quantile(0.25)].copy()\n",
    "data_top = data.loc[data['inc'] >= data['inc'].quantile(0.75)].copy()\n",
    "\n",
    "# Define helper function to create X, D, y from a subset\n",
    "def extract_XDy(df):\n",
    "    \"\"\"Given a subset of the 401k data, produce X, D, and y \n",
    "    consistent with the main analysis.\"\"\"\n",
    "    y_ = df['net_tfa'].values\n",
    "    D_ = df['e401'].values\n",
    "    X_ = df.drop([\n",
    "        'e401', 'p401', 'a401', 'tw', 'tfa', 'net_tfa', 'tfa_he',\n",
    "        'hval', 'hmort', 'hequity', 'nifa', 'net_nifa', 'net_n401',\n",
    "        'ira', 'dum91', 'icat', 'ecat', 'zhat', 'i1', 'i2', 'i3',\n",
    "        'i4', 'i5', 'i6', 'i7', 'a1', 'a2', 'a3', 'a4', 'a5'\n",
    "    ], axis=1)\n",
    "    return X_, D_, y_\n",
    "\n",
    "X_bottom, D_bottom, y_bottom = extract_XDy(data_bottom)\n",
    "X_top, D_top, y_top = extract_XDy(data_top)\n",
    "\n",
    "\n",
    "# Create a helper function that runs all models and returns a summary table\n",
    "def run_all_estimators(X, D, y, name_prefix=''):\n",
    "    \"\"\"Runs the pipeline of estimators and returns a summary results table.\"\"\"\n",
    "    table_local = []\n",
    "\n",
    "    # cross-validation setup\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "    \n",
    "    # 1) double lasso\n",
    "    lassoy = make_pipeline(transformer, StandardScaler(), LassoCV(cv=cv))\n",
    "    lassod = make_pipeline(transformer, StandardScaler(), LassoCV(cv=cv))\n",
    "    result = dml(X, D, y, lassoy, lassod, nfolds=5)\n",
    "    table_local.append(summary(*result, X, D, y, name=f'{name_prefix} double lasso'))\n",
    "\n",
    "    # 2) lasso/logistic\n",
    "    lassoy = make_pipeline(transformer, StandardScaler(), LassoCV(cv=cv))\n",
    "    lgrd = make_pipeline(transformer, StandardScaler(), LogisticRegressionCV(cv=cv))\n",
    "    result = dml(X, D, y, lassoy, lgrd, nfolds=5, classifier=True)\n",
    "    table_local.append(summary(*result, X, D, y, name=f'{name_prefix} lasso/logistic'))\n",
    "\n",
    "    # 3) random forest\n",
    "    rfy = make_pipeline(transformer,\n",
    "                        RandomForestRegressor(n_estimators=100,\n",
    "                                              min_samples_leaf=10,\n",
    "                                              ccp_alpha=0.001))\n",
    "    rfd = make_pipeline(transformer,\n",
    "                        RandomForestClassifier(n_estimators=100,\n",
    "                                               min_samples_leaf=10,\n",
    "                                               ccp_alpha=0.001))\n",
    "    result = dml(X, D, y, rfy, rfd, nfolds=5, classifier=True)\n",
    "    table_local.append(summary(*result, X, D, y, name=f'{name_prefix} random forest'))\n",
    "\n",
    "    # 4) decision tree\n",
    "    dtry = make_pipeline(transformer,\n",
    "                         DecisionTreeRegressor(min_samples_leaf=10,\n",
    "                                               ccp_alpha=0.001))\n",
    "    dtrd = make_pipeline(transformer,\n",
    "                         DecisionTreeClassifier(min_samples_leaf=10,\n",
    "                                                ccp_alpha=0.001))\n",
    "    result = dml(X, D, y, dtry, dtrd, nfolds=5, classifier=True)\n",
    "    table_local.append(summary(*result, X, D, y, name=f'{name_prefix} decision tree'))\n",
    "\n",
    "    # 5) boosted trees\n",
    "    gbfy = make_pipeline(transformer,\n",
    "                         GradientBoostingRegressor(max_depth=2,\n",
    "                                                   n_iter_no_change=5))\n",
    "    gbfd = make_pipeline(transformer,\n",
    "                         GradientBoostingClassifier(max_depth=2,\n",
    "                                                    n_iter_no_change=5))\n",
    "    result = dml(X, D, y, gbfy, gbfd, nfolds=5, classifier=True)\n",
    "    table_local.append(summary(*result, X, D, y, name=f'{name_prefix} boosted forest'))\n",
    "\n",
    "    # 6) automl (semi cross-fitting)\n",
    "    flamly = make_pipeline(transformer,\n",
    "                           AutoML(time_budget=60,  # reduce if desired\n",
    "                                  task='regression',\n",
    "                                  early_stop=True,\n",
    "                                  eval_method='cv',\n",
    "                                  n_splits=3,\n",
    "                                  metric='r2',\n",
    "                                  verbose=0))\n",
    "    flamld = make_pipeline(transformer,\n",
    "                           AutoML(time_budget=60,\n",
    "                                  task='classification',\n",
    "                                  early_stop=True,\n",
    "                                  eval_method='cv',\n",
    "                                  n_splits=3,\n",
    "                                  metric='r2',\n",
    "                                  verbose=0))\n",
    "    # Fit once on entire data\n",
    "    flamly.fit(X, y)\n",
    "    besty_model = flamly[-1].best_model_for_estimator(flamly[-1].best_estimator)\n",
    "    besty = make_pipeline(transformer, clone(besty_model))\n",
    "\n",
    "    flamld.fit(X, D)\n",
    "    bestd_model = flamld[-1].best_model_for_estimator(flamld[-1].best_estimator)\n",
    "    bestd = make_pipeline(transformer, clone(bestd_model))\n",
    "\n",
    "    result = dml(X, D, y, besty, bestd, nfolds=5, classifier=True)\n",
    "    table_local.append(summary(*result, X, D, y, name=f'{name_prefix} automl (semi-cfit)'))\n",
    "\n",
    "    # 7) stacked (semi-cfit)\n",
    "    # Re-use the same base models we created, but put them into lists:\n",
    "    # - Notice we must re-create them fresh so they are unfitted before cross_val_predict\n",
    "    #   or cross_val_predict won't do what we expect.\n",
    "    lassoy_ = make_pipeline(transformer, StandardScaler(), LassoCV(cv=cv))\n",
    "    lgrd_ = make_pipeline(transformer, StandardScaler(), LogisticRegressionCV(cv=cv))\n",
    "    rfy_ = make_pipeline(transformer,\n",
    "                         RandomForestRegressor(n_estimators=100,\n",
    "                                               min_samples_leaf=10,\n",
    "                                               ccp_alpha=0.001))\n",
    "    rfd_ = make_pipeline(transformer,\n",
    "                         RandomForestClassifier(n_estimators=100,\n",
    "                                                min_samples_leaf=10,\n",
    "                                                ccp_alpha=0.001))\n",
    "    dtry_ = make_pipeline(transformer,\n",
    "                          DecisionTreeRegressor(min_samples_leaf=10,\n",
    "                                                ccp_alpha=0.001))\n",
    "    dtrd_ = make_pipeline(transformer,\n",
    "                          DecisionTreeClassifier(min_samples_leaf=10,\n",
    "                                                 ccp_alpha=0.001))\n",
    "    gbfy_ = make_pipeline(transformer,\n",
    "                          GradientBoostingRegressor(max_depth=2,\n",
    "                                                    n_iter_no_change=5))\n",
    "    gbfd_ = make_pipeline(transformer,\n",
    "                          GradientBoostingClassifier(max_depth=2,\n",
    "                                                     n_iter_no_change=5))\n",
    "\n",
    "    modely_list = [lassoy_, rfy_, dtry_, gbfy_]\n",
    "    modeld_list = [lgrd_, rfd_, dtrd_, gbfd_]\n",
    "\n",
    "    result = dml_dirty(X, D, y, modely_list, modeld_list,\n",
    "                       stacker=LinearRegression(),\n",
    "                       nfolds=5, classifier=True)\n",
    "    table_local.append(summary(*result, X, D, y, name=f'{name_prefix} stacked (semi-cfit)'))\n",
    "\n",
    "    # Concatenate all results\n",
    "    return pd.concat(table_local)\n",
    "\n",
    "\n",
    "table_bottom = run_all_estimators(X_bottom, D_bottom, y_bottom, name_prefix='bottom25%')\n",
    "table_top = run_all_estimators(X_top, D_top, y_top, name_prefix='top25%')\n",
    "\n",
    "print(\"=== Bottom 25% Income Sample ===\")\n",
    "print(table_bottom)\n",
    "\n",
    "print(\"\\n=== Top 25% Income Sample ===\")\n",
    "print(table_top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the PLR setting, there indeed seems to be heterogeneity in the treatment with respect to income, with the bottom 25% of earners seeing estimates around 3.8k and the top 25% seeing estimates of around 17.5k. The different machine learning models are broadly consistent across all three income groups.  \n",
    "  \n",
    "Next we more to the IRM part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>estimate</th>\n",
       "      <th>stderr</th>\n",
       "      <th>lower</th>\n",
       "      <th>upper</th>\n",
       "      <th>rmse y</th>\n",
       "      <th>rmse D</th>\n",
       "      <th>accuracy D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lasso/logistic</th>\n",
       "      <td>7726.585781</td>\n",
       "      <td>1159.322663</td>\n",
       "      <td>5454.313361</td>\n",
       "      <td>9998.858202</td>\n",
       "      <td>54060.702446</td>\n",
       "      <td>0.444041</td>\n",
       "      <td>0.687948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random forest</th>\n",
       "      <td>7716.528104</td>\n",
       "      <td>1154.020996</td>\n",
       "      <td>5454.646952</td>\n",
       "      <td>9978.409256</td>\n",
       "      <td>55696.898087</td>\n",
       "      <td>0.444532</td>\n",
       "      <td>0.688754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>decision tree</th>\n",
       "      <td>7846.333657</td>\n",
       "      <td>1255.457709</td>\n",
       "      <td>5385.636547</td>\n",
       "      <td>10307.030767</td>\n",
       "      <td>60491.245996</td>\n",
       "      <td>0.446437</td>\n",
       "      <td>0.688048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>boosted forest</th>\n",
       "      <td>8531.153341</td>\n",
       "      <td>1141.431078</td>\n",
       "      <td>6293.948429</td>\n",
       "      <td>10768.358253</td>\n",
       "      <td>55795.871457</td>\n",
       "      <td>0.443315</td>\n",
       "      <td>0.690671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>automl (semi-cfit)</th>\n",
       "      <td>8129.709678</td>\n",
       "      <td>1132.995563</td>\n",
       "      <td>5909.038374</td>\n",
       "      <td>10350.380982</td>\n",
       "      <td>55338.022903</td>\n",
       "      <td>0.443595</td>\n",
       "      <td>0.690973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stacked (semi-cfit)</th>\n",
       "      <td>7989.236728</td>\n",
       "      <td>1131.062039</td>\n",
       "      <td>5772.355130</td>\n",
       "      <td>10206.118325</td>\n",
       "      <td>53689.794609</td>\n",
       "      <td>0.442923</td>\n",
       "      <td>0.689965</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        estimate       stderr        lower         upper  \\\n",
       "lasso/logistic       7726.585781  1159.322663  5454.313361   9998.858202   \n",
       "random forest        7716.528104  1154.020996  5454.646952   9978.409256   \n",
       "decision tree        7846.333657  1255.457709  5385.636547  10307.030767   \n",
       "boosted forest       8531.153341  1141.431078  6293.948429  10768.358253   \n",
       "automl (semi-cfit)   8129.709678  1132.995563  5909.038374  10350.380982   \n",
       "stacked (semi-cfit)  7989.236728  1131.062039  5772.355130  10206.118325   \n",
       "\n",
       "                           rmse y    rmse D  accuracy D  \n",
       "lasso/logistic       54060.702446  0.444041    0.687948  \n",
       "random forest        55696.898087  0.444532    0.688754  \n",
       "decision tree        60491.245996  0.446437    0.688048  \n",
       "boosted forest       55795.871457  0.443315    0.690671  \n",
       "automl (semi-cfit)   55338.022903  0.443595    0.690973  \n",
       "stacked (semi-cfit)  53689.794609  0.442923    0.689965  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dr(X, D, y, modely0, modely1, modeld, *, trimming=0.01, nfolds):\n",
    "    '''\n",
    "    DML for the Interactive Regression Model setting (Doubly Robust Learning)\n",
    "    with cross-fitting\n",
    "\n",
    "    Input\n",
    "    -----\n",
    "    X: the controls\n",
    "    D: the treatment\n",
    "    y: the outcome\n",
    "    modely0: the ML model for predicting the outcome y in the control population\n",
    "    modely1: the ML model for predicting the outcome y in the treated population\n",
    "    modeld: the ML model for predicting the treatment D\n",
    "    trimming: threshold below which to trim propensities\n",
    "    nfolds: the number of folds in cross-fitting\n",
    "\n",
    "    Output\n",
    "    ------\n",
    "    point: the point estimate of the treatment effect of D on y\n",
    "    stderr: the standard error of the treatment effect\n",
    "    yhat: the cross-fitted predictions for the outcome y\n",
    "    Dhat: the cross-fitted predictions for the outcome D\n",
    "    resy: the outcome residuals\n",
    "    resD: the treatment residuals\n",
    "    drhat: the doubly robust quantity for each sample\n",
    "    '''\n",
    "    cv = KFold(n_splits=nfolds, shuffle=True, random_state=123)\n",
    "    yhat0, yhat1 = np.zeros(y.shape), np.zeros(y.shape)\n",
    "    # we will fit a model E[Y| D, X] by fitting a separate model for D==0\n",
    "    # and a separate model for D==1.\n",
    "    for train, test in cv.split(X, y):\n",
    "        # train a model on training data that received treatment zero and predict on all data in test set\n",
    "        yhat0[test] = clone(modely0).fit(X.iloc[train][D[train] == 0], y[train][D[train] == 0]).predict(X.iloc[test])\n",
    "        # train a model on training data that received treatment one and predict on all data in test set\n",
    "        yhat1[test] = clone(modely1).fit(X.iloc[train][D[train] == 1], y[train][D[train] == 1]).predict(X.iloc[test])\n",
    "    # prediction for observed treatment\n",
    "    yhat = yhat0 * (1 - D) + yhat1 * D\n",
    "    # propensity scores\n",
    "    Dhat = cross_val_predict(modeld, X, D, cv=cv, method='predict_proba', n_jobs=-1)[:, 1]\n",
    "    Dhat = np.clip(Dhat, trimming, 1 - trimming)\n",
    "    # doubly robust quantity for every sample\n",
    "    drhat = yhat1 - yhat0 + (y - yhat) * (D / Dhat - (1 - D) / (1 - Dhat))\n",
    "    point = np.mean(drhat)\n",
    "    var = np.var(drhat)\n",
    "    stderr = np.sqrt(var / X.shape[0])\n",
    "    return point, stderr, yhat, Dhat, y - yhat, D - Dhat, drhat\n",
    "\n",
    "v = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "lassoytest = make_pipeline(transformer, StandardScaler(), LassoCV(cv=cv))\n",
    "lgrdtest = make_pipeline(transformer, StandardScaler(), LogisticRegressionCV(cv=cv))\n",
    "result = dr(X, D, y, lassoytest, lassoytest, lgrdtest, nfolds=5)\n",
    "seed_estimates = summary(*result, X, D, y, name='lasso/logistic')\n",
    "\n",
    "for i in range(9):\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    lassoytest = make_pipeline(transformer, StandardScaler(), LassoCV(cv=cv))\n",
    "    lgrdtest = make_pipeline(transformer, StandardScaler(), LogisticRegressionCV(cv=cv))\n",
    "    result = dr(X, D, y, lassoytest, lassoytest, lgrdtest, nfolds=5)\n",
    "    seed_estimates = pd.concat([seed_estimates, summary(*result, X, D, y, name='lasso/logistic')])\n",
    "\n",
    "med_theta = np.median(seed_estimates.values[:, 0])\n",
    "se_med = np.sqrt(np.median((seed_estimates.values[:, 1])**2 + (seed_estimates.values[:, 0] - med_theta)**2))\n",
    "tabledr = pd.DataFrame({'estimate': med_theta,\n",
    "                        'stderr': se_med,\n",
    "                        'lower': med_theta - 1.96 * se_med,\n",
    "                        'upper': med_theta + 1.96 * se_med,\n",
    "                        'rmse y': np.median(seed_estimates.values[:, 4]),\n",
    "                        'rmse D': np.median(seed_estimates.values[:, 5]),\n",
    "                        'accuracy D': np.median(seed_estimates.values[:, 6]),\n",
    "                        }, index=['lasso/logistic'])\n",
    "\n",
    "rfy = make_pipeline(transformer, RandomForestRegressor(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001))\n",
    "rfd = make_pipeline(transformer, RandomForestClassifier(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001))\n",
    "result = dr(X, D, y, rfy, rfy, rfd, nfolds=5)\n",
    "tabledr = pd.concat([tabledr, summary(*result, X, D, y, name='random forest')])\n",
    "dtry = make_pipeline(transformer, DecisionTreeRegressor(min_samples_leaf=10, ccp_alpha=.001))\n",
    "dtrd = make_pipeline(transformer, DecisionTreeClassifier(min_samples_leaf=10, ccp_alpha=.001))\n",
    "result = dr(X, D, y, dtry, dtry, dtrd, nfolds=5)\n",
    "tabledr = pd.concat([tabledr, summary(*result, X, D, y, name='decision tree')])\n",
    "gbfy = make_pipeline(transformer, GradientBoostingRegressor(max_depth=2, n_iter_no_change=5))\n",
    "gbfd = make_pipeline(transformer, GradientBoostingClassifier(max_depth=2, n_iter_no_change=5))\n",
    "result = dr(X, D, y, gbfy, gbfy, gbfd, nfolds=5)\n",
    "tabledr = pd.concat([tabledr, summary(*result, X, D, y, name='boosted forest')])\n",
    "# semi cross-fitting\n",
    "flamly0 = make_pipeline(transformer, AutoML(time_budget=60, task='regression', early_stop=True,\n",
    "                                            eval_method='cv', n_splits=3, metric='r2', verbose=0))\n",
    "flamly1 = make_pipeline(transformer, AutoML(time_budget=60, task='regression', early_stop=True,\n",
    "                                            eval_method='cv', n_splits=3, metric='r2', verbose=0))\n",
    "flamld = make_pipeline(transformer, AutoML(time_budget=60, task='classification', early_stop=True,\n",
    "                                           eval_method='cv', n_splits=3, metric='r2', verbose=0))\n",
    "\n",
    "flamly0.fit(X[D == 0], y[D == 0])\n",
    "besty0 = make_pipeline(transformer, clone(flamly0[-1].best_model_for_estimator(flamly0[-1].best_estimator)))\n",
    "flamly1.fit(X[D == 1], y[D == 1])\n",
    "besty1 = make_pipeline(transformer, clone(flamly1[-1].best_model_for_estimator(flamly1[-1].best_estimator)))\n",
    "flamld.fit(X, D)\n",
    "bestd = make_pipeline(transformer, clone(flamld[-1].best_model_for_estimator(flamld[-1].best_estimator)))\n",
    "result = dr(X, D, y, besty0, besty1, bestd, nfolds=5)\n",
    "tabledr = pd.concat([tabledr, summary(*result, X, D, y, name='automl (semi-cfit)')])\n",
    "def dr_dirty(X, D, y, modely0_list, modely1_list, modeld_list, *,\n",
    "             stacker=LinearRegression(), trimming=0.01, nfolds):\n",
    "    '''\n",
    "    DML for the Interactive Regression Model setting (Doubly Robust Learning)\n",
    "    with cross-fitting\n",
    "\n",
    "    Input\n",
    "    -----\n",
    "    X: the controls\n",
    "    D: the treatment\n",
    "    y: the outcome\n",
    "    modely_list: list of ML models for predicting the outcome y\n",
    "    modeld_list: list of ML models for predicting the treatment D\n",
    "    stacker: model used to aggregate predictions of each of the base models\n",
    "    trimming: threshold below which to trim propensities\n",
    "    nfolds: the number of folds in cross-fitting\n",
    "\n",
    "    Output\n",
    "    ------\n",
    "    point: the point estimate of the treatment effect of D on y\n",
    "    stderr: the standard error of the treatment effect\n",
    "    yhat: the cross-fitted predictions for the outcome y\n",
    "    Dhat: the cross-fitted predictions for the outcome D\n",
    "    resy: the outcome residuals\n",
    "    resD: the treatment residuals\n",
    "    drhat: the doubly robust quantity for each sample\n",
    "    '''\n",
    "    cv = KFold(n_splits=nfolds, shuffle=True, random_state=123)\n",
    "\n",
    "    # we will fit a model E[Y| D, X] by fitting a separate model for D==0\n",
    "    # and a separate model for D==1. We do that for each model type in modely_list\n",
    "    yhats0, yhats1 = np.zeros((y.shape[0], len(modely0_list))), np.zeros((y.shape[0], len(modely1_list)))\n",
    "    for train, test in cv.split(X, y):\n",
    "        for it, modely0 in enumerate(modely0_list):\n",
    "            mdl = clone(modely0).fit(X.iloc[train][D[train] == 0], y[train][D[train] == 0])\n",
    "            yhats0[test, it] = mdl.predict(X.iloc[test])\n",
    "        for it, modely1 in enumerate(modely1_list):\n",
    "            mdl = clone(modely1).fit(X.iloc[train][D[train] == 1], y[train][D[train] == 1])\n",
    "            yhats1[test, it] = mdl.predict(X.iloc[test])\n",
    "\n",
    "    # calculate stacking weights for the outcome model for each population\n",
    "    # and combine the outcome model predictions\n",
    "    yhat0 = clone(stacker).fit(yhats0[D == 0], y[D == 0]).predict(yhats0)\n",
    "    yhat1 = clone(stacker).fit(yhats1[D == 1], y[D == 1]).predict(yhats1)\n",
    "\n",
    "    # prediction for observed treatment using the stacked model\n",
    "    yhat = yhat0 * (1 - D) + yhat1 * D\n",
    "\n",
    "    # propensity scores\n",
    "    Dhats = np.array([cross_val_predict(modeld, X, D, cv=cv, method='predict_proba', n_jobs=-1)[:, 1]\n",
    "                     for modeld in modeld_list]).T\n",
    "    # construct coefficients on each model based on stacker\n",
    "    Dhat = clone(stacker).fit(Dhats, D).predict(Dhats)\n",
    "    # trim propensities\n",
    "    Dhat = np.clip(Dhat, trimming, 1 - trimming)\n",
    "\n",
    "    # doubly robust quantity for every sample\n",
    "    drhat = yhat1 - yhat0 + (y - yhat) * (D / Dhat - (1 - D) / (1 - Dhat))\n",
    "    point = np.mean(drhat)\n",
    "    var = np.var(drhat)\n",
    "    stderr = np.sqrt(var / X.shape[0])\n",
    "    return point, stderr, yhat, Dhat, y - yhat, D - Dhat, drhat\n",
    "\n",
    "result = dr_dirty(X, D, y, [lassoy, rfy, dtry, gbfy], [lassoy, rfy, dtry, gbfy], [lgrd, rfd, dtrd, gbfd], nfolds=5)\n",
    "tabledr = pd.concat([tabledr, summary(*result, X, D, y, name='stacked (semi-cfit)')])\n",
    "tabledr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Doubly Robust Results: Bottom 25% Income ===\n",
      "                        estimate       stderr        lower         upper  \\\n",
      "lasso/logistic       4451.346490  1021.649465  2448.913538   6453.779442   \n",
      "random forest        4414.302218   932.452433  2586.695449   6241.908988   \n",
      "decision tree        8448.821293  4357.294763   -91.476442  16989.119027   \n",
      "boosted forest       4927.998798   985.194209  2997.018148   6858.979448   \n",
      "automl (semi-cfit)   4176.134761  1145.621803  1930.716026   6421.553496   \n",
      "stacked (semi-cfit)  3922.257750  1004.465543  1953.505285   5891.010214   \n",
      "\n",
      "                           rmse y    rmse D  accuracy D  \n",
      "lasso/logistic       13323.560722  0.359078    0.846030  \n",
      "random forest        13462.908700  0.346004    0.845627  \n",
      "decision tree        14229.940513  0.380676    0.804111  \n",
      "boosted forest       13360.056669  0.345551    0.845224  \n",
      "automl (semi-cfit)   13177.371099  0.351340    0.835953  \n",
      "stacked (semi-cfit)  13214.050444  0.343302    0.843611  \n",
      "\n",
      "=== Doubly Robust Results: Top 25% Income ===\n",
      "                         estimate        stderr         lower         upper  \\\n",
      "lasso/logistic       17259.513464   3916.988487   9582.216029  24936.810898   \n",
      "random forest        16128.857736   4070.081031   8151.498915  24106.216556   \n",
      "decision tree        46612.868418  26498.324752  -5323.848095  98549.584931   \n",
      "boosted forest       16108.066269   3943.820631   8378.177832  23837.954707   \n",
      "automl (semi-cfit)   17098.040516   3750.769267   9746.532753  24449.548279   \n",
      "stacked (semi-cfit)  18149.589478   3828.507320  10645.715130  25653.463826   \n",
      "\n",
      "                            rmse y    rmse D  accuracy D  \n",
      "lasso/logistic        91534.931249  0.482861    0.603066  \n",
      "random forest         96646.723123  0.485632    0.605889  \n",
      "decision tree        103739.400617  0.543962    0.536910  \n",
      "boosted forest        97650.436196  0.484326    0.596612  \n",
      "automl (semi-cfit)    94421.196487  0.484561    0.603066  \n",
      "stacked (semi-cfit)   90888.949799  0.481984    0.606293  \n"
     ]
    }
   ],
   "source": [
    "data_bottom = data.query('inc < inc.quantile(.25)').copy()\n",
    "data_top    = data.query('inc > inc.quantile(.75)').copy()\n",
    "\n",
    "def extract_XDy(df):\n",
    "    \"\"\"Given a subset of the 401k data, produce X, D, and y \n",
    "    consistent with the main DR analysis.\"\"\"\n",
    "    y_ = df['net_tfa'].values\n",
    "    D_ = df['e401'].values\n",
    "    X_ = df.drop([\n",
    "        'e401', 'p401', 'a401', 'tw', 'tfa', 'net_tfa', 'tfa_he',\n",
    "        'hval', 'hmort', 'hequity', 'nifa', 'net_nifa', 'net_n401',\n",
    "        'ira', 'dum91', 'icat', 'ecat', 'zhat', 'i1', 'i2', 'i3',\n",
    "        'i4', 'i5', 'i6', 'i7', 'a1', 'a2', 'a3', 'a4', 'a5'\n",
    "    ], axis=1)\n",
    "    return X_, D_, y_\n",
    "\n",
    "X_bottom, D_bottom, y_bottom = extract_XDy(data_bottom)\n",
    "X_top, D_top, y_top          = extract_XDy(data_top)\n",
    "\n",
    "def dr(X, D, y, modely0, modely1, modeld, *, trimming=0.01, nfolds=5):\n",
    "    '''\n",
    "    DML for the Interactive Regression Model setting (Doubly Robust Learning)\n",
    "    with cross-fitting\n",
    "    '''\n",
    "    cv = KFold(n_splits=nfolds, shuffle=True, random_state=123)\n",
    "    yhat0, yhat1 = np.zeros(y.shape), np.zeros(y.shape)\n",
    "    for train, test in cv.split(X, y):\n",
    "        # Fit E[Y|D=0,X]\n",
    "        mdl0 = clone(modely0).fit(X.iloc[train][D[train] == 0], y[train][D[train] == 0])\n",
    "        yhat0[test] = mdl0.predict(X.iloc[test])\n",
    "        # Fit E[Y|D=1,X]\n",
    "        mdl1 = clone(modely1).fit(X.iloc[train][D[train] == 1], y[train][D[train] == 1])\n",
    "        yhat1[test] = mdl1.predict(X.iloc[test])\n",
    "\n",
    "    # Combine to get E[Y|D,X] predictions for the observed D\n",
    "    yhat = yhat0 * (1 - D) + yhat1 * D\n",
    "\n",
    "    # Propensity scores\n",
    "    Dhat = cross_val_predict(modeld, X, D, cv=cv, method='predict_proba', n_jobs=-1)[:, 1]\n",
    "    Dhat = np.clip(Dhat, trimming, 1 - trimming)\n",
    "\n",
    "    # Doubly robust quantity for each sample\n",
    "    drhat = yhat1 - yhat0 + (y - yhat) * (D / Dhat - (1 - D) / (1 - Dhat))\n",
    "    point = np.mean(drhat)\n",
    "    var   = np.var(drhat)\n",
    "    stderr= np.sqrt(var / X.shape[0])\n",
    "\n",
    "    # Return:\n",
    "    #   - point: the point estimate\n",
    "    #   - stderr: standard error\n",
    "    #   - yhat: cross-fitted predictions for the observed D\n",
    "    #   - Dhat: cross-fitted propensities\n",
    "    #   - (y - yhat): outcome residual\n",
    "    #   - (D - Dhat): treatment residual\n",
    "    #   - drhat: doubly robust terms\n",
    "    return point, stderr, yhat, Dhat, (y - yhat), (D - Dhat), drhat\n",
    "\n",
    "def dr_dirty(\n",
    "    X, D, y,\n",
    "    modely0_list, modely1_list, modeld_list,\n",
    "    stacker=LinearRegression(), trimming=0.01, nfolds=5\n",
    "):\n",
    "    '''\n",
    "    DML for the Interactive Regression Model setting (Doubly Robust Learning)\n",
    "    with cross-fitting and stacking\n",
    "    '''\n",
    "    cv = KFold(n_splits=nfolds, shuffle=True, random_state=123)\n",
    "\n",
    "    # Prepare space for multiple model predictions\n",
    "    yhats0 = np.zeros((X.shape[0], len(modely0_list)))\n",
    "    yhats1 = np.zeros((X.shape[0], len(modely1_list)))\n",
    "\n",
    "    # Cross-fitting: fit each base model E[Y|D=0,X], E[Y|D=1,X]\n",
    "    for train, test in cv.split(X, y):\n",
    "        for i_m0, m0 in enumerate(modely0_list):\n",
    "            m0_cl = clone(m0).fit(X.iloc[train][D[train] == 0], y[train][D[train] == 0])\n",
    "            yhats0[test, i_m0] = m0_cl.predict(X.iloc[test])\n",
    "        for i_m1, m1 in enumerate(modely1_list):\n",
    "            m1_cl = clone(m1).fit(X.iloc[train][D[train] == 1], y[train][D[train] == 1])\n",
    "            yhats1[test, i_m1] = m1_cl.predict(X.iloc[test])\n",
    "\n",
    "    # Stack them for D=0 predictions\n",
    "    yhat0 = clone(stacker).fit(yhats0[D == 0], y[D == 0]).predict(yhats0)\n",
    "    # Stack them for D=1 predictions\n",
    "    yhat1 = clone(stacker).fit(yhats1[D == 1], y[D == 1]).predict(yhats1)\n",
    "    # Combined model prediction for Y\n",
    "    yhat = yhat0 * (1 - D) + yhat1 * D\n",
    "\n",
    "    # Now do the same for propensity models\n",
    "    Dhats_list = []\n",
    "    for md in modeld_list:\n",
    "        Dhats_list.append(cross_val_predict(md, X, D, cv=cv, method='predict_proba', n_jobs=-1)[:, 1])\n",
    "    Dhats_arr = np.vstack(Dhats_list).T  # shape: (n_samples, n_models)\n",
    "\n",
    "    # Stack the predicted propensity\n",
    "    Dhat = clone(stacker).fit(Dhats_arr, D).predict(Dhats_arr)\n",
    "    Dhat = np.clip(Dhat, trimming, 1 - trimming)\n",
    "\n",
    "    # Doubly robust quantity\n",
    "    drhat = yhat1 - yhat0 + (y - yhat) * (D / Dhat - (1 - D) / (1 - Dhat))\n",
    "    point = np.mean(drhat)\n",
    "    var   = np.var(drhat)\n",
    "    stderr= np.sqrt(var / X.shape[0])\n",
    "\n",
    "    return point, stderr, yhat, Dhat, (y - yhat), (D - Dhat), drhat\n",
    "\n",
    "\n",
    "def summary(point, stderr, yhat, Dhat, resy, resD, drhat, X, D, y, *, name=''):\n",
    "    return pd.DataFrame({\n",
    "        'estimate': point,  # point estimate\n",
    "        'stderr': stderr,   # standard error\n",
    "        'lower': point - 1.96 * stderr,\n",
    "        'upper': point + 1.96 * stderr,\n",
    "        'rmse y': np.sqrt(np.mean(resy**2)),\n",
    "        'rmse D': np.sqrt(np.mean(resD**2)),\n",
    "        # classification accuracy for D, if it's binary:\n",
    "        'accuracy D': np.mean(np.abs(resD) < 0.5),\n",
    "    }, index=[name])\n",
    "\n",
    "\n",
    "def run_dr_analysis(X, D, y):\n",
    "    \"\"\"\n",
    "    Replicates the DR analysis shown in the original code:\n",
    "      - lasso/logistic with repeated seeds, then median-based estimate\n",
    "      - random forest\n",
    "      - decision tree\n",
    "      - boosted forest\n",
    "      - automl (semi-cfit)\n",
    "      - stacking\n",
    "    Returns a single table of results.\n",
    "    \"\"\"\n",
    "    # -- 1) Lasso/logistic repeated over seeds, then median-based estimate\n",
    "    seed_estimates = None\n",
    "\n",
    "    # We'll define a base 5-fold for the model pipelines:\n",
    "    cv_5fold = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "\n",
    "    # One example run with seed=123\n",
    "    lassoy = make_pipeline(transformer, StandardScaler(), LassoCV(cv=cv_5fold))\n",
    "    lgrd   = make_pipeline(transformer, StandardScaler(), LogisticRegressionCV(cv=cv_5fold))\n",
    "\n",
    "    # DR for single seed first\n",
    "    result = dr(X, D, y, lassoy, lassoy, lgrd, nfolds=5)\n",
    "    seed_estimates = summary(*result, X, D, y, name='lasso/logistic')\n",
    "\n",
    "    # Loop over multiple seeds\n",
    "    for i in range(9):\n",
    "        cv_seed = KFold(n_splits=5, shuffle=True, random_state=i)\n",
    "        lassoy_i = make_pipeline(transformer, StandardScaler(), LassoCV(cv=cv_seed))\n",
    "        lgrd_i   = make_pipeline(transformer, StandardScaler(), LogisticRegressionCV(cv=cv_seed))\n",
    "\n",
    "        result_i = dr(X, D, y, lassoy_i, lassoy_i, lgrd_i, nfolds=5)\n",
    "        seed_estimates = pd.concat([seed_estimates, summary(*result_i, X, D, y, name='lasso/logistic')])\n",
    "\n",
    "    # Compute median-based point estimate and standard error\n",
    "    med_theta  = np.median(seed_estimates['estimate'].values)\n",
    "    # sqrt(median(var_i) + var( point_i ))\n",
    "    # but the original code basically does:\n",
    "    se_med     = np.sqrt(\n",
    "        np.median(seed_estimates['stderr'].values ** 2)\n",
    "        + np.median((seed_estimates['estimate'].values - med_theta) ** 2)\n",
    "    )\n",
    "    tabledr = pd.DataFrame({\n",
    "        'estimate': med_theta,\n",
    "        'stderr':   se_med,\n",
    "        'lower':    med_theta - 1.96 * se_med,\n",
    "        'upper':    med_theta + 1.96 * se_med,\n",
    "        'rmse y':   np.median(seed_estimates['rmse y'].values),\n",
    "        'rmse D':   np.median(seed_estimates['rmse D'].values),\n",
    "        'accuracy D': np.median(seed_estimates['accuracy D'].values)\n",
    "    }, index=['lasso/logistic'])\n",
    "\n",
    "    # -- 2) Random Forest\n",
    "    rfy = make_pipeline(transformer, \n",
    "                        RandomForestRegressor(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001))\n",
    "    rfd = make_pipeline(transformer, \n",
    "                        RandomForestClassifier(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001))\n",
    "    result = dr(X, D, y, rfy, rfy, rfd, nfolds=5)\n",
    "    tabledr = pd.concat([tabledr, summary(*result, X, D, y, name='random forest')])\n",
    "\n",
    "    # -- 3) Decision Tree\n",
    "    dtry = make_pipeline(transformer,\n",
    "                         DecisionTreeRegressor(min_samples_leaf=10, ccp_alpha=.001))\n",
    "    dtrd = make_pipeline(transformer,\n",
    "                         DecisionTreeClassifier(min_samples_leaf=10, ccp_alpha=.001))\n",
    "    result = dr(X, D, y, dtry, dtry, dtrd, nfolds=5)\n",
    "    tabledr = pd.concat([tabledr, summary(*result, X, D, y, name='decision tree')])\n",
    "\n",
    "    # -- 4) Boosted Forest\n",
    "    gbfy = make_pipeline(transformer, \n",
    "                         GradientBoostingRegressor(max_depth=2, n_iter_no_change=5))\n",
    "    gbfd = make_pipeline(transformer, \n",
    "                         GradientBoostingClassifier(max_depth=2, n_iter_no_change=5))\n",
    "    result = dr(X, D, y, gbfy, gbfy, gbfd, nfolds=5)\n",
    "    tabledr = pd.concat([tabledr, summary(*result, X, D, y, name='boosted forest')])\n",
    "\n",
    "    # -- 5) AutoML (semi cross-fitting)\n",
    "    flamly0 = make_pipeline(transformer,\n",
    "                            AutoML(time_budget=60, task='regression', early_stop=True,\n",
    "                                   eval_method='cv', n_splits=3, metric='r2', verbose=0))\n",
    "    flamly1 = make_pipeline(transformer,\n",
    "                            AutoML(time_budget=60, task='regression', early_stop=True,\n",
    "                                   eval_method='cv', n_splits=3, metric='r2', verbose=0))\n",
    "    flamld  = make_pipeline(transformer,\n",
    "                            AutoML(time_budget=60, task='classification', early_stop=True,\n",
    "                                   eval_method='cv', n_splits=3, metric='r2', verbose=0))\n",
    "\n",
    "    # Fit Y|D=0, Y|D=1 on subsets\n",
    "    flamly0.fit(X[D == 0], y[D == 0])\n",
    "    besty0_model = flamly0[-1].best_model_for_estimator(flamly0[-1].best_estimator)\n",
    "    besty0 = make_pipeline(transformer, clone(besty0_model))\n",
    "\n",
    "    flamly1.fit(X[D == 1], y[D == 1])\n",
    "    besty1_model = flamly1[-1].best_model_for_estimator(flamly1[-1].best_estimator)\n",
    "    besty1 = make_pipeline(transformer, clone(besty1_model))\n",
    "\n",
    "    # Fit propensities on the full sample\n",
    "    flamld.fit(X, D)\n",
    "    bestd_model = flamld[-1].best_model_for_estimator(flamld[-1].best_estimator)\n",
    "    bestd = make_pipeline(transformer, clone(bestd_model))\n",
    "\n",
    "    result = dr(X, D, y, besty0, besty1, bestd, nfolds=5)\n",
    "    tabledr = pd.concat([tabledr, summary(*result, X, D, y, name='automl (semi-cfit)')])\n",
    "\n",
    "    # -- 6) Stacked (semi-cfit)\n",
    "    # Prepare lists of base models for Y|D=0, Y|D=1\n",
    "    lassoy_ = make_pipeline(transformer, StandardScaler(), LassoCV(cv=cv_5fold))\n",
    "    rfy_    = make_pipeline(transformer, \n",
    "                            RandomForestRegressor(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001))\n",
    "    dtry_   = make_pipeline(transformer, \n",
    "                            DecisionTreeRegressor(min_samples_leaf=10, ccp_alpha=.001))\n",
    "    gbfy_   = make_pipeline(transformer, \n",
    "                            GradientBoostingRegressor(max_depth=2, n_iter_no_change=5))\n",
    "\n",
    "    modely0_list = [lassoy_, rfy_, dtry_, gbfy_]  # for D=0\n",
    "    modely1_list = [lassoy_, rfy_, dtry_, gbfy_]  # for D=1\n",
    "\n",
    "    lgrd_   = make_pipeline(transformer, StandardScaler(), LogisticRegressionCV(cv=cv_5fold))\n",
    "    rfd_    = make_pipeline(transformer, \n",
    "                            RandomForestClassifier(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001))\n",
    "    dtrd_   = make_pipeline(transformer, \n",
    "                            DecisionTreeClassifier(min_samples_leaf=10, ccp_alpha=.001))\n",
    "    gbfd_   = make_pipeline(transformer, \n",
    "                            GradientBoostingClassifier(max_depth=2, n_iter_no_change=5))\n",
    "    modeld_list = [lgrd_, rfd_, dtrd_, gbfd_]\n",
    "\n",
    "    result = dr_dirty(X, D, y, modely0_list, modely1_list, modeld_list, stacker=LinearRegression(), nfolds=5)\n",
    "    tabledr = pd.concat([tabledr, summary(*result, X, D, y, name='stacked (semi-cfit)')])\n",
    "\n",
    "    return tabledr\n",
    "\n",
    "table_dr_bottom = run_dr_analysis(X_bottom, D_bottom, y_bottom)\n",
    "table_dr_top    = run_dr_analysis(X_top,    D_top,    y_top   )\n",
    "\n",
    "print(\"=== Doubly Robust Results: Bottom 25% Income ===\")\n",
    "print(table_dr_bottom)\n",
    "\n",
    "print(\"\\n=== Doubly Robust Results: Top 25% Income ===\")\n",
    "print(table_dr_top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the IRM setting, we again see heterogeneity in the treatment effect with respect to income, with the bottom 25% of earners seeing estimates around 4.5k and the top 25% seeing estimates of around 17k. The different machine learning models are broadly consistent across all three income groups with the exception of the decision tree, which has far higher estimates and standard errors than other methods.  \n",
    "  \n",
    "\n",
    "Now we move to implemetting semi-crossfitting with best model selection, first for PLR and then for IRM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>estimate</th>\n",
       "      <th>stderr</th>\n",
       "      <th>lower</th>\n",
       "      <th>upper</th>\n",
       "      <th>rmse y</th>\n",
       "      <th>rmse D</th>\n",
       "      <th>accuracy D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>select-best (semi-cfit) PLR</th>\n",
       "      <td>8870.159144</td>\n",
       "      <td>1317.050568</td>\n",
       "      <td>6288.740030</td>\n",
       "      <td>11451.578258</td>\n",
       "      <td>54254.468883</td>\n",
       "      <td>0.443545</td>\n",
       "      <td>0.692284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>select-best (semi-cfit) PLR (bottom 25%)</th>\n",
       "      <td>3832.072545</td>\n",
       "      <td>1099.922462</td>\n",
       "      <td>1676.224520</td>\n",
       "      <td>5987.920571</td>\n",
       "      <td>13400.361810</td>\n",
       "      <td>0.345369</td>\n",
       "      <td>0.846836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>select-best (semi-cfit) PLR (top 25%)</th>\n",
       "      <td>18204.194590</td>\n",
       "      <td>3867.405524</td>\n",
       "      <td>10624.079763</td>\n",
       "      <td>25784.309418</td>\n",
       "      <td>91393.039963</td>\n",
       "      <td>0.482708</td>\n",
       "      <td>0.601049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              estimate       stderr  \\\n",
       "select-best (semi-cfit) PLR                8870.159144  1317.050568   \n",
       "select-best (semi-cfit) PLR (bottom 25%)   3832.072545  1099.922462   \n",
       "select-best (semi-cfit) PLR (top 25%)     18204.194590  3867.405524   \n",
       "\n",
       "                                                 lower         upper  \\\n",
       "select-best (semi-cfit) PLR                6288.740030  11451.578258   \n",
       "select-best (semi-cfit) PLR (bottom 25%)   1676.224520   5987.920571   \n",
       "select-best (semi-cfit) PLR (top 25%)     10624.079763  25784.309418   \n",
       "\n",
       "                                                rmse y    rmse D  accuracy D  \n",
       "select-best (semi-cfit) PLR               54254.468883  0.443545    0.692284  \n",
       "select-best (semi-cfit) PLR (bottom 25%)  13400.361810  0.345369    0.846836  \n",
       "select-best (semi-cfit) PLR (top 25%)     91393.039963  0.482708    0.601049  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# b.)\n",
    "from sklearn.model_selection import cross_val_predict, KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from copy import deepcopy\n",
    "\n",
    "def get_oof_and_mse(model, X, y, cv, classifier=False):\n",
    "    \"\"\"\n",
    "    Returns out-of-fold predictions and the MSE (or 'regression MSE' if classifier=True).\n",
    "    If classifier=True, we use model.predict_proba(...).\n",
    "    \"\"\"\n",
    "    if classifier:\n",
    "        # For binary D, treat the problem as a regression on [0,1], \n",
    "        # so we measure MSE on predicted probabilities\n",
    "        preds = cross_val_predict(model, X, y, cv=cv, method='predict_proba')[:, 1]\n",
    "    else:\n",
    "        preds = cross_val_predict(model, X, y, cv=cv)\n",
    "    mse_val = mean_squared_error(y, preds)\n",
    "    return preds, mse_val\n",
    "\n",
    "\n",
    "def dml_select_best(X, D, y, modely_list, modeld_list, *, nfolds=5, classifier=False, trimming=0.01):\n",
    "    \"\"\"\n",
    "    Semi-cross-fitting for the Partially Linear Model (PLR) by selecting\n",
    "    the single best model for Y and single best model for D among user-provided lists.\n",
    "\n",
    "    Steps:\n",
    "      1) For each candidate in modely_list, get OOF predictions vs. y. Pick the best by MSE.\n",
    "      2) For each candidate in modeld_list, get OOF predictions vs. D. Pick the best by MSE.\n",
    "         If classifier=True, each model in modeld_list is treated as a classifier,\n",
    "         but we still measure MSE on the predicted probability vs. the true D.\n",
    "      3) Use the chosen best model's OOF predictions for yhat and Dhat.\n",
    "      4) Compute partial linear estimate as in the usual DML formula:\n",
    "            theta = E[(y - yhat)*(D - Dhat)] / E[(D - Dhat)^2]\n",
    "         and the standard error formula.\n",
    "\n",
    "    Returns:\n",
    "      point, stderr, yhat, Dhat, resy, resD, epsilon\n",
    "    \"\"\"\n",
    "    cv = KFold(n_splits=nfolds, shuffle=True, random_state=123)\n",
    "\n",
    "    # --- 1) Select best model for y\n",
    "    best_mse_y = np.inf\n",
    "    best_preds_y = None\n",
    "    best_model_y = None\n",
    "\n",
    "    for candidate in modely_list:\n",
    "        # Clone so we don't pollute the original pipeline with partial fits\n",
    "        cand = deepcopy(candidate)\n",
    "        preds, mse_val = get_oof_and_mse(cand, X, y, cv, classifier=False)\n",
    "        if mse_val < best_mse_y:\n",
    "            best_mse_y = mse_val\n",
    "            best_preds_y = preds\n",
    "            best_model_y = cand\n",
    "\n",
    "    # --- 2) Select best model for D\n",
    "    best_mse_d = np.inf\n",
    "    best_preds_d = None\n",
    "    best_model_d = None\n",
    "\n",
    "    for candidate in modeld_list:\n",
    "        # If classifier=True, we measure MSE on predicted probabilities\n",
    "        cand = deepcopy(candidate)\n",
    "        preds, mse_val = get_oof_and_mse(cand, X, D, cv, classifier=classifier)\n",
    "        if mse_val < best_mse_d:\n",
    "            best_mse_d = mse_val\n",
    "            best_preds_d = preds\n",
    "            best_model_d = cand\n",
    "\n",
    "    # Residuals\n",
    "    resy = y - best_preds_y\n",
    "    resD = D - best_preds_d\n",
    "\n",
    "    # Final partial linear estimate\n",
    "    point = np.mean(resy * resD) / np.mean(resD**2)\n",
    "    epsilon = resy - point * resD\n",
    "\n",
    "    var = np.mean(epsilon**2 * resD**2) / (np.mean(resD**2)**2)\n",
    "    stderr = np.sqrt(var / X.shape[0])\n",
    "\n",
    "    return point, stderr, best_preds_y, best_preds_d, resy, resD, epsilon\n",
    "\n",
    "modely_list = [lassoy, rfy, dtry, gbfy]\n",
    "modeld_list = [lgrd, rfd, dtrd, gbfd]\n",
    "\n",
    "point, stderr, yhat, Dhat, resy, resD, epsilon = dml_select_best(\n",
    "    X, D, y, modely_list, modeld_list, nfolds=5, classifier=True\n",
    ")\n",
    "\n",
    "table_select = summary(\n",
    "    point, stderr, yhat, Dhat, resy, resD, epsilon, X, D, y,\n",
    "    name='select-best (semi-cfit) PLR'\n",
    ")\n",
    "point_bottom, stderr_bottom, yhat_bottom, Dhat_bottom, resy_bottom, resD_bottom, epsilon_bottom = dml_select_best(\n",
    "    X_bottom, D_bottom, y_bottom, modely_list, modeld_list, nfolds=5, classifier=True\n",
    ")\n",
    "table_select_bottom = summary(\n",
    "    point_bottom, stderr_bottom, yhat_bottom, Dhat_bottom, resy_bottom, resD_bottom, epsilon_bottom, X_bottom, D_bottom, y_bottom,\n",
    "    name='select-best (semi-cfit) PLR (bottom 25%)'\n",
    ")\n",
    "table_select = pd.concat([table_select, table_select_bottom])\n",
    "point_top, stderr_top, yhat_top, Dhat_top, resy_top, resD_top, epsilon_top = dml_select_best(\n",
    "    X_top, D_top, y_top, modely_list, modeld_list, nfolds=5, classifier=True\n",
    ")\n",
    "table_select_top = summary(\n",
    "    point_top, stderr_top, yhat_top, Dhat_top, resy_top, resD_top, epsilon_top, X_top, D_top, y_top,\n",
    "    name='select-best (semi-cfit) PLR (top 25%)'\n",
    ")\n",
    "table_select = pd.concat([table_select, table_select_top])\n",
    "table_select"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For IRM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>estimate</th>\n",
       "      <th>stderr</th>\n",
       "      <th>lower</th>\n",
       "      <th>upper</th>\n",
       "      <th>rmse y</th>\n",
       "      <th>rmse D</th>\n",
       "      <th>accuracy D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>select best IRM with semi cross fitting all samples</th>\n",
       "      <td>7716.394260</td>\n",
       "      <td>1117.818732</td>\n",
       "      <td>5525.469546</td>\n",
       "      <td>9907.318975</td>\n",
       "      <td>54026.928780</td>\n",
       "      <td>0.443615</td>\n",
       "      <td>0.690066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>select best IRM with semi cross fitting bottom 25\\% income</th>\n",
       "      <td>3985.926019</td>\n",
       "      <td>980.508563</td>\n",
       "      <td>2064.129235</td>\n",
       "      <td>5907.722803</td>\n",
       "      <td>13329.992579</td>\n",
       "      <td>0.345253</td>\n",
       "      <td>0.844418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>select best IRM with semi cross fitting top 25\\% income</th>\n",
       "      <td>18393.721922</td>\n",
       "      <td>3820.594388</td>\n",
       "      <td>10905.356922</td>\n",
       "      <td>25882.086922</td>\n",
       "      <td>91533.229313</td>\n",
       "      <td>0.482708</td>\n",
       "      <td>0.601049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        estimate       stderr  \\\n",
       "select best IRM with semi cross fitting all sam...   7716.394260  1117.818732   \n",
       "select best IRM with semi cross fitting bottom ...   3985.926019   980.508563   \n",
       "select best IRM with semi cross fitting top 25\\...  18393.721922  3820.594388   \n",
       "\n",
       "                                                           lower  \\\n",
       "select best IRM with semi cross fitting all sam...   5525.469546   \n",
       "select best IRM with semi cross fitting bottom ...   2064.129235   \n",
       "select best IRM with semi cross fitting top 25\\...  10905.356922   \n",
       "\n",
       "                                                           upper  \\\n",
       "select best IRM with semi cross fitting all sam...   9907.318975   \n",
       "select best IRM with semi cross fitting bottom ...   5907.722803   \n",
       "select best IRM with semi cross fitting top 25\\...  25882.086922   \n",
       "\n",
       "                                                          rmse y    rmse D  \\\n",
       "select best IRM with semi cross fitting all sam...  54026.928780  0.443615   \n",
       "select best IRM with semi cross fitting bottom ...  13329.992579  0.345253   \n",
       "select best IRM with semi cross fitting top 25\\...  91533.229313  0.482708   \n",
       "\n",
       "                                                    accuracy D  \n",
       "select best IRM with semi cross fitting all sam...    0.690066  \n",
       "select best IRM with semi cross fitting bottom ...    0.844418  \n",
       "select best IRM with semi cross fitting top 25\\...    0.601049  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_oof_and_mse_irm(model, X, y, D, which_d, cv):\n",
    "    \"\"\"\n",
    "    For IRM, we measure OOF performance only on the subset where D == which_d.\n",
    "    We do cross-fitting: train on all 'train' points that have D==which_d, \n",
    "    predict on the entire test fold, but compute MSE only for the test fold \n",
    "    members that also have D==which_d.\n",
    "    \n",
    "    Returns OOF predictions (full length, but only truly valid for D==which_d),\n",
    "    plus MSE measured on that subset.\n",
    "    \"\"\"\n",
    "    preds_full = np.zeros(len(y), dtype=float)\n",
    "    for train_idx, test_idx in cv.split(X, y):\n",
    "        # filter the training portion to only those with D==which_d\n",
    "        train_sub = train_idx[D[train_idx] == which_d]\n",
    "        # Fit on (X, y) for that sub-sample\n",
    "        model_cl = deepcopy(model).fit(X.iloc[train_sub], y[train_sub])\n",
    "        # Predict on the entire test fold\n",
    "        preds_full[test_idx] = model_cl.predict(X.iloc[test_idx])\n",
    "    # MSE only on the subset that has D==which_d\n",
    "    mse_val = mean_squared_error(y[D==which_d], preds_full[D==which_d])\n",
    "    return preds_full, mse_val\n",
    "\n",
    "def dr_select_best(X, D, y, modely0_list, modely1_list, modeld_list,\n",
    "                   trimming=0.01, nfolds=5):\n",
    "    \"\"\"\n",
    "    Doubly-Robust (IRM) with semi-cross-fitting:\n",
    "    Select single best model for Y|D=0 from modely0_list,\n",
    "    single best model for Y|D=1 from modely1_list,\n",
    "    single best model for the propensity from modeld_list (all data).\n",
    "    \n",
    "    Then run the standard cross-fitting formula to construct:\n",
    "      yhat0, yhat1, Dhat, drhat,\n",
    "    and output the usual IRM results.\n",
    "    \"\"\"\n",
    "    cv = KFold(n_splits=nfolds, shuffle=True, random_state=123)\n",
    "\n",
    "    # --- 1) find best model for Y|D=0\n",
    "    best_mse_y0 = np.inf\n",
    "    best_preds_y0 = None\n",
    "    best_model_y0 = None\n",
    "    for candidate in modely0_list:\n",
    "        preds0, mse0 = get_oof_and_mse_irm(candidate, X, y, D, which_d=0, cv=cv)\n",
    "        if mse0 < best_mse_y0:\n",
    "            best_mse_y0 = mse0\n",
    "            best_preds_y0 = preds0\n",
    "            best_model_y0 = candidate\n",
    "\n",
    "    # --- 2) find best model for Y|D=1\n",
    "    best_mse_y1 = np.inf\n",
    "    best_preds_y1 = None\n",
    "    best_model_y1 = None\n",
    "    for candidate in modely1_list:\n",
    "        preds1, mse1 = get_oof_and_mse_irm(candidate, X, y, D, which_d=1, cv=cv)\n",
    "        if mse1 < best_mse_y1:\n",
    "            best_mse_y1 = mse1\n",
    "            best_preds_y1 = preds1\n",
    "            best_model_y1 = candidate\n",
    "\n",
    "    # --- 3) find best model for D (propensity), measure MSE on entire sample\n",
    "    #         but we treat D as binary, using predicted probability\n",
    "    cv2 = KFold(n_splits=nfolds, shuffle=True, random_state=123)\n",
    "    best_mse_d = np.inf\n",
    "    best_preds_d = None\n",
    "    best_model_d = None\n",
    "    for candidate in modeld_list:\n",
    "        preds_d, mse_d = get_oof_and_mse(candidate, X, D, cv2, classifier=True)\n",
    "        if mse_d < best_mse_d:\n",
    "            best_mse_d = mse_d\n",
    "            best_preds_d = preds_d\n",
    "            best_model_d = candidate\n",
    "\n",
    "    # --- 4) IRM formula\n",
    "    # We already have cross-fitted yhat0, yhat1, Dhat = best_preds_d\n",
    "    yhat = best_preds_y0*(1 - D) + best_preds_y1*D\n",
    "    Dhat = np.clip(best_preds_d, trimming, 1 - trimming)\n",
    "\n",
    "    # DR score\n",
    "    drhat = best_preds_y1 - best_preds_y0 + (y - yhat) * (\n",
    "        D / Dhat - (1 - D)/(1 - Dhat)\n",
    "    )\n",
    "    point = np.mean(drhat)\n",
    "    var = np.var(drhat)\n",
    "    stderr = np.sqrt(var / X.shape[0])\n",
    "\n",
    "    return (\n",
    "        point,\n",
    "        stderr,\n",
    "        yhat,                # cross-fitted E[Y|D=observed, X]\n",
    "        Dhat,                # cross-fitted e(X)\n",
    "        (y - yhat),          # residual in Y space\n",
    "        (D - Dhat),          # residual in D space\n",
    "        drhat\n",
    "    )\n",
    "    \n",
    "modely0_list = [lassoy, rfy, dtry, gbfy]\n",
    "modely1_list = [lassoy, rfy, dtry, gbfy]\n",
    "modeld_list  = [lgrd,  rfd,  dtrd, gbfd]\n",
    "\n",
    "res_all = dr_select_best(\n",
    "    X, D, y, modely0_list, modely1_list, modeld_list, nfolds=5\n",
    ")\n",
    "res_bottom_25 = dr_select_best(\n",
    "    X_bottom, D_bottom, y_bottom, modely0_list, modely1_list, modeld_list, nfolds=5\n",
    ")\n",
    "res_top_25 = dr_select_best(\n",
    "    X_top, D_top, y_top, modely1_list, modely0_list, modeld_list, nfolds=5\n",
    ")\n",
    "table_all = summary(*res_all, X, D, y, name=\"select best IRM with semi cross fitting all samples\")\n",
    "table_bottom_25 = summary(*res_bottom_25, X_bottom, D_bottom, y_bottom, name=f\"select best IRM with semi cross fitting bottom 25\\% income\")\n",
    "table_top_25 = summary(*res_top_25, X_top, D_top, y_top, name=f\"select best IRM with semi cross fitting top 25\\% income\")\n",
    "\n",
    "table_final = pd.concat([table_all, table_bottom_25, table_top_25])\n",
    "table_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- Double Lasso in EconML for PLR------------\n",
      "Coefficient Results:  X is None, please call intercept_inference to learn the constant!\n",
      "                        CATE Intercept Results                        \n",
      "======================================================================\n",
      "               point_estimate  stderr  zstat pvalue ci_lower  ci_upper\n",
      "----------------------------------------------------------------------\n",
      "cate_intercept       9108.559 1360.917 6.693    0.0 6441.211 11775.907\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "<sub>A linear parametric conditional average treatment effect (CATE) model was fitted:\n",
      "$Y = \\Theta(X)\\cdot T + g(X, W) + \\epsilon$\n",
      "where for every outcome $i$ and treatment $j$ the CATE $\\Theta_{ij}(X)$ has the form:\n",
      "$\\Theta_{ij}(X) = X' coef_{ij} + cate\\_intercept_{ij}$\n",
      "Coefficient Results table portrays the $coef_{ij}$ parameter vector for each outcome $i$ and treatment $j$. Intercept Results table portrays the $cate\\_intercept_{ij}$ parameter.</sub>\n",
      "-----------------Random Forest in EconML for PLR------------\n",
      "Coefficient Results:  X is None, please call intercept_inference to learn the constant!\n",
      "                        CATE Intercept Results                        \n",
      "======================================================================\n",
      "               point_estimate  stderr  zstat pvalue ci_lower  ci_upper\n",
      "----------------------------------------------------------------------\n",
      "cate_intercept        8603.31 1333.541 6.451    0.0 5989.619 11217.002\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "<sub>A linear parametric conditional average treatment effect (CATE) model was fitted:\n",
      "$Y = \\Theta(X)\\cdot T + g(X, W) + \\epsilon$\n",
      "where for every outcome $i$ and treatment $j$ the CATE $\\Theta_{ij}(X)$ has the form:\n",
      "$\\Theta_{ij}(X) = X' coef_{ij} + cate\\_intercept_{ij}$\n",
      "Coefficient Results table portrays the $coef_{ij}$ parameter vector for each outcome $i$ and treatment $j$. Intercept Results table portrays the $cate\\_intercept_{ij}$ parameter.</sub>\n",
      "-----------------Gradient Boosting in EconML for PLR------------\n",
      "Coefficient Results:  X is None, please call intercept_inference to learn the constant!\n",
      "                        CATE Intercept Results                       \n",
      "=====================================================================\n",
      "               point_estimate  stderr zstat pvalue ci_lower  ci_upper\n",
      "---------------------------------------------------------------------\n",
      "cate_intercept       9129.167 1379.02  6.62    0.0 6426.337 11831.997\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "<sub>A linear parametric conditional average treatment effect (CATE) model was fitted:\n",
      "$Y = \\Theta(X)\\cdot T + g(X, W) + \\epsilon$\n",
      "where for every outcome $i$ and treatment $j$ the CATE $\\Theta_{ij}(X)$ has the form:\n",
      "$\\Theta_{ij}(X) = X' coef_{ij} + cate\\_intercept_{ij}$\n",
      "Coefficient Results table portrays the $coef_{ij}$ parameter vector for each outcome $i$ and treatment $j$. Intercept Results table portrays the $cate\\_intercept_{ij}$ parameter.</sub>\n",
      "-----------------Decision Tree in EconML for PLR------------\n",
      "Coefficient Results:  X is None, please call intercept_inference to learn the constant!\n",
      "                        CATE Intercept Results                        \n",
      "======================================================================\n",
      "               point_estimate  stderr  zstat pvalue ci_lower  ci_upper\n",
      "----------------------------------------------------------------------\n",
      "cate_intercept       8772.347 1448.641 6.056    0.0 5933.064 11611.631\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "<sub>A linear parametric conditional average treatment effect (CATE) model was fitted:\n",
      "$Y = \\Theta(X)\\cdot T + g(X, W) + \\epsilon$\n",
      "where for every outcome $i$ and treatment $j$ the CATE $\\Theta_{ij}(X)$ has the form:\n",
      "$\\Theta_{ij}(X) = X' coef_{ij} + cate\\_intercept_{ij}$\n",
      "Coefficient Results table portrays the $coef_{ij}$ parameter vector for each outcome $i$ and treatment $j$. Intercept Results table portrays the $cate\\_intercept_{ij}$ parameter.</sub>\n",
      "-----------------Double Lasso in DoubleML for PLR------------\n",
      "-----------------Random Forest in DoubleML for PLR------------\n",
      "          coef      std err         t         P>|t|        2.5 %        97.5 %\n",
      "d  8844.033734  1344.626944  6.577314  4.790222e-11  6208.613352  11479.454116\n",
      "-----------------Decision Tree in DoubleML for PLR------------\n",
      "          coef      std err         t         P>|t|        2.5 %        97.5 %\n",
      "d  8940.781007  1467.445067  6.092753  1.109849e-09  6064.641528  11816.920487\n",
      "-----------------Gradient Boosting in DoubleML for PLR------------\n",
      "         coef      std err         t         P>|t|        2.5 %        97.5 %\n",
      "d  8997.31593  1343.648914  6.696181  2.139372e-11  6363.812451  11630.819409\n",
      "-----------------Random Forest in EconML for IRM------------\n",
      "Coefficient Results:  X is None, please call intercept_inference to learn the constant!\n",
      "                        CATE Intercept Results                       \n",
      "=====================================================================\n",
      "               point_estimate  stderr  zstat pvalue ci_lower ci_upper\n",
      "---------------------------------------------------------------------\n",
      "cate_intercept       7848.221 1137.572 6.899    0.0 5618.621 10077.82\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "<sub>A linear parametric conditional average treatment effect (CATE) model was fitted:\n",
      "$Y = \\Theta(X)\\cdot T + g(X, W) + \\epsilon$\n",
      "where $T$ is the one-hot-encoding of the discrete treatment and for every outcome $i$ and treatment $j$ the CATE $\\Theta_{ij}(X)$ has the form:\n",
      "$\\Theta_{ij}(X) = \\phi(X)' coef_{ij} + cate\\_intercept_{ij}$\n",
      "where $\\phi(X)$ is the output of the `featurizer` or $X$ if `featurizer`=None. Coefficient Results table portrays the $coef_{ij}$ parameter vector for each outcome $i$ and the designated treatment $j$ passed to summary. Intercept Results table portrays the $cate\\_intercept_{ij}$ parameter.</sub>\n",
      "-----------------Random Forest in EconML for IRM (separate models)------------\n",
      "Coefficient Results:  X is None, please call intercept_inference to learn the constant!\n",
      "                        CATE Intercept Results                       \n",
      "=====================================================================\n",
      "               point_estimate  stderr  zstat pvalue ci_lower ci_upper\n",
      "---------------------------------------------------------------------\n",
      "cate_intercept       7447.758 1154.143 6.453    0.0 5185.679 9709.837\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "<sub>A linear parametric conditional average treatment effect (CATE) model was fitted:\n",
      "$Y = \\Theta(X)\\cdot T + g(X, W) + \\epsilon$\n",
      "where $T$ is the one-hot-encoding of the discrete treatment and for every outcome $i$ and treatment $j$ the CATE $\\Theta_{ij}(X)$ has the form:\n",
      "$\\Theta_{ij}(X) = \\phi(X)' coef_{ij} + cate\\_intercept_{ij}$\n",
      "where $\\phi(X)$ is the output of the `featurizer` or $X$ if `featurizer`=None. Coefficient Results table portrays the $coef_{ij}$ parameter vector for each outcome $i$ and the designated treatment $j$ passed to summary. Intercept Results table portrays the $cate\\_intercept_{ij}$ parameter.</sub>\n",
      "-----------------Decision Tree in EconML for IRM------------\n",
      "Coefficient Results:  X is None, please call intercept_inference to learn the constant!\n",
      "                        CATE Intercept Results                        \n",
      "======================================================================\n",
      "               point_estimate  stderr  zstat pvalue ci_lower  ci_upper\n",
      "----------------------------------------------------------------------\n",
      "cate_intercept       9222.086 1241.257  7.43    0.0 6789.267 11654.905\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "<sub>A linear parametric conditional average treatment effect (CATE) model was fitted:\n",
      "$Y = \\Theta(X)\\cdot T + g(X, W) + \\epsilon$\n",
      "where $T$ is the one-hot-encoding of the discrete treatment and for every outcome $i$ and treatment $j$ the CATE $\\Theta_{ij}(X)$ has the form:\n",
      "$\\Theta_{ij}(X) = \\phi(X)' coef_{ij} + cate\\_intercept_{ij}$\n",
      "where $\\phi(X)$ is the output of the `featurizer` or $X$ if `featurizer`=None. Coefficient Results table portrays the $coef_{ij}$ parameter vector for each outcome $i$ and the designated treatment $j$ passed to summary. Intercept Results table portrays the $cate\\_intercept_{ij}$ parameter.</sub>\n",
      "-----------------Decision Tree in EconML for IRM (separate models)------------\n",
      "Coefficient Results:  X is None, please call intercept_inference to learn the constant!\n",
      "                        CATE Intercept Results                        \n",
      "======================================================================\n",
      "               point_estimate  stderr  zstat pvalue ci_lower  ci_upper\n",
      "----------------------------------------------------------------------\n",
      "cate_intercept       8044.622 1189.481 6.763    0.0 5713.283 10375.961\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "<sub>A linear parametric conditional average treatment effect (CATE) model was fitted:\n",
      "$Y = \\Theta(X)\\cdot T + g(X, W) + \\epsilon$\n",
      "where $T$ is the one-hot-encoding of the discrete treatment and for every outcome $i$ and treatment $j$ the CATE $\\Theta_{ij}(X)$ has the form:\n",
      "$\\Theta_{ij}(X) = \\phi(X)' coef_{ij} + cate\\_intercept_{ij}$\n",
      "where $\\phi(X)$ is the output of the `featurizer` or $X$ if `featurizer`=None. Coefficient Results table portrays the $coef_{ij}$ parameter vector for each outcome $i$ and the designated treatment $j$ passed to summary. Intercept Results table portrays the $cate\\_intercept_{ij}$ parameter.</sub>\n",
      "-----------------Gradient Boosting in EconML for IRM------------\n",
      "Coefficient Results:  X is None, please call intercept_inference to learn the constant!\n",
      "                        CATE Intercept Results                        \n",
      "======================================================================\n",
      "               point_estimate  stderr  zstat pvalue ci_lower  ci_upper\n",
      "----------------------------------------------------------------------\n",
      "cate_intercept       8368.693 1128.212 7.418    0.0 6157.438 10579.947\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "<sub>A linear parametric conditional average treatment effect (CATE) model was fitted:\n",
      "$Y = \\Theta(X)\\cdot T + g(X, W) + \\epsilon$\n",
      "where $T$ is the one-hot-encoding of the discrete treatment and for every outcome $i$ and treatment $j$ the CATE $\\Theta_{ij}(X)$ has the form:\n",
      "$\\Theta_{ij}(X) = \\phi(X)' coef_{ij} + cate\\_intercept_{ij}$\n",
      "where $\\phi(X)$ is the output of the `featurizer` or $X$ if `featurizer`=None. Coefficient Results table portrays the $coef_{ij}$ parameter vector for each outcome $i$ and the designated treatment $j$ passed to summary. Intercept Results table portrays the $cate\\_intercept_{ij}$ parameter.</sub>\n",
      "-----------------Gradient Boosting in EconML for IRM (separate models)------------\n",
      "Coefficient Results:  X is None, please call intercept_inference to learn the constant!\n",
      "                        CATE Intercept Results                       \n",
      "=====================================================================\n",
      "               point_estimate  stderr  zstat pvalue ci_lower ci_upper\n",
      "---------------------------------------------------------------------\n",
      "cate_intercept       8191.684 1149.427 7.127    0.0 5938.849 10444.52\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "<sub>A linear parametric conditional average treatment effect (CATE) model was fitted:\n",
      "$Y = \\Theta(X)\\cdot T + g(X, W) + \\epsilon$\n",
      "where $T$ is the one-hot-encoding of the discrete treatment and for every outcome $i$ and treatment $j$ the CATE $\\Theta_{ij}(X)$ has the form:\n",
      "$\\Theta_{ij}(X) = \\phi(X)' coef_{ij} + cate\\_intercept_{ij}$\n",
      "where $\\phi(X)$ is the output of the `featurizer` or $X$ if `featurizer`=None. Coefficient Results table portrays the $coef_{ij}$ parameter vector for each outcome $i$ and the designated treatment $j$ passed to summary. Intercept Results table portrays the $cate\\_intercept_{ij}$ parameter.</sub>\n",
      "-----------------Random Forest in DoubleML for IRM------------\n",
      "          coef      std err         t         P>|t|       2.5 %       97.5 %\n",
      "d  7506.933727  1141.238795  6.577882  4.771978e-11  5270.14679  9743.720664\n",
      "-----------------Decision Tree in DoubleML for IRM------------\n",
      "          coef    std err         t         P>|t|        2.5 %       97.5 %\n",
      "d  7225.485969  1207.2451  5.985103  2.162533e-09  4859.329053  9591.642885\n",
      "-----------------Gradient Boosting in DoubleML for IRM------------\n",
      "          coef      std err         t         P>|t|        2.5 %        97.5 %\n",
      "d  8483.306715  1145.000981  7.408995  1.272600e-13  6239.146029  10727.467401\n"
     ]
    }
   ],
   "source": [
    " #c.) \n",
    "W = StandardScaler().fit_transform(transformer.fit_transform(X))\n",
    "# PLR in econml\n",
    "# ! pip install econml\n",
    "from econml.dml import LinearDML\n",
    "\n",
    "\n",
    "# double lasso in econml\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "ldml_lasso = LinearDML(\n",
    "    model_y=LassoCV(cv=cv),\n",
    "    model_t=LassoCV(cv=cv),\n",
    ").fit(y, D, W=W)\n",
    "print(\"----------------- Double Lasso in EconML for PLR------------\")\n",
    "print(ldml_lasso.summary())\n",
    "\n",
    "# random forest in econml\n",
    "ldml_rf = LinearDML(\n",
    "    model_y=RandomForestRegressor(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "    model_t=RandomForestClassifier(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "    cv=5,\n",
    "    discrete_treatment=True,\n",
    "    random_state=123\n",
    ").fit(y, D, W=W)\n",
    "print(\"-----------------Random Forest in EconML for PLR------------\")\n",
    "print(ldml_rf.summary())\n",
    "\n",
    "# gradient boosting in econml\n",
    "ldml_gb = LinearDML(\n",
    "    model_y=GradientBoostingRegressor(max_depth=2, n_iter_no_change=5, random_state=123),\n",
    "    model_t=GradientBoostingClassifier(max_depth=2, n_iter_no_change=5, random_state=123),\n",
    "    cv=5,\n",
    "    discrete_treatment=True,\n",
    "    random_state=123\n",
    ").fit(y, D, W=W)\n",
    "print(\"-----------------Gradient Boosting in EconML for PLR------------\")\n",
    "print(ldml_gb.summary())\n",
    "\n",
    "# PLR with Decision Tree\n",
    "ldml_dt = LinearDML(\n",
    "    model_y=DecisionTreeRegressor(min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "    model_t=DecisionTreeClassifier(min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "    cv=5,\n",
    "    discrete_treatment=True,\n",
    "    random_state=123\n",
    ").fit(y, D, W=W)\n",
    "print(\"-----------------Decision Tree in EconML for PLR------------\")\n",
    "print(ldml_dt.summary())\n",
    "\n",
    "# plr in double ml\n",
    "# ! pip install doubleml\n",
    "from doubleml import DoubleMLData\n",
    "import doubleml as dbml\n",
    "\n",
    "\n",
    "dml_data = DoubleMLData.from_arrays(W, y, D)\n",
    "\n",
    "try:\n",
    "    # double lasso\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "    dml_plr_lasso = dbml.DoubleMLPLR(\n",
    "        dml_data,\n",
    "        LassoCV(cv=cv),\n",
    "        LassoCV(cv=cv),\n",
    "        n_folds=5,\n",
    "    )\n",
    "    dml_plr_lasso.fit()\n",
    "    print(\"-----------------Double Lasso in DoubleML for PLR------------\")\n",
    "    # random forest\n",
    "    dml_plr_rf = dbml.DoubleMLPLR(\n",
    "        dml_data,\n",
    "        RandomForestRegressor(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "        RandomForestClassifier(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "        n_folds=5,\n",
    "    )\n",
    "    dml_plr_rf.fit()\n",
    "    print(\"-----------------Random Forest in DoubleML for PLR------------\")\n",
    "    print(dml_plr_rf.summary)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(\"DoubleML failed to run the PLR models\")\n",
    "\n",
    "# decision tree\n",
    "dml_plr_dt = dbml.DoubleMLPLR(\n",
    "    dml_data,\n",
    "    DecisionTreeRegressor(min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "    DecisionTreeClassifier(min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "    n_folds=5,\n",
    ")\n",
    "\n",
    "dml_plr_dt.fit()\n",
    "print(\"-----------------Decision Tree in DoubleML for PLR------------\")\n",
    "print(dml_plr_dt.summary)\n",
    "\n",
    "# gradient boosting\n",
    "\n",
    "dml_plr_gb = dbml.DoubleMLPLR(\n",
    "    dml_data,\n",
    "    GradientBoostingRegressor(max_depth=2, n_iter_no_change=5, random_state=123),\n",
    "    GradientBoostingClassifier(max_depth=2, n_iter_no_change=5, random_state=123),\n",
    "    n_folds=5,\n",
    ")\n",
    "\n",
    "dml_plr_gb.fit() \n",
    "print(\"-----------------Gradient Boosting in DoubleML for PLR------------\")\n",
    "print(dml_plr_gb.summary)\n",
    "\n",
    "\n",
    "# irm with econml\n",
    "\n",
    "from econml.dr import LinearDRLearner   \n",
    "from econml.utilities import SeparateModel\n",
    "\n",
    "# random forest\n",
    "dr_forest = LinearDRLearner(\n",
    "    model_regression=RandomForestRegressor(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "    model_propensity=RandomForestClassifier(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "    cv=5,\n",
    ")\n",
    "dr_forest.fit(y, D, W=W)\n",
    "print(\"-----------------Random Forest in EconML for IRM------------\")\n",
    "print(dr_forest.summary(T=1))\n",
    "\n",
    "# random forest using seperate models for model_regression\n",
    "dr_forest_sep = LinearDRLearner(\n",
    "    model_regression=SeparateModel(\n",
    "        RandomForestRegressor(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "        RandomForestRegressor(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "    ),\n",
    "    model_propensity=RandomForestClassifier(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "    cv=5,\n",
    ")\n",
    "dr_forest_sep.fit(y, D, W=W)\n",
    "print(\"-----------------Random Forest in EconML for IRM (separate models)------------\")\n",
    "print(dr_forest_sep.summary(T=1))\n",
    "\n",
    "# decision tree\n",
    "dr_tree = LinearDRLearner(\n",
    "    model_regression=DecisionTreeRegressor(min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "    model_propensity=DecisionTreeClassifier(min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "    cv=5,\n",
    ")\n",
    "dr_tree.fit(y, D, W=W)\n",
    "print(\"-----------------Decision Tree in EconML for IRM------------\")\n",
    "print(dr_tree.summary(T=1))\n",
    "\n",
    "# decision tree using seperate models for model_regression\n",
    "dr_tree_sep = LinearDRLearner(\n",
    "    model_regression=SeparateModel(\n",
    "        DecisionTreeRegressor(min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "        DecisionTreeRegressor(min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "    ),\n",
    "    model_propensity=DecisionTreeClassifier(min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "    cv=5,\n",
    ")\n",
    "dr_tree_sep.fit(y, D, W=W)\n",
    "print(\"-----------------Decision Tree in EconML for IRM (separate models)------------\")\n",
    "print(dr_tree_sep.summary(T=1))\n",
    "\n",
    "# gradient boosting\n",
    "dr_gb = LinearDRLearner(\n",
    "    model_regression=GradientBoostingRegressor(max_depth=2, n_iter_no_change=5, random_state=123),\n",
    "    model_propensity=GradientBoostingClassifier(max_depth=2, n_iter_no_change=5, random_state=123),\n",
    "    cv=5,\n",
    ")\n",
    "dr_gb.fit(y, D, W=W)\n",
    "print(\"-----------------Gradient Boosting in EconML for IRM------------\")\n",
    "print(dr_gb.summary(T=1))\n",
    "\n",
    "# gradient boosting using seperate models for model_regression\n",
    "dr_gb_sep = LinearDRLearner(\n",
    "    model_regression=SeparateModel(\n",
    "        GradientBoostingRegressor(max_depth=2, n_iter_no_change=5, random_state=123),\n",
    "        GradientBoostingRegressor(max_depth=2, n_iter_no_change=5, random_state=123),\n",
    "    ),\n",
    "    model_propensity=GradientBoostingClassifier(max_depth=2, n_iter_no_change=5, random_state=123),\n",
    "    cv=5,\n",
    ")\n",
    "dr_gb_sep.fit(y, D, W=W)\n",
    "print(\"-----------------Gradient Boosting in EconML for IRM (separate models)------------\")\n",
    "print(dr_gb_sep.summary(T=1))\n",
    "# irm with double ml\n",
    "\n",
    "# random forest\n",
    "dml_irm_rf = dbml.DoubleMLIRM(\n",
    "    dml_data,\n",
    "    RandomForestRegressor(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "    RandomForestClassifier(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "    n_folds=5,\n",
    ")\n",
    "dml_irm_rf.fit()\n",
    "print(\"-----------------Random Forest in DoubleML for IRM------------\")\n",
    "print(dml_irm_rf.summary)\n",
    "\n",
    "# decision tree\n",
    "\n",
    "dml_irm_dt = dbml.DoubleMLIRM(\n",
    "    dml_data,\n",
    "    DecisionTreeRegressor(min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "    DecisionTreeClassifier(min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "    n_folds=5,\n",
    ")\n",
    "dml_irm_dt.fit()\n",
    "print(\"-----------------Decision Tree in DoubleML for IRM------------\")\n",
    "print(dml_irm_dt.summary)\n",
    "# gradient boosting\n",
    "\n",
    "dml_irm_gb = dbml.DoubleMLIRM(\n",
    "    dml_data,\n",
    "    GradientBoostingRegressor(max_depth=2, n_iter_no_change=5, random_state=123),\n",
    "    GradientBoostingClassifier(max_depth=2, n_iter_no_change=5, \n",
    "                               \n",
    "                               random_state=123),\n",
    "    n_folds=5,\n",
    ")\n",
    "dml_irm_gb.fit()\n",
    "print(\"-----------------Gradient Boosting in DoubleML for IRM------------\")\n",
    "print(dml_irm_gb.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Econml can work with most of the base learners (random forest, decision tree, boosted forest), as can doubleML. Both can work with any scikit-learn model in fact, so long as they implement fit() and predict() functions, according to the documentation. Theoretically, one could thus also build a custom class that implements the scikit api for stacking or semi crossfitting with choosing the best model, but neither library can work directly with stacking or perform the semi cross-fitting with the custom implementations we built as far as I could tell from the documentation of the packages. In theory one could write custom scikit-learn compatible interfaces for the custom implementations, but that would be a lot of work.  \n",
    "  \n",
    "In practice however, I found that double lasso did not work with the doubleML library. Upon closer examination, I found a section of the econml docs that explains that it specifically handles cases of models that do hyperparameter searches internally across folds. This is not the case for the doubleML library, which needs all folds to be treated strictly independently of each other. This is a fundamental difference in the way the two libraries are built, and it means that doubleML cannot be used with double lasso.  \n",
    "   \n",
    "As for the results, for PLR (note that results may change slightly upon rerunning/exporting due to randomness. Estimate first, then standard error in parentheses, see above for other metrics): \n",
    "  \n",
    "For PLR:  \n",
    "\n",
    "double lasso previous:  \n",
    "double lasso econml:  \n",
    "double lasso doubleML:  n/a  \n",
    "  \n",
    "random forest previous:  8905 (1357)  \n",
    "random forest econml:  8603 (1333)   \n",
    "random forest doubleML: 8523 (1346)  \n",
    "  \n",
    "decision tree previous: 9236 (1440)  \n",
    "decision tree econml: 8772 (1449)  \n",
    "decision tree doubleML: 8734 (1455)  \n",
    "  \n",
    "boosted forest previous: 8840 (1334)  \n",
    "boosted forest econml: 9129 (1379)  \n",
    "boosted forest doubleML: 8834 (1366)  \n",
    "\n",
    "For IRM:  \n",
    "random forest previous: 7699 (1159)  \n",
    "random forest econml: 8023 (1121)  \n",
    "random forest doubleML: 7805 (1155)  \n",
    "\n",
    "decision tree previous: 7836 (1255)  \n",
    "decision tree econml: 8352 (1250)  \n",
    "decision tree doubleML: 7714 (1238) \n",
    "\n",
    "boosted forest previous: 8593 (1157)  \n",
    "boosted forest econml: 8118 (1135)  \n",
    "boosted forest doubleML: 8683 (1258)  \n",
    "\n",
    "The results are broadly consistent across the different methods and libraries, including for the decision tree. Apart from the decision tree in IRM, the results are also consistent with the estimates from the custom methods. For EconML and Double ML, the decision tree in the IRM setting is more in line with the other models than for the custom implementation, where it differs significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True ATE in the semi-synthetic world:  7448.65797745209\n",
      "\n",
      "=== Semi-Synthetic Data with n=1000 ===\n",
      "** PLR Results n = 1000 **\n",
      "                             estimate       stderr        lower         upper  \\\n",
      "double lasso              8517.648973  4225.009545   236.630265  16798.667681   \n",
      "lasso/logistic            8272.461258  4309.573350  -174.302508  16719.225024   \n",
      "random forest            10602.450248  3961.635467  2837.644733  18367.255763   \n",
      "decision tree            13991.298730  3971.837178  6206.497862  21776.099598   \n",
      "boosted forest           10005.588683  4173.908236  1824.728540  18186.448826   \n",
      "automl (semi-cfit)       10756.858829  4023.172259  2871.441201  18642.276456   \n",
      "stacked (semi-cfit)       8311.912314  4090.190313   295.139301  16328.685327   \n",
      "select-best (semi-cfit)   8272.461258  4309.573350  -174.302508  16719.225024   \n",
      "\n",
      "                               rmse y    rmse D  accuracy D        error  \\\n",
      "double lasso             58417.628247  0.461692       0.643  1068.990996   \n",
      "lasso/logistic           58417.628247  0.460641       0.647   823.803281   \n",
      "random forest            59753.550402  0.463100       0.653  3153.792271   \n",
      "decision tree            66992.217724  0.518130       0.606  6542.640753   \n",
      "boosted forest           61887.404544  0.463241       0.636  2556.930706   \n",
      "automl (semi-cfit)       60357.047091  0.462988       0.643  3308.200851   \n",
      "stacked (semi-cfit)      57901.902727  0.458441       0.664   863.254337   \n",
      "select-best (semi-cfit)  58417.628247  0.460641       0.647   823.803281   \n",
      "\n",
      "                         rmse E[y|D,X]  rmse E[D|X]  covered  \n",
      "double lasso              14263.844056     0.072905        1  \n",
      "lasso/logistic            14263.844056     0.079331        1  \n",
      "random forest             19815.635039     0.094640        1  \n",
      "decision tree             32591.789121     0.261762        1  \n",
      "boosted forest            23132.879493     0.088935        1  \n",
      "automl (semi-cfit)        20378.464731     0.087887        1  \n",
      "stacked (semi-cfit)       15186.354688     0.067172        1  \n",
      "select-best (semi-cfit)   14263.844056     0.079331        1  \n",
      "** IRM Results n = 1000**\n",
      "                             estimate        stderr         lower  \\\n",
      "lasso/logistic            9162.611121   3967.123470   1387.049120   \n",
      "random forest             9570.811652   3883.919073   1958.330269   \n",
      "decision tree            10329.214024  21065.266604 -30958.708520   \n",
      "boosted forest            7118.253019   3946.062955   -616.030372   \n",
      "automl (semi-cfit)       10088.590473   3589.528453   3053.114706   \n",
      "stacked (semi-cfit)       8386.353381   3775.419442    986.531275   \n",
      "select-best (semi-cfit)   8213.246909   3997.528074    378.091884   \n",
      "\n",
      "                                upper        rmse y    rmse D  accuracy D  \\\n",
      "lasso/logistic           16938.173122  58715.553937  0.464368       0.637   \n",
      "random forest            17183.293035  59439.744639  0.461356       0.653   \n",
      "decision tree            51617.136569  62657.647472  0.518624       0.605   \n",
      "boosted forest           14852.536410  70270.783896  0.463513       0.646   \n",
      "automl (semi-cfit)       17124.066240  60266.606846  0.462159       0.647   \n",
      "stacked (semi-cfit)      15786.175486  58257.959081  0.458797       0.662   \n",
      "select-best (semi-cfit)  16048.401933  58804.552552  0.460641       0.647   \n",
      "\n",
      "                               error  rmse E[y|D,X]  rmse E[D|X]  covered  \n",
      "lasso/logistic           1713.953143   14827.780706     0.084054        1  \n",
      "random forest            2122.153675   19431.514030     0.090651        1  \n",
      "decision tree            2880.556047   27805.749907     0.259744        1  \n",
      "boosted forest            330.404959   41390.043903     0.090588        1  \n",
      "automl (semi-cfit)       2639.932496   18872.000088     0.092433        1  \n",
      "stacked (semi-cfit)       937.695403   15409.665916     0.066557        1  \n",
      "select-best (semi-cfit)   764.588931   15583.377134     0.079331        1  \n",
      "\n",
      "=== Semi-Synthetic Data with n=10000 ===\n",
      "** PLR Results n = 10000 **\n",
      "                            estimate       stderr        lower         upper  \\\n",
      "double lasso             8328.237751  1374.189115  5634.827086  11021.648416   \n",
      "lasso/logistic           8421.858340  1364.484722  5747.468285  11096.248396   \n",
      "random forest            8677.907603  1372.702915  5987.409889  11368.405316   \n",
      "decision tree            8930.101158  1416.057429  6154.628597  11705.573719   \n",
      "boosted forest           8820.094224  1377.452593  6120.287141  11519.901307   \n",
      "automl (semi-cfit)       8609.990341  1357.450718  5949.386933  11270.593748   \n",
      "stacked (semi-cfit)      8703.817416  1363.709597  6030.946606  11376.688226   \n",
      "select-best (semi-cfit)  8758.947586  1367.579254  6078.492248  11439.402925   \n",
      "\n",
      "                               rmse y    rmse D  accuracy D        error  \\\n",
      "double lasso             53439.388399  0.453003      0.6742   879.579773   \n",
      "lasso/logistic           53439.388399  0.453209      0.6735   973.200363   \n",
      "random forest            52899.759220  0.453465      0.6745  1229.249625   \n",
      "decision tree            56329.787896  0.454686      0.6726  1481.443181   \n",
      "boosted forest           53098.120217  0.452781      0.6740  1371.436247   \n",
      "automl (semi-cfit)       52550.690354  0.452977      0.6736  1161.332363   \n",
      "stacked (semi-cfit)      52598.288764  0.452324      0.6733  1255.159439   \n",
      "select-best (semi-cfit)  52896.642153  0.453052      0.6733  1310.289609   \n",
      "\n",
      "                         rmse E[y|D,X]  rmse E[D|X]  covered  \n",
      "double lasso              13449.812040     0.042480        1  \n",
      "lasso/logistic            13449.812040     0.046176        1  \n",
      "random forest             12448.905121     0.039425        1  \n",
      "decision tree             23097.733896     0.048233        1  \n",
      "boosted forest            12191.884627     0.035313        1  \n",
      "automl (semi-cfit)        10312.460079     0.036829        1  \n",
      "stacked (semi-cfit)       10464.052081     0.029721        1  \n",
      "select-best (semi-cfit)   12356.465330     0.036035        1  \n",
      "** IRM Results n = 10000**\n",
      "                            estimate       stderr        lower         upper  \\\n",
      "lasso/logistic           8001.664717  1424.009553  5210.605993  10792.723440   \n",
      "random forest            7795.154973  1373.270836  5103.544134  10486.765811   \n",
      "decision tree            8782.442144  1630.360038  5586.936470  11977.947819   \n",
      "boosted forest           7699.717915  1351.719724  5050.347257  10349.088574   \n",
      "automl (semi-cfit)       7897.494559  1357.584212  5236.629504  10558.359614   \n",
      "stacked (semi-cfit)      7602.326745  1376.034677  4905.298778  10299.354712   \n",
      "select-best (semi-cfit)  7824.015958  1371.371237  5136.128333  10511.903582   \n",
      "\n",
      "                               rmse y    rmse D  accuracy D        error  \\\n",
      "lasso/logistic           53345.789082  0.453119      0.6736   553.006739   \n",
      "random forest            52511.316597  0.453282      0.6734   346.496995   \n",
      "decision tree            56603.988037  0.454686      0.6726  1333.784167   \n",
      "boosted forest           53151.233093  0.453066      0.6749   251.059938   \n",
      "automl (semi-cfit)       52302.284877  0.452977      0.6736   448.836582   \n",
      "stacked (semi-cfit)      52219.142610  0.452387      0.6735   153.668767   \n",
      "select-best (semi-cfit)  52520.255696  0.453189      0.6745   375.357980   \n",
      "\n",
      "                         rmse E[y|D,X]  rmse E[D|X]  covered  \n",
      "lasso/logistic            12899.019583     0.047368        1  \n",
      "random forest             10293.674965     0.038772        1  \n",
      "decision tree             22778.574601     0.048233        1  \n",
      "boosted forest            12346.403614     0.035904        1  \n",
      "automl (semi-cfit)         8955.346612     0.036829        1  \n",
      "stacked (semi-cfit)        8542.872216     0.030056        1  \n",
      "select-best (semi-cfit)   10280.876241     0.034990        1  \n",
      "\n",
      "=== Semi-Synthetic Data with n=50000 ===\n",
      "** PLR Results n = 50000 **\n",
      "                            estimate      stderr        lower        upper  \\\n",
      "double lasso             8615.517657  573.957976  7490.560023  9740.475291   \n",
      "lasso/logistic           8617.674768  572.625534  7495.328721  9740.020815   \n",
      "random forest            8228.037517  571.094388  7108.692517  9347.382518   \n",
      "decision tree            8362.260201  589.717808  7206.413298  9518.107104   \n",
      "boosted forest           8480.637757  564.173545  7374.857609  9586.417905   \n",
      "automl (semi-cfit)       8403.941571  565.668437  7295.231434  9512.651707   \n",
      "stacked (semi-cfit)      8397.717480  563.711334  7292.843265  9502.591695   \n",
      "select-best (semi-cfit)  8469.708924  564.093248  7364.086158  9575.331690   \n",
      "\n",
      "                               rmse y    rmse D  accuracy D        error  \\\n",
      "double lasso             53859.225607  0.452819     0.67016  1166.859680   \n",
      "lasso/logistic           53859.225607  0.452796     0.67096  1169.016791   \n",
      "random forest            53911.592975  0.453626     0.67092   779.379540   \n",
      "decision tree            56066.736622  0.454101     0.67018   913.602224   \n",
      "boosted forest           53086.914893  0.451944     0.67260  1031.979780   \n",
      "automl (semi-cfit)       52971.848547  0.452844     0.67110   955.283593   \n",
      "stacked (semi-cfit)      53024.353294  0.451825     0.67230   949.059502   \n",
      "select-best (semi-cfit)  53120.451678  0.451927     0.67318  1021.050947   \n",
      "\n",
      "                         rmse E[y|D,X]  rmse E[D|X]  covered  \n",
      "double lasso              13548.220234     0.034404        0  \n",
      "lasso/logistic            13548.220234     0.034362        0  \n",
      "random forest             13995.677057     0.046922        1  \n",
      "decision tree             20955.541176     0.046633        1  \n",
      "boosted forest            10468.670881     0.025600        1  \n",
      "automl (semi-cfit)         9668.646273     0.038258        1  \n",
      "stacked (semi-cfit)       10109.447491     0.024626        1  \n",
      "select-best (semi-cfit)   10595.289186     0.025690        1  \n",
      "** IRM Results n = 50000**\n",
      "                            estimate      stderr        lower        upper  \\\n",
      "lasso/logistic           8092.235156  689.901036  6740.029125  9444.441187   \n",
      "random forest            7545.961244  571.628150  6425.570070  8666.352419   \n",
      "decision tree            7274.972191  643.884260  6012.959042  8536.985340   \n",
      "boosted forest           7707.051249  591.730956  6547.258576  8866.843923   \n",
      "automl (semi-cfit)       7934.739426  658.395286  6644.284665  9225.194187   \n",
      "stacked (semi-cfit)      7698.999696  611.733161  6500.002701  8897.996691   \n",
      "select-best (semi-cfit)  7868.072769  592.459239  6706.852660  9029.292878   \n",
      "\n",
      "                               rmse y    rmse D  accuracy D       error  \\\n",
      "lasso/logistic           53627.385809  0.452776     0.67138  643.577179   \n",
      "random forest            53105.420530  0.453582     0.67060   97.303267   \n",
      "decision tree            56173.424440  0.454101     0.67018  173.685786   \n",
      "boosted forest           52879.804166  0.452031     0.67316  258.393272   \n",
      "automl (semi-cfit)       52650.456528  0.453015     0.66992  486.081448   \n",
      "stacked (semi-cfit)      52597.264332  0.451892     0.67312  250.341719   \n",
      "select-best (semi-cfit)  52851.871834  0.451930     0.67250  419.414792   \n",
      "\n",
      "                         rmse E[y|D,X]  rmse E[D|X]  covered  \n",
      "lasso/logistic            12605.213283     0.034224        1  \n",
      "random forest             10189.781111     0.046769        1  \n",
      "decision tree             20678.160349     0.046633        1  \n",
      "boosted forest             9164.746238     0.026419        1  \n",
      "automl (semi-cfit)         7556.373772     0.039669        1  \n",
      "stacked (semi-cfit)        7460.936520     0.024567        1  \n",
      "select-best (semi-cfit)    9104.656134     0.025404        1  \n"
     ]
    }
   ],
   "source": [
    "# d.) \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_predict, KFold\n",
    "from sklearn.base import clone\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class semisynth:\n",
    "    \n",
    "    def fit(self, X, D, y, transformer, random_state=None):\n",
    "        \"\"\"\n",
    "        X, D, y: the real data\n",
    "        transformer: any sklearn-compatible Transformer for pre-processing\n",
    "        \"\"\"\n",
    "        self.X_ = X.copy()\n",
    "\n",
    "        # Model for Y|D=0\n",
    "        self.est0_ = make_pipeline(transformer,\n",
    "                                   RandomForestRegressor(min_samples_leaf=20,\n",
    "                                                         ccp_alpha=0.001,\n",
    "                                                         random_state=random_state)\n",
    "                                  ).fit(X[D==0], y[D==0])\n",
    "        self.res0_ = y[D==0] - self.est0_.predict(X[D==0])\n",
    "        # De-mean the residual distribution\n",
    "        self.res0_ -= np.mean(self.res0_)\n",
    "\n",
    "        # Model for Y|D=1\n",
    "        self.est1_ = make_pipeline(transformer,\n",
    "                                   RandomForestRegressor(min_samples_leaf=20,\n",
    "                                                         ccp_alpha=0.001,\n",
    "                                                         random_state=random_state)\n",
    "                                  ).fit(X[D==1], y[D==1])\n",
    "        self.res1_ = y[D==1] - self.est1_.predict(X[D==1])\n",
    "        self.res1_ -= np.mean(self.res1_)\n",
    "\n",
    "        # Model for D|X\n",
    "        self.prop_ = make_pipeline(transformer,\n",
    "                                   RandomForestClassifier(min_samples_leaf=20,\n",
    "                                                          ccp_alpha=0.001,\n",
    "                                                          random_state=random_state)\n",
    "                                  ).fit(X, D)\n",
    "        return self\n",
    "\n",
    "    def generate_data(self, n):\n",
    "        \"\"\"\n",
    "        Returns (X, D, Y, Y1, Y0):\n",
    "          X, D, Y: the new sample\n",
    "          Y1, Y0: potential outcomes for each row\n",
    "        \"\"\"\n",
    "        # Resample X from the empirical distribution\n",
    "        X = self.X_.iloc[np.random.choice(self.X_.shape[0], n, replace=True)]\n",
    "        \n",
    "        # Simulate D ~ Bernoulli(\\hat{p}(X))\n",
    "        pX = self.prop_.predict_proba(X)[:, 1]\n",
    "        D = np.random.binomial(1, pX)\n",
    "\n",
    "        # Construct Y0, Y1 by re-sampling from residual distribution\n",
    "        y0 = self.est0_.predict(X) + self.res0_[np.random.choice(self.res0_.shape[0], n, replace=True)]\n",
    "        y1 = self.est1_.predict(X) + self.res1_[np.random.choice(self.res1_.shape[0], n, replace=True)]\n",
    "        \n",
    "        # Observed Y\n",
    "        y = y0*(1 - D) + y1*D\n",
    "        return X, D, y, y1, y0\n",
    "    \n",
    "    def y_cef(self, X, D):\n",
    "        \"\"\"\n",
    "        Returns the 'true' E[Y|X, D] in the semi-synthetic world\n",
    "        = the random forest predictions from the original data\n",
    "        \"\"\"\n",
    "        return self.est1_.predict(X)*D + self.est0_.predict(X)*(1 - D)\n",
    "    \n",
    "    def D_cef(self, X):\n",
    "        \"\"\"\n",
    "        Returns the 'true' E[D|X] in the semi-synthetic world\n",
    "        \"\"\"\n",
    "        return self.prop_.predict_proba(X)[:, 1]\n",
    "\n",
    "    @property\n",
    "    def true_ate(self):\n",
    "        \"\"\"\n",
    "        The 'true' ATE in the semi-synthetic world, i.e. E[f1(X) - f0(X)]\n",
    "        using the entire original X_ distribution.\n",
    "        \"\"\"\n",
    "        return np.mean(self.est1_.predict(self.X_) - self.est0_.predict(self.X_))\n",
    "\n",
    "\n",
    "def summary(\n",
    "    point, stderr,\n",
    "    yhat, Dhat,    # cross-fitted predictions for y and D\n",
    "    resy, resD,    # residuals y-yhat, D-Dhat\n",
    "    final_residual, # epsilon or drhat\n",
    "    X, D, y,\n",
    "    *,\n",
    "    name,\n",
    "    synth  # the semisynth object, so we can compare to the \"true\" functions\n",
    "):\n",
    "    true_ate = synth.true_ate\n",
    "    covered = (point - 1.96*stderr <= true_ate <= point + 1.96*stderr)\n",
    "\n",
    "    # We'll compute the \"true\" E[Y|D,X], E[D|X]\n",
    "    y_cef_true = synth.y_cef(X, D)\n",
    "    d_cef_true = synth.D_cef(X)\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'estimate':    [point],\n",
    "        'stderr':      [stderr],\n",
    "        'lower':       [point - 1.96*stderr],\n",
    "        'upper':       [point + 1.96*stderr],\n",
    "        'rmse y':      [np.sqrt(np.mean(resy**2))],  # RMSE vs. *observed* Y\n",
    "        'rmse D':      [np.sqrt(np.mean(resD**2))],  # RMSE vs. *observed* D\n",
    "        'accuracy D':  [np.mean(np.abs(resD) < 0.5)],# classification accuracy\n",
    "        # New columns:\n",
    "        'error':       [abs(point - true_ate)],    # how far from true\n",
    "        'rmse E[y|D,X]':[np.sqrt(np.mean((yhat - y_cef_true)**2))],\n",
    "        'rmse E[D|X]': [np.sqrt(np.mean((Dhat - d_cef_true)**2))],\n",
    "        'covered':     [1 if covered else 0]       # did CI cover true ATE?\n",
    "    }, index=[name])\n",
    "    \n",
    "    \n",
    "from copy import deepcopy\n",
    "\n",
    "synth = semisynth().fit(X, D, y, transformer, random_state=123)\n",
    "\n",
    "\n",
    "\n",
    "def run_plr_methods(X_train, D_train, y_train, synth):\n",
    "    \"\"\"\n",
    "    X_train, D_train, y_train come from synth.generate_data(...)\n",
    "    We'll replicate your DML approach for partial linear model\n",
    "    with multiple model combos and stack them in a table.\n",
    "    \"\"\"\n",
    "    results_table = []\n",
    "\n",
    "    # 1) Double Lasso with cross-fitting\n",
    "    # (a) specify pipelines\n",
    "    lassoy_ = deepcopy(lassoy)\n",
    "    lassod_ = deepcopy(lassod)\n",
    "    # (b) run\n",
    "    point, stderr, yhat, Dhat, resy, resD, eps = dml(X_train, D_train, y_train, lassoy_, lassod_, nfolds=5)\n",
    "    # (c) summary\n",
    "    df_ = summary(point, stderr, yhat, Dhat, resy, resD, eps,\n",
    "                  X_train, D_train, y_train, name='double lasso', synth=synth)\n",
    "    results_table.append(df_)\n",
    "\n",
    "    # 2) lasso / logistic\n",
    "    lassoy_ = deepcopy(lassoy)\n",
    "    lgrd_   = deepcopy(lgrd)\n",
    "    point, stderr, yhat, Dhat, resy, resD, eps = dml(X_train, D_train, y_train,\n",
    "                                                     lassoy_, lgrd_, nfolds=5, classifier=True)\n",
    "    df_ = summary(point, stderr, yhat, Dhat, resy, resD, eps,\n",
    "                  X_train, D_train, y_train, name='lasso/logistic', synth=synth)\n",
    "    results_table.append(df_)\n",
    "\n",
    "    # 3) Random Forest\n",
    "    rfy_ = deepcopy(rfy)\n",
    "    rfd_ = deepcopy(rfd)\n",
    "    point, stderr, yhat, Dhat, resy, resD, eps = dml(X_train, D_train, y_train,\n",
    "                                                     rfy_, rfd_, nfolds=5, classifier=True)\n",
    "    df_ = summary(point, stderr, yhat, Dhat, resy, resD, eps,\n",
    "                  X_train, D_train, y_train, name='random forest', synth=synth)\n",
    "    results_table.append(df_)\n",
    "\n",
    "    # 4) Decision Tree\n",
    "    dtry_ = deepcopy(dtry)\n",
    "    dtrd_ = deepcopy(dtrd)\n",
    "    point, stderr, yhat, Dhat, resy, resD, eps = dml(X_train, D_train, y_train,\n",
    "                                                     dtry_, dtrd_, nfolds=5, classifier=True)\n",
    "    df_ = summary(point, stderr, yhat, Dhat, resy, resD, eps,\n",
    "                  X_train, D_train, y_train, name='decision tree', synth=synth)\n",
    "    results_table.append(df_)\n",
    "\n",
    "    # 5) Boosted trees\n",
    "    gbfy_ = deepcopy(gbfy)\n",
    "    gbfd_ = deepcopy(gbfd)\n",
    "    point, stderr, yhat, Dhat, resy, resD, eps = dml(X_train, D_train, y_train,\n",
    "                                                     gbfy_, gbfd_, nfolds=5, classifier=True)\n",
    "    df_ = summary(point, stderr, yhat, Dhat, resy, resD, eps,\n",
    "                  X_train, D_train, y_train, name='boosted forest', synth=synth)\n",
    "    results_table.append(df_)\n",
    "\n",
    "    # 6) automl (semi-cfit)\n",
    "    # Similarly for stacking (semi-cfit).\n",
    "    flamly_ = make_pipeline(transformer, AutoML(time_budget=50,\n",
    "                                                task='regression',\n",
    "                                                early_stop=True,\n",
    "                                                eval_method='cv',\n",
    "                                                n_splits=3,\n",
    "                                                metric='r2',\n",
    "                                                verbose=0))\n",
    "    flamld_ = make_pipeline(transformer, AutoML(time_budget=50,\n",
    "                                                task='classification',\n",
    "                                                early_stop=True,\n",
    "                                                eval_method='cv',\n",
    "                                                n_splits=3,\n",
    "                                                metric='r2',\n",
    "                                                verbose=0))\n",
    "    # Fit Y, D on entire X_train\n",
    "    flamly_.fit(X_train, y_train)\n",
    "    besty_model = flamly_[-1].best_model_for_estimator(flamly_[-1].best_estimator)\n",
    "    besty = make_pipeline(transformer, clone(besty_model))\n",
    "\n",
    "    flamld_.fit(X_train, D_train)\n",
    "    bestd_model = flamld_[-1].best_model_for_estimator(flamld_[-1].best_estimator)\n",
    "    bestd = make_pipeline(transformer, clone(bestd_model))\n",
    "\n",
    "    point, stderr, yhat, Dhat, resy, resD, eps = dml(X_train, D_train, y_train,\n",
    "                                                     besty, bestd, nfolds=5, classifier=True)\n",
    "    df_ = summary(point, stderr, yhat, Dhat, resy, resD, eps,\n",
    "                  X_train, D_train, y_train, name='automl (semi-cfit)', synth=synth)\n",
    "    results_table.append(df_)\n",
    "\n",
    "    # 7) stacking (semi-cfit)\n",
    "    point, stderr, yhat, Dhat, resy, resD, eps = dml_dirty(\n",
    "        X_train, D_train, y_train,\n",
    "        [lassoy, rfy, dtry, gbfy],\n",
    "        [lgrd, rfd, dtrd, gbfd],\n",
    "        nfolds=5, classifier=True\n",
    "    )\n",
    "    df_ = summary(point, stderr, yhat, Dhat, resy, resD, eps,\n",
    "                  X_train, D_train, y_train, name='stacked (semi-cfit)', synth=synth)\n",
    "    results_table.append(df_)\n",
    "    \n",
    "    #8 select best\n",
    "    point, stderr, yhat, Dhat, resy, resD, eps = dml_select_best(\n",
    "        X_train, D_train, y_train,\n",
    "        [lassoy, rfy, dtry, gbfy],\n",
    "        [lgrd, rfd, dtrd, gbfd],\n",
    "        nfolds=5, classifier=True\n",
    "    )\n",
    "    df_ = summary(point, stderr, yhat, Dhat, resy, resD, eps,\n",
    "                    X_train, D_train, y_train, name='select-best (semi-cfit)', synth=synth)\n",
    "    results_table.append(df_)\n",
    "\n",
    "    return pd.concat(results_table)\n",
    "\n",
    "\n",
    "def run_irm_methods(X_train, D_train, y_train, synth):\n",
    "    \"\"\"\n",
    "    X_train, D_train, y_train from synth.generate_data(...)\n",
    "    We'll replicate your IRM approach with multiple model combos and stack in a table.\n",
    "    \"\"\"\n",
    "    results_table = []\n",
    "\n",
    "    # 1) lasso-lasso, logistic repeated seeds + median aggregator\n",
    "    # We'll do a single run for demonstration:\n",
    "    lassoy_ = deepcopy(lassoytest)  # or define a pipeline as in the notebook\n",
    "    lgrd_   = deepcopy(lgrdtest)\n",
    "\n",
    "    point, stderr, yhat, Dhat, resy, resD, drhat = dr(X_train, D_train, y_train,\n",
    "                                                      lassoy_, lassoy_,\n",
    "                                                      lgrd_, nfolds=5)\n",
    "    df_ = summary(point, stderr, yhat, Dhat, resy, resD, drhat,\n",
    "                  X_train, D_train, y_train, name='lasso/logistic', synth=synth)\n",
    "    results_table.append(df_)\n",
    "\n",
    "    # 2) random forest\n",
    "    rfy_ = deepcopy(rfy)\n",
    "    rfd_ = deepcopy(rfd)\n",
    "    point, stderr, yhat, Dhat, resy, resD, drhat = dr(X_train, D_train, y_train,\n",
    "                                                      rfy_, rfy_, rfd_, nfolds=5)\n",
    "    df_ = summary(point, stderr, yhat, Dhat, resy, resD, drhat,\n",
    "                  X_train, D_train, y_train, name='random forest', synth=synth)\n",
    "    results_table.append(df_)\n",
    "\n",
    "    # 3) decision tree\n",
    "    dtry_ = deepcopy(dtry)\n",
    "    dtrd_ = deepcopy(dtrd)\n",
    "    point, stderr, yhat, Dhat, resy, resD, drhat = dr(X_train, D_train, y_train,\n",
    "                                                      dtry_, dtry_, dtrd_, nfolds=5)\n",
    "    df_ = summary(point, stderr, yhat, Dhat, resy, resD, drhat,\n",
    "                  X_train, D_train, y_train, name='decision tree', synth=synth)\n",
    "    results_table.append(df_)\n",
    "\n",
    "    # 4) boosted forest\n",
    "    gbfy_ = deepcopy(gbfy)\n",
    "    gbfd_ = deepcopy(gbfd)\n",
    "    point, stderr, yhat, Dhat, resy, resD, drhat = dr(X_train, D_train, y_train,\n",
    "                                                      gbfy_, gbfy_, gbfd_, nfolds=5)\n",
    "    df_ = summary(point, stderr, yhat, Dhat, resy, resD, drhat,\n",
    "                  X_train, D_train, y_train, name='boosted forest', synth=synth)\n",
    "    results_table.append(df_)\n",
    "\n",
    "    # 5) automl\n",
    "    flamly0_ = make_pipeline(transformer, AutoML(time_budget=30, task='regression', early_stop=True,\n",
    "                                                eval_method='cv', n_splits=3, metric='r2', verbose=0))\n",
    "    flamly1_ = make_pipeline(transformer, AutoML(time_budget=30, task='regression', early_stop=True,\n",
    "                                                eval_method='cv', n_splits=3, metric='r2', verbose=0))\n",
    "    flamld_  = make_pipeline(transformer, AutoML(time_budget=30, task='classification', early_stop=True,\n",
    "                                                eval_method='cv', n_splits=3, metric='r2', verbose=0))\n",
    "    # Fit for Y|D=0, Y|D=1\n",
    "    flamly0_.fit(X_train[D_train == 0], y_train[D_train == 0])\n",
    "    besty0_model = flamly0_[-1].best_model_for_estimator(flamly0_[-1].best_estimator)\n",
    "    besty0 = make_pipeline(transformer, clone(besty0_model))\n",
    "\n",
    "    flamly1_.fit(X_train[D_train == 1], y_train[D_train == 1])\n",
    "    besty1_model = flamly1_[-1].best_model_for_estimator(flamly1_[-1].best_estimator)\n",
    "    besty1 = make_pipeline(transformer, clone(besty1_model))\n",
    "\n",
    "    flamld_.fit(X_train, D_train)\n",
    "    bestd_model = flamld_[-1].best_model_for_estimator(flamld_[-1].best_estimator)\n",
    "    bestd = make_pipeline(transformer, clone(bestd_model))\n",
    "\n",
    "    point, stderr, yhat, Dhat, resy, resD, drhat = dr(X_train, D_train, y_train,\n",
    "                                                      besty0, besty1, bestd,\n",
    "                                                      nfolds=5)\n",
    "    df_ = summary(point, stderr, yhat, Dhat, resy, resD, drhat,\n",
    "                  X_train, D_train, y_train, name='automl (semi-cfit)', synth=synth)\n",
    "    results_table.append(df_)\n",
    "\n",
    "    # 6) stacking (semi-cfit):\n",
    "    lassoy_ = deepcopy(lassoy)\n",
    "    rfy_ = deepcopy(rfy)\n",
    "    dtry_ = deepcopy(dtry)\n",
    "    gbfy_ = deepcopy(gbfy)\n",
    "\n",
    "    lgrd_ = deepcopy(lgrd)\n",
    "    rfd_  = deepcopy(rfd)\n",
    "    dtrd_ = deepcopy(dtrd)\n",
    "    gbfd_ = deepcopy(gbfd)\n",
    "\n",
    "    point, stderr, yhat, Dhat, resy, resD, drhat = dr_dirty(\n",
    "        X_train, D_train, y_train,\n",
    "        [lassoy_, rfy_, dtry_, gbfy_],\n",
    "        [lassoy_, rfy_, dtry_, gbfy_],\n",
    "        [lgrd_, rfd_, dtrd_, gbfd_],\n",
    "        nfolds=5\n",
    "    )\n",
    "    df_ = summary(point, stderr, yhat, Dhat, resy, resD, drhat,\n",
    "                  X_train, D_train, y_train, name='stacked (semi-cfit)', synth=synth)\n",
    "    results_table.append(df_)\n",
    "    \n",
    "    # 7) select best\n",
    "    \n",
    "    point, stderr, yhat, Dhat, resy, resD, drhat = dr_select_best(\n",
    "        X_train, D_train, y_train,\n",
    "        [lassoy, rfy, dtry, gbfy],\n",
    "        [lassoy, rfy, dtry, gbfy],\n",
    "        [lgrd, rfd, dtrd, gbfd],\n",
    "        nfolds=5\n",
    "    )\n",
    "    df_ = summary(point, stderr, yhat, Dhat, resy, resD, drhat,\n",
    "                    X_train, D_train, y_train, name='select-best (semi-cfit)', synth=synth)\n",
    "    results_table.append(df_)\n",
    "\n",
    "    return pd.concat(results_table)\n",
    "\n",
    "\n",
    "for n in [1000, 10000, 50000]:\n",
    "    print(f\"\\n=== Semi-Synthetic Data with n={n} ===\")\n",
    "    print(\"True ATE in the semi-synthetic world: \", synth.true_ate)\n",
    "    X_synth, D_synth, y_synth, y1_synth, y0_synth = synth.generate_data(n)\n",
    "\n",
    "\n",
    "    print(f\"** PLR Results n = {n} **\")\n",
    "    print(\"True ATE in the semi-synthetic world: \", synth.true_ate)\n",
    "    table_plr = run_plr_methods(X_synth, D_synth, y_synth, synth)\n",
    "    print(table_plr)\n",
    "\n",
    "    print(f\"** IRM Results n = {n}**\")\n",
    "    table_irm = run_irm_methods(X_synth, D_synth, y_synth, synth)\n",
    "    print(table_irm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For n = 1000:  \n",
    "In the PLR setting, lasso/logistic regression outperforms all other methods in terms of the estimate. \n",
    "\n",
    "In the IRM setting, the boosted forest outperforms all other methods in terms of the estimate. \n",
    "\n",
    "So neither automl nor stacking perform as well as the best model alone.\n",
    "\n",
    "For n = 10000:\n",
    "\n",
    "In the PLR setting, the double Lasso performs best in terms of the estimate. All models exhibit upward bias in the estimate.  \n",
    "In the IRM setting, stacked semi-crossfitting performs best in terms of the estimate. All models exhibit upward bias in the estimate.\n",
    "So stacking performs as well as the best model alone, which in this case is Random Forest.  \n",
    "\n",
    "For n = 50000:\n",
    "\n",
    "In the PLR setting, Random Forest performs best in terms of the estimate. All models exhibit upward bias in the estimate.  \n",
    "In the IRM setting, Random Forest performs best in terms of the estimate. Not all models exhibit upward bias in the estimate (decision tree is below true ATE).  \n",
    "Neither automl nor stacking perform as well as the best model alone.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs288_alt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
