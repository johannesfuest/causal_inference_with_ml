{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1\n",
    "\n",
    "We start with the PLR part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTHONWARNINGS=ignore\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>estimate</th>\n",
       "      <th>stderr</th>\n",
       "      <th>lower</th>\n",
       "      <th>upper</th>\n",
       "      <th>rmse y</th>\n",
       "      <th>rmse D</th>\n",
       "      <th>accuracy D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>double lasso</th>\n",
       "      <td>9035.120004</td>\n",
       "      <td>1295.135748</td>\n",
       "      <td>6496.653938</td>\n",
       "      <td>11573.586070</td>\n",
       "      <td>54254.468883</td>\n",
       "      <td>0.443406</td>\n",
       "      <td>0.688553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lasso/logistic</th>\n",
       "      <td>9092.508157</td>\n",
       "      <td>1304.398170</td>\n",
       "      <td>6535.887743</td>\n",
       "      <td>11649.128571</td>\n",
       "      <td>54254.468883</td>\n",
       "      <td>0.444043</td>\n",
       "      <td>0.687847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random forest</th>\n",
       "      <td>8826.461380</td>\n",
       "      <td>1354.715815</td>\n",
       "      <td>6171.218382</td>\n",
       "      <td>11481.704379</td>\n",
       "      <td>54925.512714</td>\n",
       "      <td>0.444586</td>\n",
       "      <td>0.688754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>decision tree</th>\n",
       "      <td>9236.195678</td>\n",
       "      <td>1440.551643</td>\n",
       "      <td>6412.714457</td>\n",
       "      <td>12059.676898</td>\n",
       "      <td>59427.392172</td>\n",
       "      <td>0.446437</td>\n",
       "      <td>0.688048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>boosted forest</th>\n",
       "      <td>9087.370903</td>\n",
       "      <td>1383.338157</td>\n",
       "      <td>6376.028116</td>\n",
       "      <td>11798.713690</td>\n",
       "      <td>56498.174500</td>\n",
       "      <td>0.443402</td>\n",
       "      <td>0.691679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>automl (semi-cfit)</th>\n",
       "      <td>8751.417736</td>\n",
       "      <td>1311.731268</td>\n",
       "      <td>6180.424451</td>\n",
       "      <td>11322.411021</td>\n",
       "      <td>54152.742254</td>\n",
       "      <td>0.443595</td>\n",
       "      <td>0.690973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stacked (semi-cfit)</th>\n",
       "      <td>8921.560919</td>\n",
       "      <td>1308.715491</td>\n",
       "      <td>6356.478558</td>\n",
       "      <td>11486.643281</td>\n",
       "      <td>54007.291672</td>\n",
       "      <td>0.442737</td>\n",
       "      <td>0.690066</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        estimate       stderr        lower         upper  \\\n",
       "double lasso         9035.120004  1295.135748  6496.653938  11573.586070   \n",
       "lasso/logistic       9092.508157  1304.398170  6535.887743  11649.128571   \n",
       "random forest        8826.461380  1354.715815  6171.218382  11481.704379   \n",
       "decision tree        9236.195678  1440.551643  6412.714457  12059.676898   \n",
       "boosted forest       9087.370903  1383.338157  6376.028116  11798.713690   \n",
       "automl (semi-cfit)   8751.417736  1311.731268  6180.424451  11322.411021   \n",
       "stacked (semi-cfit)  8921.560919  1308.715491  6356.478558  11486.643281   \n",
       "\n",
       "                           rmse y    rmse D  accuracy D  \n",
       "double lasso         54254.468883  0.443406    0.688553  \n",
       "lasso/logistic       54254.468883  0.444043    0.687847  \n",
       "random forest        54925.512714  0.444586    0.688754  \n",
       "decision tree        59427.392172  0.446437    0.688048  \n",
       "boosted forest       56498.174500  0.443402    0.691679  \n",
       "automl (semi-cfit)   54152.742254  0.443595    0.690973  \n",
       "stacked (semi-cfit)  54007.291672  0.442737    0.690066  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%env PYTHONWARNINGS=ignore\n",
    "\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=ConvergenceWarning)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter('ignore')\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\".*did not converge.*\",\n",
    "    category=ConvergenceWarning\n",
    ")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_predict, KFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LassoCV, LinearRegression, LogisticRegressionCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.base import TransformerMixin, BaseEstimator, clone\n",
    "from formulaic import Formula\n",
    "from flaml.automl import AutoML\n",
    "np.random.seed(1234)\n",
    "# set random seed for all other libraries\n",
    "import random\n",
    "random.seed(1234)\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '1234'\n",
    "\n",
    "\n",
    "file = \"https://raw.githubusercontent.com/CausalAIBook/MetricsMLNotebooks/main/data/401k.csv\"\n",
    "data = pd.read_csv(file)\n",
    "y = data['net_tfa'].values\n",
    "D = data['e401'].values\n",
    "D2 = data['p401'].values\n",
    "D3 = data['a401'].values\n",
    "X = data.drop(['e401', 'p401', 'a401', 'tw', 'tfa', 'net_tfa', 'tfa_he',\n",
    "               'hval', 'hmort', 'hequity',\n",
    "               'nifa', 'net_nifa', 'net_n401', 'ira',\n",
    "               'dum91', 'icat', 'ecat', 'zhat',\n",
    "               'i1', 'i2', 'i3', 'i4', 'i5', 'i6', 'i7',\n",
    "               'a1', 'a2', 'a3', 'a4', 'a5'], axis=1)\n",
    "\n",
    "class FormulaTransformer(TransformerMixin, BaseEstimator):\n",
    "\n",
    "    def __init__(self, formula, array=False):\n",
    "        self.formula = formula\n",
    "        self.array = array\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        df = Formula(self.formula).get_model_matrix(X)\n",
    "        if self.array:\n",
    "            return df.values\n",
    "        return df\n",
    "transformer = FormulaTransformer(\"0 + poly(age, degree=6, raw=True) + poly(inc, degree=8, raw=True) \"\n",
    "                                 \"+ poly(educ, degree=4, raw=True) + poly(fsize, degree=2, raw=True) \"\n",
    "                                 \"+ male + marr + twoearn + db + pira + hown\", array=True)\n",
    "\n",
    "def dml(X, D, y, modely, modeld, *, nfolds, classifier=False):\n",
    "    '''\n",
    "    DML for the Partially Linear Model setting with cross-fitting\n",
    "\n",
    "    Input\n",
    "    -----\n",
    "    X: the controls\n",
    "    D: the treatment\n",
    "    y: the outcome\n",
    "    modely: the ML model for predicting the outcome y\n",
    "    modeld: the ML model for predicting the treatment D\n",
    "    nfolds: the number of folds in cross-fitting\n",
    "    classifier: bool, whether the modeld is a classifier or a regressor\n",
    "\n",
    "    Output\n",
    "    ------\n",
    "    point: the point estimate of the treatment effect of D on y\n",
    "    stderr: the standard error of the treatment effect\n",
    "    yhat: the cross-fitted predictions for the outcome y\n",
    "    Dhat: the cross-fitted predictions for the treatment D\n",
    "    resy: the outcome residuals\n",
    "    resD: the treatment residuals\n",
    "    epsilon: the final residual-on-residual OLS regression residual\n",
    "    '''\n",
    "    cv = KFold(n_splits=nfolds, shuffle=True, random_state=123)  # shuffled k-folds\n",
    "    yhat = cross_val_predict(modely, X, y, cv=cv, n_jobs=-1)  # out-of-fold predictions for y\n",
    "    # out-of-fold predictions for D\n",
    "    # use predict or predict_proba dependent on classifier or regressor for D\n",
    "    if classifier:\n",
    "        Dhat = cross_val_predict(modeld, X, D, cv=cv, method='predict_proba', n_jobs=-1)[:, 1]\n",
    "    else:\n",
    "        Dhat = cross_val_predict(modeld, X, D, cv=cv, n_jobs=-1)\n",
    "    # calculate outcome and treatment residuals\n",
    "    resy = y - yhat\n",
    "    resD = D - Dhat\n",
    "\n",
    "    # final stage ols based point estimate and standard error\n",
    "    point = np.mean(resy * resD) / np.mean(resD**2)\n",
    "    epsilon = resy - point * resD\n",
    "    var = np.mean(epsilon**2 * resD**2) / np.mean(resD**2)**2\n",
    "    stderr = np.sqrt(var / X.shape[0])\n",
    "\n",
    "    return point, stderr, yhat, Dhat, resy, resD, epsilon\n",
    "\n",
    "def summary(point, stderr, yhat, Dhat, resy, resD, epsilon, X, D, y, *, name):\n",
    "    '''\n",
    "    Convenience summary function that takes the results of the DML function\n",
    "    and summarizes several estimation quantities and performance metrics.\n",
    "    '''\n",
    "    return pd.DataFrame({'estimate': point,  # point estimate\n",
    "                         'stderr': stderr,  # standard error\n",
    "                         'lower': point - 1.96 * stderr,  # lower end of 95% confidence interval\n",
    "                         'upper': point + 1.96 * stderr,  # upper end of 95% confidence interval\n",
    "                         'rmse y': np.sqrt(np.mean(resy**2)),  # RMSE of model that predicts outcome y\n",
    "                         'rmse D': np.sqrt(np.mean(resD**2)),  # RMSE of model that predicts treatment D\n",
    "                         'accuracy D': np.mean(np.abs(resD) < .5),  # binary classification accuracy of model for D\n",
    "                         }, index=[name])\n",
    "# double lasso with cross-fitting\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "lassoy = make_pipeline(transformer, StandardScaler(), LassoCV(cv=cv))\n",
    "lassod = make_pipeline(transformer, StandardScaler(), LassoCV(cv=cv))\n",
    "result = dml(X, D, y, lassoy, lassod, nfolds=5)\n",
    "table = summary(*result, X, D, y, name='double lasso')\n",
    "\n",
    "# penalized logreg for D (default is l2 penalty)\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "lassoy = make_pipeline(transformer, StandardScaler(), LassoCV(cv=cv))\n",
    "lgrd = make_pipeline(transformer, StandardScaler(), LogisticRegressionCV(cv=cv))\n",
    "result = dml(X, D, y, lassoy, lgrd, nfolds=5, classifier=True)\n",
    "result = dml(X, D, y, lassoy, lgrd, nfolds=5, classifier=True)\n",
    "table = pd.concat([table, summary(*result, X, D, y, name='lasso/logistic')])\n",
    "\n",
    "# random forest\n",
    "rfy = make_pipeline(transformer, RandomForestRegressor(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001))\n",
    "rfd = make_pipeline(transformer, RandomForestClassifier(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001))\n",
    "result = dml(X, D, y, rfy, rfd, nfolds=5, classifier=True)\n",
    "table = pd.concat([table, summary(*result, X, D, y, name='random forest')])\n",
    "\n",
    "# decision tree\n",
    "dtry = make_pipeline(transformer, DecisionTreeRegressor(min_samples_leaf=10, ccp_alpha=.001))\n",
    "dtrd = make_pipeline(transformer, DecisionTreeClassifier(min_samples_leaf=10, ccp_alpha=.001))\n",
    "result = dml(X, D, y, dtry, dtrd, nfolds=5, classifier=True)\n",
    "table = pd.concat([table, summary(*result, X, D, y, name='decision tree')])\n",
    "\n",
    "# boosted trees\n",
    "gbfy = make_pipeline(transformer, GradientBoostingRegressor(max_depth=2, n_iter_no_change=5))\n",
    "gbfd = make_pipeline(transformer, GradientBoostingClassifier(max_depth=2, n_iter_no_change=5))\n",
    "result = dml(X, D, y, gbfy, gbfd, nfolds=5, classifier=True)\n",
    "table = pd.concat([table, summary(*result, X, D, y, name='boosted forest')])\n",
    "\n",
    "# semi cross fitting: To avoid the computational cost of performing model selection within each fold (assuming that we don't select among an exponential set of hyperparameters/models in the number of samples), it is ok to perform model selection using all the data and then perform cross-fitting with the selected model\n",
    "flamly = make_pipeline(transformer, AutoML(time_budget=200, task='regression', early_stop=True,\n",
    "                                           eval_method='cv', n_splits=3, metric='r2', verbose=0))\n",
    "flamld = make_pipeline(transformer, AutoML(time_budget=200, task='classification', early_stop=True,\n",
    "                                           eval_method='cv', n_splits=3, metric='r2', verbose=0))\n",
    "flamly.fit(X, y)\n",
    "besty = make_pipeline(transformer, clone(flamly[-1].best_model_for_estimator(flamly[-1].best_estimator)))\n",
    "flamld.fit(X, D)\n",
    "bestd = make_pipeline(transformer, clone(flamld[-1].best_model_for_estimator(flamld[-1].best_estimator)))\n",
    "result = dml(X, D, y, besty, bestd, nfolds=5, classifier=True)\n",
    "table = pd.concat([table, summary(*result, X, D, y, name='automl (semi-cfit)')])\n",
    "\n",
    "# semi cross fitting with stacking\n",
    "def dml_dirty(X, D, y, modely_list, modeld_list, *,\n",
    "              stacker=LinearRegression(), nfolds, classifier=False):\n",
    "    '''\n",
    "    DML for the Partially Linear Model setting with semi-cross-fitting\n",
    "\n",
    "    Input\n",
    "    -----\n",
    "    X: the controls\n",
    "    D: the treatment\n",
    "    y: the outcome\n",
    "    modely: the ML model for predicting the outcome y\n",
    "    modeld: the ML model for predicting the treatment D\n",
    "    stacker: model used to aggregate predictions of each of the base models\n",
    "    nfolds: the number of folds in cross-fitting\n",
    "    classifier: bool, whether the modeld is a classifier or a regressor\n",
    "\n",
    "    Output\n",
    "    ------\n",
    "    point: the point estimate of the treatment effect of D on y\n",
    "    stderr: the standard error of the treatment effect\n",
    "    yhat: the cross-fitted predictions for the outcome y\n",
    "    Dhat: the cross-fitted predictions for the treatment D\n",
    "    resy: the outcome residuals\n",
    "    resD: the treatment residuals\n",
    "    epsilon: the final residual-on-residual OLS regression residual\n",
    "    '''\n",
    "    # construct out-of-fold predictions for each model\n",
    "    cv = KFold(n_splits=nfolds, shuffle=True, random_state=123)\n",
    "    yhats = np.array([cross_val_predict(modely, X, y, cv=cv, n_jobs=-1) for modely in modely_list]).T\n",
    "    if classifier:\n",
    "        Dhats = np.array([cross_val_predict(modeld, X, D, cv=cv, method='predict_proba', n_jobs=-1)[:, 1]\n",
    "                         for modeld in modeld_list]).T\n",
    "    else:\n",
    "        Dhats = np.array([cross_val_predict(modeld, X, D, cv=cv, n_jobs=-1) for modeld in modeld_list]).T\n",
    "    # calculate stacked residuals by finding optimal coefficients\n",
    "    # and weigthing out-of-sample predictions by these coefficients\n",
    "    yhat = stacker.fit(yhats, y).predict(yhats)\n",
    "    Dhat = stacker.fit(Dhats, D).predict(Dhats)\n",
    "    resy = y - yhat\n",
    "    resD = D - Dhat\n",
    "    # go with the stacked residuals\n",
    "    point = np.mean(resy * resD) / np.mean(resD**2)\n",
    "    epsilon = resy - point * resD\n",
    "    var = np.mean(epsilon**2 * resD**2) / np.mean(resD**2)**2\n",
    "    stderr = np.sqrt(var / X.shape[0])\n",
    "    return point, stderr, yhat, Dhat, resy, resD, epsilon\n",
    "\n",
    "result = dml_dirty(X, D, y, [lassoy, rfy, dtry, gbfy], [lgrd, rfd, dtrd, gbfd],\n",
    "                   nfolds=5, classifier=True)\n",
    "table = pd.concat([table, summary(*result, X, D, y, name='stacked (semi-cfit)')])\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Bottom 25% Income Sample ===\n",
      "                                  estimate       stderr        lower  \\\n",
      "bottom25% double lasso         3769.763582  1092.945664  1627.590080   \n",
      "bottom25% lasso/logistic       3803.759038  1072.578658  1701.504869   \n",
      "bottom25% random forest        4365.883758  1082.272281  2244.630087   \n",
      "bottom25% decision tree        3403.100024  1008.827922  1425.797297   \n",
      "bottom25% boosted forest       4051.659840  1112.072646  1871.997453   \n",
      "bottom25% automl (semi-cfit)   3818.527486  1094.432778  1673.439240   \n",
      "bottom25% stacked (semi-cfit)  3939.176531  1107.247758  1768.970925   \n",
      "\n",
      "                                     upper        rmse y    rmse D  accuracy D  \n",
      "bottom25% double lasso         5911.937083  13400.361810  0.343801    0.846433  \n",
      "bottom25% lasso/logistic       5906.013207  13400.361810  0.354799    0.844015  \n",
      "bottom25% random forest        6487.137429  13510.061700  0.346124    0.846030  \n",
      "bottom25% decision tree        5380.402751  14728.635257  0.380472    0.803305  \n",
      "bottom25% boosted forest       6231.322227  13629.668856  0.346010    0.846030  \n",
      "bottom25% automl (semi-cfit)   5963.615731  13480.512942  0.351340    0.835953  \n",
      "bottom25% stacked (semi-cfit)  6109.382138  13372.412559  0.342460    0.847642  \n",
      "\n",
      "=== Top 25% Income Sample ===\n",
      "                                estimate       stderr         lower  \\\n",
      "top25% double lasso         17505.836955  3902.624431   9856.693070   \n",
      "top25% lasso/logistic       18204.194590  3867.405524  10624.079763   \n",
      "top25% random forest        17628.655285  3995.921443   9796.649256   \n",
      "top25% decision tree        15541.699685  3747.898126   8195.819359   \n",
      "top25% boosted forest       16716.805044  3882.931371   9106.259557   \n",
      "top25% automl (semi-cfit)   16797.918288  3874.918822   9203.077398   \n",
      "top25% stacked (semi-cfit)  18235.568741  3919.492075  10553.364273   \n",
      "\n",
      "                                   upper         rmse y    rmse D  accuracy D  \n",
      "top25% double lasso         25154.980841   91393.039963  0.483189    0.601049  \n",
      "top25% lasso/logistic       25784.309418   91393.039963  0.482708    0.601049  \n",
      "top25% random forest        25460.661313   93828.488353  0.484847    0.603469  \n",
      "top25% decision tree        22887.580011  101948.859766  0.544857    0.536103  \n",
      "top25% boosted forest       24327.350530   95141.243406  0.484786    0.599839  \n",
      "top25% automl (semi-cfit)   24392.759178   95109.874876  0.484524    0.604679  \n",
      "top25% stacked (semi-cfit)  25917.773208   91294.404339  0.481494    0.611537  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_bottom = data.loc[data['inc'] <= data['inc'].quantile(0.25)].copy()\n",
    "data_top = data.loc[data['inc'] >= data['inc'].quantile(0.75)].copy()\n",
    "\n",
    "# Define helper function to create X, D, y from a subset\n",
    "def extract_XDy(df):\n",
    "    \"\"\"Given a subset of the 401k data, produce X, D, and y \n",
    "    consistent with the main analysis.\"\"\"\n",
    "    y_ = df['net_tfa'].values\n",
    "    D_ = df['e401'].values\n",
    "    X_ = df.drop([\n",
    "        'e401', 'p401', 'a401', 'tw', 'tfa', 'net_tfa', 'tfa_he',\n",
    "        'hval', 'hmort', 'hequity', 'nifa', 'net_nifa', 'net_n401',\n",
    "        'ira', 'dum91', 'icat', 'ecat', 'zhat', 'i1', 'i2', 'i3',\n",
    "        'i4', 'i5', 'i6', 'i7', 'a1', 'a2', 'a3', 'a4', 'a5'\n",
    "    ], axis=1)\n",
    "    return X_, D_, y_\n",
    "\n",
    "X_bottom, D_bottom, y_bottom = extract_XDy(data_bottom)\n",
    "X_top, D_top, y_top = extract_XDy(data_top)\n",
    "\n",
    "\n",
    "# Create a helper function that runs all models and returns a summary table\n",
    "def run_all_estimators(X, D, y, name_prefix=''):\n",
    "    \"\"\"Runs the pipeline of estimators and returns a summary results table.\"\"\"\n",
    "    table_local = []\n",
    "\n",
    "    # cross-validation setup\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "    \n",
    "    # 1) double lasso\n",
    "    lassoy = make_pipeline(transformer, StandardScaler(), LassoCV(cv=cv))\n",
    "    lassod = make_pipeline(transformer, StandardScaler(), LassoCV(cv=cv))\n",
    "    result = dml(X, D, y, lassoy, lassod, nfolds=5)\n",
    "    table_local.append(summary(*result, X, D, y, name=f'{name_prefix} double lasso'))\n",
    "\n",
    "    # 2) lasso/logistic\n",
    "    lassoy = make_pipeline(transformer, StandardScaler(), LassoCV(cv=cv))\n",
    "    lgrd = make_pipeline(transformer, StandardScaler(), LogisticRegressionCV(cv=cv))\n",
    "    result = dml(X, D, y, lassoy, lgrd, nfolds=5, classifier=True)\n",
    "    table_local.append(summary(*result, X, D, y, name=f'{name_prefix} lasso/logistic'))\n",
    "\n",
    "    # 3) random forest\n",
    "    rfy = make_pipeline(transformer,\n",
    "                        RandomForestRegressor(n_estimators=100,\n",
    "                                              min_samples_leaf=10,\n",
    "                                              ccp_alpha=0.001))\n",
    "    rfd = make_pipeline(transformer,\n",
    "                        RandomForestClassifier(n_estimators=100,\n",
    "                                               min_samples_leaf=10,\n",
    "                                               ccp_alpha=0.001))\n",
    "    result = dml(X, D, y, rfy, rfd, nfolds=5, classifier=True)\n",
    "    table_local.append(summary(*result, X, D, y, name=f'{name_prefix} random forest'))\n",
    "\n",
    "    # 4) decision tree\n",
    "    dtry = make_pipeline(transformer,\n",
    "                         DecisionTreeRegressor(min_samples_leaf=10,\n",
    "                                               ccp_alpha=0.001))\n",
    "    dtrd = make_pipeline(transformer,\n",
    "                         DecisionTreeClassifier(min_samples_leaf=10,\n",
    "                                                ccp_alpha=0.001))\n",
    "    result = dml(X, D, y, dtry, dtrd, nfolds=5, classifier=True)\n",
    "    table_local.append(summary(*result, X, D, y, name=f'{name_prefix} decision tree'))\n",
    "\n",
    "    # 5) boosted trees\n",
    "    gbfy = make_pipeline(transformer,\n",
    "                         GradientBoostingRegressor(max_depth=2,\n",
    "                                                   n_iter_no_change=5))\n",
    "    gbfd = make_pipeline(transformer,\n",
    "                         GradientBoostingClassifier(max_depth=2,\n",
    "                                                    n_iter_no_change=5))\n",
    "    result = dml(X, D, y, gbfy, gbfd, nfolds=5, classifier=True)\n",
    "    table_local.append(summary(*result, X, D, y, name=f'{name_prefix} boosted forest'))\n",
    "\n",
    "    # 6) automl (semi cross-fitting)\n",
    "    flamly = make_pipeline(transformer,\n",
    "                           AutoML(time_budget=60,  # reduce if desired\n",
    "                                  task='regression',\n",
    "                                  early_stop=True,\n",
    "                                  eval_method='cv',\n",
    "                                  n_splits=3,\n",
    "                                  metric='r2',\n",
    "                                  verbose=0))\n",
    "    flamld = make_pipeline(transformer,\n",
    "                           AutoML(time_budget=60,\n",
    "                                  task='classification',\n",
    "                                  early_stop=True,\n",
    "                                  eval_method='cv',\n",
    "                                  n_splits=3,\n",
    "                                  metric='r2',\n",
    "                                  verbose=0))\n",
    "    # Fit once on entire data\n",
    "    flamly.fit(X, y)\n",
    "    besty_model = flamly[-1].best_model_for_estimator(flamly[-1].best_estimator)\n",
    "    besty = make_pipeline(transformer, clone(besty_model))\n",
    "\n",
    "    flamld.fit(X, D)\n",
    "    bestd_model = flamld[-1].best_model_for_estimator(flamld[-1].best_estimator)\n",
    "    bestd = make_pipeline(transformer, clone(bestd_model))\n",
    "\n",
    "    result = dml(X, D, y, besty, bestd, nfolds=5, classifier=True)\n",
    "    table_local.append(summary(*result, X, D, y, name=f'{name_prefix} automl (semi-cfit)'))\n",
    "\n",
    "    # 7) stacked (semi-cfit)\n",
    "    # Re-use the same base models we created, but put them into lists:\n",
    "    # - Notice we must re-create them fresh so they are unfitted before cross_val_predict\n",
    "    #   or cross_val_predict won't do what we expect.\n",
    "    lassoy_ = make_pipeline(transformer, StandardScaler(), LassoCV(cv=cv))\n",
    "    lgrd_ = make_pipeline(transformer, StandardScaler(), LogisticRegressionCV(cv=cv))\n",
    "    rfy_ = make_pipeline(transformer,\n",
    "                         RandomForestRegressor(n_estimators=100,\n",
    "                                               min_samples_leaf=10,\n",
    "                                               ccp_alpha=0.001))\n",
    "    rfd_ = make_pipeline(transformer,\n",
    "                         RandomForestClassifier(n_estimators=100,\n",
    "                                                min_samples_leaf=10,\n",
    "                                                ccp_alpha=0.001))\n",
    "    dtry_ = make_pipeline(transformer,\n",
    "                          DecisionTreeRegressor(min_samples_leaf=10,\n",
    "                                                ccp_alpha=0.001))\n",
    "    dtrd_ = make_pipeline(transformer,\n",
    "                          DecisionTreeClassifier(min_samples_leaf=10,\n",
    "                                                 ccp_alpha=0.001))\n",
    "    gbfy_ = make_pipeline(transformer,\n",
    "                          GradientBoostingRegressor(max_depth=2,\n",
    "                                                    n_iter_no_change=5))\n",
    "    gbfd_ = make_pipeline(transformer,\n",
    "                          GradientBoostingClassifier(max_depth=2,\n",
    "                                                     n_iter_no_change=5))\n",
    "\n",
    "    modely_list = [lassoy_, rfy_, dtry_, gbfy_]\n",
    "    modeld_list = [lgrd_, rfd_, dtrd_, gbfd_]\n",
    "\n",
    "    result = dml_dirty(X, D, y, modely_list, modeld_list,\n",
    "                       stacker=LinearRegression(),\n",
    "                       nfolds=5, classifier=True)\n",
    "    table_local.append(summary(*result, X, D, y, name=f'{name_prefix} stacked (semi-cfit)'))\n",
    "\n",
    "    # Concatenate all results\n",
    "    return pd.concat(table_local)\n",
    "\n",
    "\n",
    "table_bottom = run_all_estimators(X_bottom, D_bottom, y_bottom, name_prefix='bottom25%')\n",
    "table_top = run_all_estimators(X_top, D_top, y_top, name_prefix='top25%')\n",
    "\n",
    "print(\"=== Bottom 25% Income Sample ===\")\n",
    "print(table_bottom)\n",
    "\n",
    "print(\"\\n=== Top 25% Income Sample ===\")\n",
    "print(table_top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the PLR setting, there indeed seems to be heterogeneity in the treatment with respect to income, with the bottom 25% of earners seeing estimates around 3.8k and the top 25% seeing estimates of around 17.5k. The different machine learning models are broadly consistent across all three income groups.  \n",
    "  \n",
    "Next we more to the IRM part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>estimate</th>\n",
       "      <th>stderr</th>\n",
       "      <th>lower</th>\n",
       "      <th>upper</th>\n",
       "      <th>rmse y</th>\n",
       "      <th>rmse D</th>\n",
       "      <th>accuracy D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lasso/logistic</th>\n",
       "      <td>7726.585781</td>\n",
       "      <td>1159.322663</td>\n",
       "      <td>5454.313361</td>\n",
       "      <td>9998.858202</td>\n",
       "      <td>54060.702446</td>\n",
       "      <td>0.444041</td>\n",
       "      <td>0.687948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random forest</th>\n",
       "      <td>7765.368071</td>\n",
       "      <td>1150.546198</td>\n",
       "      <td>5510.297522</td>\n",
       "      <td>10020.438620</td>\n",
       "      <td>55719.962830</td>\n",
       "      <td>0.444489</td>\n",
       "      <td>0.688048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>decision tree</th>\n",
       "      <td>7841.536117</td>\n",
       "      <td>1255.451237</td>\n",
       "      <td>5380.851692</td>\n",
       "      <td>10302.220542</td>\n",
       "      <td>60491.245996</td>\n",
       "      <td>0.446437</td>\n",
       "      <td>0.688048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>boosted forest</th>\n",
       "      <td>8697.815755</td>\n",
       "      <td>1158.203893</td>\n",
       "      <td>6427.736126</td>\n",
       "      <td>10967.895385</td>\n",
       "      <td>55255.465437</td>\n",
       "      <td>0.443394</td>\n",
       "      <td>0.690469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>automl (semi-cfit)</th>\n",
       "      <td>8129.709678</td>\n",
       "      <td>1132.995563</td>\n",
       "      <td>5909.038374</td>\n",
       "      <td>10350.380982</td>\n",
       "      <td>55338.022903</td>\n",
       "      <td>0.443595</td>\n",
       "      <td>0.690973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stacked (semi-cfit)</th>\n",
       "      <td>7845.447482</td>\n",
       "      <td>1130.536458</td>\n",
       "      <td>5629.596023</td>\n",
       "      <td>10061.298940</td>\n",
       "      <td>53745.549497</td>\n",
       "      <td>0.442801</td>\n",
       "      <td>0.689158</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        estimate       stderr        lower         upper  \\\n",
       "lasso/logistic       7726.585781  1159.322663  5454.313361   9998.858202   \n",
       "random forest        7765.368071  1150.546198  5510.297522  10020.438620   \n",
       "decision tree        7841.536117  1255.451237  5380.851692  10302.220542   \n",
       "boosted forest       8697.815755  1158.203893  6427.736126  10967.895385   \n",
       "automl (semi-cfit)   8129.709678  1132.995563  5909.038374  10350.380982   \n",
       "stacked (semi-cfit)  7845.447482  1130.536458  5629.596023  10061.298940   \n",
       "\n",
       "                           rmse y    rmse D  accuracy D  \n",
       "lasso/logistic       54060.702446  0.444041    0.687948  \n",
       "random forest        55719.962830  0.444489    0.688048  \n",
       "decision tree        60491.245996  0.446437    0.688048  \n",
       "boosted forest       55255.465437  0.443394    0.690469  \n",
       "automl (semi-cfit)   55338.022903  0.443595    0.690973  \n",
       "stacked (semi-cfit)  53745.549497  0.442801    0.689158  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dr(X, D, y, modely0, modely1, modeld, *, trimming=0.01, nfolds):\n",
    "    '''\n",
    "    DML for the Interactive Regression Model setting (Doubly Robust Learning)\n",
    "    with cross-fitting\n",
    "\n",
    "    Input\n",
    "    -----\n",
    "    X: the controls\n",
    "    D: the treatment\n",
    "    y: the outcome\n",
    "    modely0: the ML model for predicting the outcome y in the control population\n",
    "    modely1: the ML model for predicting the outcome y in the treated population\n",
    "    modeld: the ML model for predicting the treatment D\n",
    "    trimming: threshold below which to trim propensities\n",
    "    nfolds: the number of folds in cross-fitting\n",
    "\n",
    "    Output\n",
    "    ------\n",
    "    point: the point estimate of the treatment effect of D on y\n",
    "    stderr: the standard error of the treatment effect\n",
    "    yhat: the cross-fitted predictions for the outcome y\n",
    "    Dhat: the cross-fitted predictions for the outcome D\n",
    "    resy: the outcome residuals\n",
    "    resD: the treatment residuals\n",
    "    drhat: the doubly robust quantity for each sample\n",
    "    '''\n",
    "    cv = KFold(n_splits=nfolds, shuffle=True, random_state=123)\n",
    "    yhat0, yhat1 = np.zeros(y.shape), np.zeros(y.shape)\n",
    "    # we will fit a model E[Y| D, X] by fitting a separate model for D==0\n",
    "    # and a separate model for D==1.\n",
    "    for train, test in cv.split(X, y):\n",
    "        # train a model on training data that received treatment zero and predict on all data in test set\n",
    "        yhat0[test] = clone(modely0).fit(X.iloc[train][D[train] == 0], y[train][D[train] == 0]).predict(X.iloc[test])\n",
    "        # train a model on training data that received treatment one and predict on all data in test set\n",
    "        yhat1[test] = clone(modely1).fit(X.iloc[train][D[train] == 1], y[train][D[train] == 1]).predict(X.iloc[test])\n",
    "    # prediction for observed treatment\n",
    "    yhat = yhat0 * (1 - D) + yhat1 * D\n",
    "    # propensity scores\n",
    "    Dhat = cross_val_predict(modeld, X, D, cv=cv, method='predict_proba', n_jobs=-1)[:, 1]\n",
    "    Dhat = np.clip(Dhat, trimming, 1 - trimming)\n",
    "    # doubly robust quantity for every sample\n",
    "    drhat = yhat1 - yhat0 + (y - yhat) * (D / Dhat - (1 - D) / (1 - Dhat))\n",
    "    point = np.mean(drhat)\n",
    "    var = np.var(drhat)\n",
    "    stderr = np.sqrt(var / X.shape[0])\n",
    "    return point, stderr, yhat, Dhat, y - yhat, D - Dhat, drhat\n",
    "\n",
    "v = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "lassoytest = make_pipeline(transformer, StandardScaler(), LassoCV(cv=cv))\n",
    "lgrdtest = make_pipeline(transformer, StandardScaler(), LogisticRegressionCV(cv=cv))\n",
    "result = dr(X, D, y, lassoytest, lassoytest, lgrdtest, nfolds=5)\n",
    "seed_estimates = summary(*result, X, D, y, name='lasso/logistic')\n",
    "\n",
    "for i in range(9):\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    lassoytest = make_pipeline(transformer, StandardScaler(), LassoCV(cv=cv))\n",
    "    lgrdtest = make_pipeline(transformer, StandardScaler(), LogisticRegressionCV(cv=cv))\n",
    "    result = dr(X, D, y, lassoytest, lassoytest, lgrdtest, nfolds=5)\n",
    "    seed_estimates = pd.concat([seed_estimates, summary(*result, X, D, y, name='lasso/logistic')])\n",
    "\n",
    "med_theta = np.median(seed_estimates.values[:, 0])\n",
    "se_med = np.sqrt(np.median((seed_estimates.values[:, 1])**2 + (seed_estimates.values[:, 0] - med_theta)**2))\n",
    "tabledr = pd.DataFrame({'estimate': med_theta,\n",
    "                        'stderr': se_med,\n",
    "                        'lower': med_theta - 1.96 * se_med,\n",
    "                        'upper': med_theta + 1.96 * se_med,\n",
    "                        'rmse y': np.median(seed_estimates.values[:, 4]),\n",
    "                        'rmse D': np.median(seed_estimates.values[:, 5]),\n",
    "                        'accuracy D': np.median(seed_estimates.values[:, 6]),\n",
    "                        }, index=['lasso/logistic'])\n",
    "\n",
    "rfy = make_pipeline(transformer, RandomForestRegressor(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001))\n",
    "rfd = make_pipeline(transformer, RandomForestClassifier(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001))\n",
    "result = dr(X, D, y, rfy, rfy, rfd, nfolds=5)\n",
    "tabledr = pd.concat([tabledr, summary(*result, X, D, y, name='random forest')])\n",
    "dtry = make_pipeline(transformer, DecisionTreeRegressor(min_samples_leaf=10, ccp_alpha=.001))\n",
    "dtrd = make_pipeline(transformer, DecisionTreeClassifier(min_samples_leaf=10, ccp_alpha=.001))\n",
    "result = dr(X, D, y, dtry, dtry, dtrd, nfolds=5)\n",
    "tabledr = pd.concat([tabledr, summary(*result, X, D, y, name='decision tree')])\n",
    "gbfy = make_pipeline(transformer, GradientBoostingRegressor(max_depth=2, n_iter_no_change=5))\n",
    "gbfd = make_pipeline(transformer, GradientBoostingClassifier(max_depth=2, n_iter_no_change=5))\n",
    "result = dr(X, D, y, gbfy, gbfy, gbfd, nfolds=5)\n",
    "tabledr = pd.concat([tabledr, summary(*result, X, D, y, name='boosted forest')])\n",
    "# semi cross-fitting\n",
    "flamly0 = make_pipeline(transformer, AutoML(time_budget=60, task='regression', early_stop=True,\n",
    "                                            eval_method='cv', n_splits=3, metric='r2', verbose=0))\n",
    "flamly1 = make_pipeline(transformer, AutoML(time_budget=60, task='regression', early_stop=True,\n",
    "                                            eval_method='cv', n_splits=3, metric='r2', verbose=0))\n",
    "flamld = make_pipeline(transformer, AutoML(time_budget=60, task='classification', early_stop=True,\n",
    "                                           eval_method='cv', n_splits=3, metric='r2', verbose=0))\n",
    "\n",
    "flamly0.fit(X[D == 0], y[D == 0])\n",
    "besty0 = make_pipeline(transformer, clone(flamly0[-1].best_model_for_estimator(flamly0[-1].best_estimator)))\n",
    "flamly1.fit(X[D == 1], y[D == 1])\n",
    "besty1 = make_pipeline(transformer, clone(flamly1[-1].best_model_for_estimator(flamly1[-1].best_estimator)))\n",
    "flamld.fit(X, D)\n",
    "bestd = make_pipeline(transformer, clone(flamld[-1].best_model_for_estimator(flamld[-1].best_estimator)))\n",
    "result = dr(X, D, y, besty0, besty1, bestd, nfolds=5)\n",
    "tabledr = pd.concat([tabledr, summary(*result, X, D, y, name='automl (semi-cfit)')])\n",
    "def dr_dirty(X, D, y, modely0_list, modely1_list, modeld_list, *,\n",
    "             stacker=LinearRegression(), trimming=0.01, nfolds):\n",
    "    '''\n",
    "    DML for the Interactive Regression Model setting (Doubly Robust Learning)\n",
    "    with cross-fitting\n",
    "\n",
    "    Input\n",
    "    -----\n",
    "    X: the controls\n",
    "    D: the treatment\n",
    "    y: the outcome\n",
    "    modely_list: list of ML models for predicting the outcome y\n",
    "    modeld_list: list of ML models for predicting the treatment D\n",
    "    stacker: model used to aggregate predictions of each of the base models\n",
    "    trimming: threshold below which to trim propensities\n",
    "    nfolds: the number of folds in cross-fitting\n",
    "\n",
    "    Output\n",
    "    ------\n",
    "    point: the point estimate of the treatment effect of D on y\n",
    "    stderr: the standard error of the treatment effect\n",
    "    yhat: the cross-fitted predictions for the outcome y\n",
    "    Dhat: the cross-fitted predictions for the outcome D\n",
    "    resy: the outcome residuals\n",
    "    resD: the treatment residuals\n",
    "    drhat: the doubly robust quantity for each sample\n",
    "    '''\n",
    "    cv = KFold(n_splits=nfolds, shuffle=True, random_state=123)\n",
    "\n",
    "    # we will fit a model E[Y| D, X] by fitting a separate model for D==0\n",
    "    # and a separate model for D==1. We do that for each model type in modely_list\n",
    "    yhats0, yhats1 = np.zeros((y.shape[0], len(modely0_list))), np.zeros((y.shape[0], len(modely1_list)))\n",
    "    for train, test in cv.split(X, y):\n",
    "        for it, modely0 in enumerate(modely0_list):\n",
    "            mdl = clone(modely0).fit(X.iloc[train][D[train] == 0], y[train][D[train] == 0])\n",
    "            yhats0[test, it] = mdl.predict(X.iloc[test])\n",
    "        for it, modely1 in enumerate(modely1_list):\n",
    "            mdl = clone(modely1).fit(X.iloc[train][D[train] == 1], y[train][D[train] == 1])\n",
    "            yhats1[test, it] = mdl.predict(X.iloc[test])\n",
    "\n",
    "    # calculate stacking weights for the outcome model for each population\n",
    "    # and combine the outcome model predictions\n",
    "    yhat0 = clone(stacker).fit(yhats0[D == 0], y[D == 0]).predict(yhats0)\n",
    "    yhat1 = clone(stacker).fit(yhats1[D == 1], y[D == 1]).predict(yhats1)\n",
    "\n",
    "    # prediction for observed treatment using the stacked model\n",
    "    yhat = yhat0 * (1 - D) + yhat1 * D\n",
    "\n",
    "    # propensity scores\n",
    "    Dhats = np.array([cross_val_predict(modeld, X, D, cv=cv, method='predict_proba', n_jobs=-1)[:, 1]\n",
    "                     for modeld in modeld_list]).T\n",
    "    # construct coefficients on each model based on stacker\n",
    "    Dhat = clone(stacker).fit(Dhats, D).predict(Dhats)\n",
    "    # trim propensities\n",
    "    Dhat = np.clip(Dhat, trimming, 1 - trimming)\n",
    "\n",
    "    # doubly robust quantity for every sample\n",
    "    drhat = yhat1 - yhat0 + (y - yhat) * (D / Dhat - (1 - D) / (1 - Dhat))\n",
    "    point = np.mean(drhat)\n",
    "    var = np.var(drhat)\n",
    "    stderr = np.sqrt(var / X.shape[0])\n",
    "    return point, stderr, yhat, Dhat, y - yhat, D - Dhat, drhat\n",
    "\n",
    "result = dr_dirty(X, D, y, [lassoy, rfy, dtry, gbfy], [lassoy, rfy, dtry, gbfy], [lgrd, rfd, dtrd, gbfd], nfolds=5)\n",
    "tabledr = pd.concat([tabledr, summary(*result, X, D, y, name='stacked (semi-cfit)')])\n",
    "tabledr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Doubly Robust Results: Bottom 25% Income ===\n",
      "                        estimate       stderr        lower         upper  \\\n",
      "lasso/logistic       4451.346490  1021.649465  2448.913538   6453.779442   \n",
      "random forest        4262.490988   931.478674  2436.792788   6088.189189   \n",
      "decision tree        8285.518869  4363.153616  -266.262218  16837.299956   \n",
      "boosted forest       4925.447744   970.186995  3023.881235   6827.014254   \n",
      "automl (semi-cfit)   4176.134761  1145.621803  1930.716026   6421.553496   \n",
      "stacked (semi-cfit)  4897.836047  1472.909711  2010.933013   7784.739080   \n",
      "\n",
      "                           rmse y    rmse D  accuracy D  \n",
      "lasso/logistic       13323.560722  0.359078    0.846030  \n",
      "random forest        13475.810044  0.345749    0.846433  \n",
      "decision tree        14229.940513  0.380278    0.804111  \n",
      "boosted forest       13544.383567  0.345370    0.844418  \n",
      "automl (semi-cfit)   13177.371099  0.351340    0.835953  \n",
      "stacked (semi-cfit)  13205.919202  0.343022    0.847239  \n",
      "\n",
      "=== Doubly Robust Results: Top 25% Income ===\n",
      "                         estimate        stderr         lower         upper  \\\n",
      "lasso/logistic       17259.513464   3916.988487   9582.216029  24936.810898   \n",
      "random forest        16209.244505   4086.807559   8199.101690  24219.387320   \n",
      "decision tree        46469.241476  26499.056343  -5468.908956  98407.391909   \n",
      "boosted forest       16658.658801   3850.094746   9112.473099  24204.844504   \n",
      "automl (semi-cfit)   17037.483404   3747.395234   9692.588744  24382.378063   \n",
      "stacked (semi-cfit)  18652.549662   3796.161103  11212.073900  26093.025423   \n",
      "\n",
      "                            rmse y    rmse D  accuracy D  \n",
      "lasso/logistic        91534.931249  0.482861    0.603066  \n",
      "random forest         96346.597527  0.485324    0.600645  \n",
      "decision tree        103739.400617  0.543984    0.536507  \n",
      "boosted forest        95538.717463  0.484493    0.592981  \n",
      "automl (semi-cfit)    94421.196487  0.484810    0.599435  \n",
      "stacked (semi-cfit)   90818.344994  0.481721    0.600242  \n"
     ]
    }
   ],
   "source": [
    "data_bottom = data.query('inc <= inc.quantile(.25)').copy()\n",
    "data_top    = data.query('inc >= inc.quantile(.75)').copy()\n",
    "\n",
    "def extract_XDy(df):\n",
    "    \"\"\"Given a subset of the 401k data, produce X, D, and y \n",
    "    consistent with the main DR analysis.\"\"\"\n",
    "    y_ = df['net_tfa'].values\n",
    "    D_ = df['e401'].values\n",
    "    X_ = df.drop([\n",
    "        'e401', 'p401', 'a401', 'tw', 'tfa', 'net_tfa', 'tfa_he',\n",
    "        'hval', 'hmort', 'hequity', 'nifa', 'net_nifa', 'net_n401',\n",
    "        'ira', 'dum91', 'icat', 'ecat', 'zhat', 'i1', 'i2', 'i3',\n",
    "        'i4', 'i5', 'i6', 'i7', 'a1', 'a2', 'a3', 'a4', 'a5'\n",
    "    ], axis=1)\n",
    "    return X_, D_, y_\n",
    "\n",
    "X_bottom, D_bottom, y_bottom = extract_XDy(data_bottom)\n",
    "X_top, D_top, y_top          = extract_XDy(data_top)\n",
    "\n",
    "def dr(X, D, y, modely0, modely1, modeld, *, trimming=0.01, nfolds=5):\n",
    "    '''\n",
    "    DML for the Interactive Regression Model setting (Doubly Robust Learning)\n",
    "    with cross-fitting\n",
    "    '''\n",
    "    cv = KFold(n_splits=nfolds, shuffle=True, random_state=123)\n",
    "    yhat0, yhat1 = np.zeros(y.shape), np.zeros(y.shape)\n",
    "    for train, test in cv.split(X, y):\n",
    "        # Fit E[Y|D=0,X]\n",
    "        mdl0 = clone(modely0).fit(X.iloc[train][D[train] == 0], y[train][D[train] == 0])\n",
    "        yhat0[test] = mdl0.predict(X.iloc[test])\n",
    "        # Fit E[Y|D=1,X]\n",
    "        mdl1 = clone(modely1).fit(X.iloc[train][D[train] == 1], y[train][D[train] == 1])\n",
    "        yhat1[test] = mdl1.predict(X.iloc[test])\n",
    "\n",
    "    # Combine to get E[Y|D,X] predictions for the observed D\n",
    "    yhat = yhat0 * (1 - D) + yhat1 * D\n",
    "\n",
    "    # Propensity scores\n",
    "    Dhat = cross_val_predict(modeld, X, D, cv=cv, method='predict_proba', n_jobs=-1)[:, 1]\n",
    "    Dhat = np.clip(Dhat, trimming, 1 - trimming)\n",
    "\n",
    "    # Doubly robust quantity for each sample\n",
    "    drhat = yhat1 - yhat0 + (y - yhat) * (D / Dhat - (1 - D) / (1 - Dhat))\n",
    "    point = np.mean(drhat)\n",
    "    var   = np.var(drhat)\n",
    "    stderr= np.sqrt(var / X.shape[0])\n",
    "\n",
    "    # Return:\n",
    "    #   - point: the point estimate\n",
    "    #   - stderr: standard error\n",
    "    #   - yhat: cross-fitted predictions for the observed D\n",
    "    #   - Dhat: cross-fitted propensities\n",
    "    #   - (y - yhat): outcome residual\n",
    "    #   - (D - Dhat): treatment residual\n",
    "    #   - drhat: doubly robust terms\n",
    "    return point, stderr, yhat, Dhat, (y - yhat), (D - Dhat), drhat\n",
    "\n",
    "def dr_dirty(\n",
    "    X, D, y,\n",
    "    modely0_list, modely1_list, modeld_list,\n",
    "    stacker=LinearRegression(), trimming=0.01, nfolds=5\n",
    "):\n",
    "    '''\n",
    "    DML for the Interactive Regression Model setting (Doubly Robust Learning)\n",
    "    with cross-fitting and stacking\n",
    "    '''\n",
    "    cv = KFold(n_splits=nfolds, shuffle=True, random_state=123)\n",
    "\n",
    "    # Prepare space for multiple model predictions\n",
    "    yhats0 = np.zeros((X.shape[0], len(modely0_list)))\n",
    "    yhats1 = np.zeros((X.shape[0], len(modely1_list)))\n",
    "\n",
    "    # Cross-fitting: fit each base model E[Y|D=0,X], E[Y|D=1,X]\n",
    "    for train, test in cv.split(X, y):\n",
    "        for i_m0, m0 in enumerate(modely0_list):\n",
    "            m0_cl = clone(m0).fit(X.iloc[train][D[train] == 0], y[train][D[train] == 0])\n",
    "            yhats0[test, i_m0] = m0_cl.predict(X.iloc[test])\n",
    "        for i_m1, m1 in enumerate(modely1_list):\n",
    "            m1_cl = clone(m1).fit(X.iloc[train][D[train] == 1], y[train][D[train] == 1])\n",
    "            yhats1[test, i_m1] = m1_cl.predict(X.iloc[test])\n",
    "\n",
    "    # Stack them for D=0 predictions\n",
    "    yhat0 = clone(stacker).fit(yhats0[D == 0], y[D == 0]).predict(yhats0)\n",
    "    # Stack them for D=1 predictions\n",
    "    yhat1 = clone(stacker).fit(yhats1[D == 1], y[D == 1]).predict(yhats1)\n",
    "    # Combined model prediction for Y\n",
    "    yhat = yhat0 * (1 - D) + yhat1 * D\n",
    "\n",
    "    # Now do the same for propensity models\n",
    "    Dhats_list = []\n",
    "    for md in modeld_list:\n",
    "        Dhats_list.append(cross_val_predict(md, X, D, cv=cv, method='predict_proba', n_jobs=-1)[:, 1])\n",
    "    Dhats_arr = np.vstack(Dhats_list).T  # shape: (n_samples, n_models)\n",
    "\n",
    "    # Stack the predicted propensity\n",
    "    Dhat = clone(stacker).fit(Dhats_arr, D).predict(Dhats_arr)\n",
    "    Dhat = np.clip(Dhat, trimming, 1 - trimming)\n",
    "\n",
    "    # Doubly robust quantity\n",
    "    drhat = yhat1 - yhat0 + (y - yhat) * (D / Dhat - (1 - D) / (1 - Dhat))\n",
    "    point = np.mean(drhat)\n",
    "    var   = np.var(drhat)\n",
    "    stderr= np.sqrt(var / X.shape[0])\n",
    "\n",
    "    return point, stderr, yhat, Dhat, (y - yhat), (D - Dhat), drhat\n",
    "\n",
    "\n",
    "def summary(point, stderr, yhat, Dhat, resy, resD, drhat, X, D, y, *, name=''):\n",
    "    return pd.DataFrame({\n",
    "        'estimate': point,  # point estimate\n",
    "        'stderr': stderr,   # standard error\n",
    "        'lower': point - 1.96 * stderr,\n",
    "        'upper': point + 1.96 * stderr,\n",
    "        'rmse y': np.sqrt(np.mean(resy**2)),\n",
    "        'rmse D': np.sqrt(np.mean(resD**2)),\n",
    "        # classification accuracy for D, if it's binary:\n",
    "        'accuracy D': np.mean(np.abs(resD) < 0.5),\n",
    "    }, index=[name])\n",
    "\n",
    "\n",
    "def run_dr_analysis(X, D, y):\n",
    "    \"\"\"\n",
    "    Replicates the DR analysis shown in the original code:\n",
    "      - lasso/logistic with repeated seeds, then median-based estimate\n",
    "      - random forest\n",
    "      - decision tree\n",
    "      - boosted forest\n",
    "      - automl (semi-cfit)\n",
    "      - stacking\n",
    "    Returns a single table of results.\n",
    "    \"\"\"\n",
    "    # -- 1) Lasso/logistic repeated over seeds, then median-based estimate\n",
    "    seed_estimates = None\n",
    "\n",
    "    # We'll define a base 5-fold for the model pipelines:\n",
    "    cv_5fold = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "\n",
    "    # One example run with seed=123\n",
    "    lassoy = make_pipeline(transformer, StandardScaler(), LassoCV(cv=cv_5fold))\n",
    "    lgrd   = make_pipeline(transformer, StandardScaler(), LogisticRegressionCV(cv=cv_5fold))\n",
    "\n",
    "    # DR for single seed first\n",
    "    result = dr(X, D, y, lassoy, lassoy, lgrd, nfolds=5)\n",
    "    seed_estimates = summary(*result, X, D, y, name='lasso/logistic')\n",
    "\n",
    "    # Loop over multiple seeds\n",
    "    for i in range(9):\n",
    "        cv_seed = KFold(n_splits=5, shuffle=True, random_state=i)\n",
    "        lassoy_i = make_pipeline(transformer, StandardScaler(), LassoCV(cv=cv_seed))\n",
    "        lgrd_i   = make_pipeline(transformer, StandardScaler(), LogisticRegressionCV(cv=cv_seed))\n",
    "\n",
    "        result_i = dr(X, D, y, lassoy_i, lassoy_i, lgrd_i, nfolds=5)\n",
    "        seed_estimates = pd.concat([seed_estimates, summary(*result_i, X, D, y, name='lasso/logistic')])\n",
    "\n",
    "    # Compute median-based point estimate and standard error\n",
    "    med_theta  = np.median(seed_estimates['estimate'].values)\n",
    "    # sqrt(median(var_i) + var( point_i ))\n",
    "    # but the original code basically does:\n",
    "    se_med     = np.sqrt(\n",
    "        np.median(seed_estimates['stderr'].values ** 2)\n",
    "        + np.median((seed_estimates['estimate'].values - med_theta) ** 2)\n",
    "    )\n",
    "    tabledr = pd.DataFrame({\n",
    "        'estimate': med_theta,\n",
    "        'stderr':   se_med,\n",
    "        'lower':    med_theta - 1.96 * se_med,\n",
    "        'upper':    med_theta + 1.96 * se_med,\n",
    "        'rmse y':   np.median(seed_estimates['rmse y'].values),\n",
    "        'rmse D':   np.median(seed_estimates['rmse D'].values),\n",
    "        'accuracy D': np.median(seed_estimates['accuracy D'].values)\n",
    "    }, index=['lasso/logistic'])\n",
    "\n",
    "    # -- 2) Random Forest\n",
    "    rfy = make_pipeline(transformer, \n",
    "                        RandomForestRegressor(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001))\n",
    "    rfd = make_pipeline(transformer, \n",
    "                        RandomForestClassifier(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001))\n",
    "    result = dr(X, D, y, rfy, rfy, rfd, nfolds=5)\n",
    "    tabledr = pd.concat([tabledr, summary(*result, X, D, y, name='random forest')])\n",
    "\n",
    "    # -- 3) Decision Tree\n",
    "    dtry = make_pipeline(transformer,\n",
    "                         DecisionTreeRegressor(min_samples_leaf=10, ccp_alpha=.001))\n",
    "    dtrd = make_pipeline(transformer,\n",
    "                         DecisionTreeClassifier(min_samples_leaf=10, ccp_alpha=.001))\n",
    "    result = dr(X, D, y, dtry, dtry, dtrd, nfolds=5)\n",
    "    tabledr = pd.concat([tabledr, summary(*result, X, D, y, name='decision tree')])\n",
    "\n",
    "    # -- 4) Boosted Forest\n",
    "    gbfy = make_pipeline(transformer, \n",
    "                         GradientBoostingRegressor(max_depth=2, n_iter_no_change=5))\n",
    "    gbfd = make_pipeline(transformer, \n",
    "                         GradientBoostingClassifier(max_depth=2, n_iter_no_change=5))\n",
    "    result = dr(X, D, y, gbfy, gbfy, gbfd, nfolds=5)\n",
    "    tabledr = pd.concat([tabledr, summary(*result, X, D, y, name='boosted forest')])\n",
    "\n",
    "    # -- 5) AutoML (semi cross-fitting)\n",
    "    flamly0 = make_pipeline(transformer,\n",
    "                            AutoML(time_budget=60, task='regression', early_stop=True,\n",
    "                                   eval_method='cv', n_splits=3, metric='r2', verbose=0))\n",
    "    flamly1 = make_pipeline(transformer,\n",
    "                            AutoML(time_budget=60, task='regression', early_stop=True,\n",
    "                                   eval_method='cv', n_splits=3, metric='r2', verbose=0))\n",
    "    flamld  = make_pipeline(transformer,\n",
    "                            AutoML(time_budget=60, task='classification', early_stop=True,\n",
    "                                   eval_method='cv', n_splits=3, metric='r2', verbose=0))\n",
    "\n",
    "    # Fit Y|D=0, Y|D=1 on subsets\n",
    "    flamly0.fit(X[D == 0], y[D == 0])\n",
    "    besty0_model = flamly0[-1].best_model_for_estimator(flamly0[-1].best_estimator)\n",
    "    besty0 = make_pipeline(transformer, clone(besty0_model))\n",
    "\n",
    "    flamly1.fit(X[D == 1], y[D == 1])\n",
    "    besty1_model = flamly1[-1].best_model_for_estimator(flamly1[-1].best_estimator)\n",
    "    besty1 = make_pipeline(transformer, clone(besty1_model))\n",
    "\n",
    "    # Fit propensities on the full sample\n",
    "    flamld.fit(X, D)\n",
    "    bestd_model = flamld[-1].best_model_for_estimator(flamld[-1].best_estimator)\n",
    "    bestd = make_pipeline(transformer, clone(bestd_model))\n",
    "\n",
    "    result = dr(X, D, y, besty0, besty1, bestd, nfolds=5)\n",
    "    tabledr = pd.concat([tabledr, summary(*result, X, D, y, name='automl (semi-cfit)')])\n",
    "\n",
    "    # -- 6) Stacked (semi-cfit)\n",
    "    # Prepare lists of base models for Y|D=0, Y|D=1\n",
    "    lassoy_ = make_pipeline(transformer, StandardScaler(), LassoCV(cv=cv_5fold))\n",
    "    rfy_    = make_pipeline(transformer, \n",
    "                            RandomForestRegressor(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001))\n",
    "    dtry_   = make_pipeline(transformer, \n",
    "                            DecisionTreeRegressor(min_samples_leaf=10, ccp_alpha=.001))\n",
    "    gbfy_   = make_pipeline(transformer, \n",
    "                            GradientBoostingRegressor(max_depth=2, n_iter_no_change=5))\n",
    "\n",
    "    modely0_list = [lassoy_, rfy_, dtry_, gbfy_]  # for D=0\n",
    "    modely1_list = [lassoy_, rfy_, dtry_, gbfy_]  # for D=1\n",
    "\n",
    "    lgrd_   = make_pipeline(transformer, StandardScaler(), LogisticRegressionCV(cv=cv_5fold))\n",
    "    rfd_    = make_pipeline(transformer, \n",
    "                            RandomForestClassifier(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001))\n",
    "    dtrd_   = make_pipeline(transformer, \n",
    "                            DecisionTreeClassifier(min_samples_leaf=10, ccp_alpha=.001))\n",
    "    gbfd_   = make_pipeline(transformer, \n",
    "                            GradientBoostingClassifier(max_depth=2, n_iter_no_change=5))\n",
    "    modeld_list = [lgrd_, rfd_, dtrd_, gbfd_]\n",
    "\n",
    "    result = dr_dirty(X, D, y, modely0_list, modely1_list, modeld_list, stacker=LinearRegression(), nfolds=5)\n",
    "    tabledr = pd.concat([tabledr, summary(*result, X, D, y, name='stacked (semi-cfit)')])\n",
    "\n",
    "    return tabledr\n",
    "\n",
    "table_dr_bottom = run_dr_analysis(X_bottom, D_bottom, y_bottom)\n",
    "table_dr_top    = run_dr_analysis(X_top,    D_top,    y_top   )\n",
    "\n",
    "print(\"=== Doubly Robust Results: Bottom 25% Income ===\")\n",
    "print(table_dr_bottom)\n",
    "\n",
    "print(\"\\n=== Doubly Robust Results: Top 25% Income ===\")\n",
    "print(table_dr_top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the IRM setting, we again see heterogeneity in the treatment effect with respect to income, with the bottom 25% of earners seeing estimates around 4.5k and the top 25% seeing estimates of around 17k. The different machine learning models are broadly consistent across all three income groups with the exception of the decision tree, which has far higher estimates and standard errors than other methods.  \n",
    "  \n",
    "\n",
    "Now we move to implemetting semi-crossfitting with best model selection, first for PLR and then for IRM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>estimate</th>\n",
       "      <th>stderr</th>\n",
       "      <th>lower</th>\n",
       "      <th>upper</th>\n",
       "      <th>rmse y</th>\n",
       "      <th>rmse D</th>\n",
       "      <th>accuracy D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>select-best (semi-cfit) PLR</th>\n",
       "      <td>8929.278079</td>\n",
       "      <td>1304.893146</td>\n",
       "      <td>6371.687512</td>\n",
       "      <td>11486.868646</td>\n",
       "      <td>54254.468883</td>\n",
       "      <td>0.443503</td>\n",
       "      <td>0.690368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>select-best (semi-cfit) PLR (bottom 25%)</th>\n",
       "      <td>3741.376659</td>\n",
       "      <td>1101.849517</td>\n",
       "      <td>1581.751606</td>\n",
       "      <td>5901.001712</td>\n",
       "      <td>13400.361810</td>\n",
       "      <td>0.344902</td>\n",
       "      <td>0.845224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>select-best (semi-cfit) PLR (top 25%)</th>\n",
       "      <td>18204.194590</td>\n",
       "      <td>3867.405524</td>\n",
       "      <td>10624.079763</td>\n",
       "      <td>25784.309418</td>\n",
       "      <td>91393.039963</td>\n",
       "      <td>0.482708</td>\n",
       "      <td>0.601049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              estimate       stderr  \\\n",
       "select-best (semi-cfit) PLR                8929.278079  1304.893146   \n",
       "select-best (semi-cfit) PLR (bottom 25%)   3741.376659  1101.849517   \n",
       "select-best (semi-cfit) PLR (top 25%)     18204.194590  3867.405524   \n",
       "\n",
       "                                                 lower         upper  \\\n",
       "select-best (semi-cfit) PLR                6371.687512  11486.868646   \n",
       "select-best (semi-cfit) PLR (bottom 25%)   1581.751606   5901.001712   \n",
       "select-best (semi-cfit) PLR (top 25%)     10624.079763  25784.309418   \n",
       "\n",
       "                                                rmse y    rmse D  accuracy D  \n",
       "select-best (semi-cfit) PLR               54254.468883  0.443503    0.690368  \n",
       "select-best (semi-cfit) PLR (bottom 25%)  13400.361810  0.344902    0.845224  \n",
       "select-best (semi-cfit) PLR (top 25%)     91393.039963  0.482708    0.601049  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# b.)\n",
    "from sklearn.model_selection import cross_val_predict, KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from copy import deepcopy\n",
    "\n",
    "def get_oof_and_mse(model, X, y, cv, classifier=False):\n",
    "    \"\"\"\n",
    "    Returns out-of-fold predictions and the MSE (or 'regression MSE' if classifier=True).\n",
    "    If classifier=True, we use model.predict_proba(...).\n",
    "    \"\"\"\n",
    "    if classifier:\n",
    "        # For binary D, treat the problem as a regression on [0,1], \n",
    "        # so we measure MSE on predicted probabilities\n",
    "        preds = cross_val_predict(model, X, y, cv=cv, method='predict_proba')[:, 1]\n",
    "    else:\n",
    "        preds = cross_val_predict(model, X, y, cv=cv)\n",
    "    mse_val = mean_squared_error(y, preds)\n",
    "    return preds, mse_val\n",
    "\n",
    "\n",
    "def dml_select_best(X, D, y, modely_list, modeld_list, *, nfolds=5, classifier=False):\n",
    "    \"\"\"\n",
    "    Semi-cross-fitting for the Partially Linear Model (PLR) by selecting\n",
    "    the single best model for Y and single best model for D among user-provided lists.\n",
    "\n",
    "    Steps:\n",
    "      1) For each candidate in modely_list, get OOF predictions vs. y. Pick the best by MSE.\n",
    "      2) For each candidate in modeld_list, get OOF predictions vs. D. Pick the best by MSE.\n",
    "         If classifier=True, each model in modeld_list is treated as a classifier,\n",
    "         but we still measure MSE on the predicted probability vs. the true D.\n",
    "      3) Use the chosen best model's OOF predictions for yhat and Dhat.\n",
    "      4) Compute partial linear estimate as in the usual DML formula:\n",
    "            theta = E[(y - yhat)*(D - Dhat)] / E[(D - Dhat)^2]\n",
    "         and the standard error formula.\n",
    "\n",
    "    Returns:\n",
    "      point, stderr, yhat, Dhat, resy, resD, epsilon\n",
    "    \"\"\"\n",
    "    cv = KFold(n_splits=nfolds, shuffle=True, random_state=123)\n",
    "\n",
    "    # --- 1) Select best model for y\n",
    "    best_mse_y = np.inf\n",
    "    best_preds_y = None\n",
    "    best_model_y = None\n",
    "\n",
    "    for candidate in modely_list:\n",
    "        # Clone so we don't pollute the original pipeline with partial fits\n",
    "        cand = deepcopy(candidate)\n",
    "        preds, mse_val = get_oof_and_mse(cand, X, y, cv, classifier=False)\n",
    "        if mse_val < best_mse_y:\n",
    "            best_mse_y = mse_val\n",
    "            best_preds_y = preds\n",
    "            best_model_y = cand\n",
    "\n",
    "    # --- 2) Select best model for D\n",
    "    best_mse_d = np.inf\n",
    "    best_preds_d = None\n",
    "    best_model_d = None\n",
    "\n",
    "    for candidate in modeld_list:\n",
    "        # If classifier=True, we measure MSE on predicted probabilities\n",
    "        cand = deepcopy(candidate)\n",
    "        preds, mse_val = get_oof_and_mse(cand, X, D, cv, classifier=classifier)\n",
    "        if mse_val < best_mse_d:\n",
    "            best_mse_d = mse_val\n",
    "            best_preds_d = preds\n",
    "            best_model_d = cand\n",
    "\n",
    "    # Residuals\n",
    "    resy = y - best_preds_y\n",
    "    resD = D - best_preds_d\n",
    "\n",
    "    # Final partial linear estimate\n",
    "    point = np.mean(resy * resD) / np.mean(resD**2)\n",
    "    epsilon = resy - point * resD\n",
    "\n",
    "    var = np.mean(epsilon**2 * resD**2) / (np.mean(resD**2)**2)\n",
    "    stderr = np.sqrt(var / X.shape[0])\n",
    "\n",
    "    return point, stderr, best_preds_y, best_preds_d, resy, resD, epsilon\n",
    "\n",
    "modely_list = [lassoy, rfy, dtry, gbfy]\n",
    "modeld_list = [lgrd, rfd, dtrd, gbfd]\n",
    "\n",
    "point, stderr, yhat, Dhat, resy, resD, epsilon = dml_select_best(\n",
    "    X, D, y, modely_list, modeld_list, nfolds=5, classifier=True\n",
    ")\n",
    "\n",
    "table_select = summary(\n",
    "    point, stderr, yhat, Dhat, resy, resD, epsilon, X, D, y,\n",
    "    name='select-best (semi-cfit) PLR'\n",
    ")\n",
    "point_bottom, stderr_bottom, yhat_bottom, Dhat_bottom, resy_bottom, resD_bottom, epsilon_bottom = dml_select_best(\n",
    "    X_bottom, D_bottom, y_bottom, modely_list, modeld_list, nfolds=5, classifier=True\n",
    ")\n",
    "table_select_bottom = summary(\n",
    "    point_bottom, stderr_bottom, yhat_bottom, Dhat_bottom, resy_bottom, resD_bottom, epsilon_bottom, X_bottom, D_bottom, y_bottom,\n",
    "    name='select-best (semi-cfit) PLR (bottom 25%)'\n",
    ")\n",
    "table_select = pd.concat([table_select, table_select_bottom])\n",
    "point_top, stderr_top, yhat_top, Dhat_top, resy_top, resD_top, epsilon_top = dml_select_best(\n",
    "    X_top, D_top, y_top, modely_list, modeld_list, nfolds=5, classifier=True\n",
    ")\n",
    "table_select_top = summary(\n",
    "    point_top, stderr_top, yhat_top, Dhat_top, resy_top, resD_top, epsilon_top, X_top, D_top, y_top,\n",
    "    name='select-best (semi-cfit) PLR (top 25%)'\n",
    ")\n",
    "table_select = pd.concat([table_select, table_select_top])\n",
    "table_select"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For IRM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>estimate</th>\n",
       "      <th>stderr</th>\n",
       "      <th>lower</th>\n",
       "      <th>upper</th>\n",
       "      <th>rmse y</th>\n",
       "      <th>rmse D</th>\n",
       "      <th>accuracy D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>select best IRM with semi cross fitting all samples</th>\n",
       "      <td>7721.990812</td>\n",
       "      <td>1118.811883</td>\n",
       "      <td>5529.119521</td>\n",
       "      <td>9914.862102</td>\n",
       "      <td>54026.928780</td>\n",
       "      <td>0.443681</td>\n",
       "      <td>0.688855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>select best IRM with semi cross fitting bottom 25\\% income</th>\n",
       "      <td>3985.926019</td>\n",
       "      <td>980.508563</td>\n",
       "      <td>2064.129235</td>\n",
       "      <td>5907.722803</td>\n",
       "      <td>13329.992579</td>\n",
       "      <td>0.345253</td>\n",
       "      <td>0.844418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>select best IRM with semi cross fitting top 25\\% income</th>\n",
       "      <td>18393.721922</td>\n",
       "      <td>3820.594388</td>\n",
       "      <td>10905.356922</td>\n",
       "      <td>25882.086922</td>\n",
       "      <td>91533.229313</td>\n",
       "      <td>0.482708</td>\n",
       "      <td>0.601049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        estimate       stderr  \\\n",
       "select best IRM with semi cross fitting all sam...   7721.990812  1118.811883   \n",
       "select best IRM with semi cross fitting bottom ...   3985.926019   980.508563   \n",
       "select best IRM with semi cross fitting top 25\\...  18393.721922  3820.594388   \n",
       "\n",
       "                                                           lower  \\\n",
       "select best IRM with semi cross fitting all sam...   5529.119521   \n",
       "select best IRM with semi cross fitting bottom ...   2064.129235   \n",
       "select best IRM with semi cross fitting top 25\\...  10905.356922   \n",
       "\n",
       "                                                           upper  \\\n",
       "select best IRM with semi cross fitting all sam...   9914.862102   \n",
       "select best IRM with semi cross fitting bottom ...   5907.722803   \n",
       "select best IRM with semi cross fitting top 25\\...  25882.086922   \n",
       "\n",
       "                                                          rmse y    rmse D  \\\n",
       "select best IRM with semi cross fitting all sam...  54026.928780  0.443681   \n",
       "select best IRM with semi cross fitting bottom ...  13329.992579  0.345253   \n",
       "select best IRM with semi cross fitting top 25\\...  91533.229313  0.482708   \n",
       "\n",
       "                                                    accuracy D  \n",
       "select best IRM with semi cross fitting all sam...    0.688855  \n",
       "select best IRM with semi cross fitting bottom ...    0.844418  \n",
       "select best IRM with semi cross fitting top 25\\...    0.601049  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_oof_and_mse_irm(model, X, y, D, which_d, cv):\n",
    "    \"\"\"\n",
    "    For IRM, we measure OOF performance only on the subset where D == which_d.\n",
    "    We do cross-fitting: train on all 'train' points that have D==which_d, \n",
    "    predict on the entire test fold, but compute MSE only for the test fold \n",
    "    members that also have D==which_d.\n",
    "    \n",
    "    Returns OOF predictions (full length, but only truly valid for D==which_d),\n",
    "    plus MSE measured on that subset.\n",
    "    \"\"\"\n",
    "    preds_full = np.zeros(len(y), dtype=float)\n",
    "    for train_idx, test_idx in cv.split(X, y):\n",
    "        # filter the training portion to only those with D==which_d\n",
    "        train_sub = train_idx[D[train_idx] == which_d]\n",
    "        # Fit on (X, y) for that sub-sample\n",
    "        model_cl = deepcopy(model).fit(X.iloc[train_sub], y[train_sub])\n",
    "        # Predict on the entire test fold\n",
    "        preds_full[test_idx] = model_cl.predict(X.iloc[test_idx])\n",
    "    # MSE only on the subset that has D==which_d\n",
    "    mse_val = mean_squared_error(y[D==which_d], preds_full[D==which_d])\n",
    "    return preds_full, mse_val\n",
    "\n",
    "def dr_select_best(X, D, y, modely0_list, modely1_list, modeld_list,\n",
    "                   trimming=0.01, nfolds=5):\n",
    "    \"\"\"\n",
    "    Doubly-Robust (IRM) with semi-cross-fitting:\n",
    "    Select single best model for Y|D=0 from modely0_list,\n",
    "    single best model for Y|D=1 from modely1_list,\n",
    "    single best model for the propensity from modeld_list (all data).\n",
    "    \n",
    "    Then run the standard cross-fitting formula to construct:\n",
    "      yhat0, yhat1, Dhat, drhat,\n",
    "    and output the usual IRM results.\n",
    "    \"\"\"\n",
    "    cv = KFold(n_splits=nfolds, shuffle=True, random_state=123)\n",
    "\n",
    "    # --- 1) find best model for Y|D=0\n",
    "    best_mse_y0 = np.inf\n",
    "    best_preds_y0 = None\n",
    "    best_model_y0 = None\n",
    "    for candidate in modely0_list:\n",
    "        preds0, mse0 = get_oof_and_mse_irm(candidate, X, y, D, which_d=0, cv=cv)\n",
    "        if mse0 < best_mse_y0:\n",
    "            best_mse_y0 = mse0\n",
    "            best_preds_y0 = preds0\n",
    "            best_model_y0 = candidate\n",
    "\n",
    "    # --- 2) find best model for Y|D=1\n",
    "    best_mse_y1 = np.inf\n",
    "    best_preds_y1 = None\n",
    "    best_model_y1 = None\n",
    "    for candidate in modely1_list:\n",
    "        preds1, mse1 = get_oof_and_mse_irm(candidate, X, y, D, which_d=1, cv=cv)\n",
    "        if mse1 < best_mse_y1:\n",
    "            best_mse_y1 = mse1\n",
    "            best_preds_y1 = preds1\n",
    "            best_model_y1 = candidate\n",
    "\n",
    "    # --- 3) find best model for D (propensity), measure MSE on entire sample\n",
    "    #         but we treat D as binary, using predicted probability\n",
    "    cv2 = KFold(n_splits=nfolds, shuffle=True, random_state=123)\n",
    "    best_mse_d = np.inf\n",
    "    best_preds_d = None\n",
    "    best_model_d = None\n",
    "    for candidate in modeld_list:\n",
    "        preds_d, mse_d = get_oof_and_mse(candidate, X, D, cv2, classifier=True)\n",
    "        if mse_d < best_mse_d:\n",
    "            best_mse_d = mse_d\n",
    "            best_preds_d = preds_d\n",
    "            best_model_d = candidate\n",
    "\n",
    "    # --- 4) IRM formula\n",
    "    # We already have cross-fitted yhat0, yhat1, Dhat = best_preds_d\n",
    "    yhat = best_preds_y0*(1 - D) + best_preds_y1*D\n",
    "    Dhat = np.clip(best_preds_d, trimming, 1 - trimming)\n",
    "\n",
    "    # DR score\n",
    "    drhat = best_preds_y1 - best_preds_y0 + (y - yhat) * (\n",
    "        D / Dhat - (1 - D)/(1 - Dhat)\n",
    "    )\n",
    "    point = np.mean(drhat)\n",
    "    var = np.var(drhat)\n",
    "    stderr = np.sqrt(var / X.shape[0])\n",
    "\n",
    "    return (\n",
    "        point,\n",
    "        stderr,\n",
    "        yhat,                # cross-fitted E[Y|D=observed, X]\n",
    "        Dhat,                # cross-fitted e(X)\n",
    "        (y - yhat),          # residual in Y space\n",
    "        (D - Dhat),          # residual in D space\n",
    "        drhat\n",
    "    )\n",
    "    \n",
    "modely0_list = [lassoy, rfy, dtry, gbfy]\n",
    "modely1_list = [lassoy, rfy, dtry, gbfy]\n",
    "modeld_list  = [lgrd,  rfd,  dtrd, gbfd]\n",
    "\n",
    "res_all = dr_select_best(\n",
    "    X, D, y, modely0_list, modely1_list, modeld_list, nfolds=5\n",
    ")\n",
    "res_bottom_25 = dr_select_best(\n",
    "    X_bottom, D_bottom, y_bottom, modely0_list, modely1_list, modeld_list, nfolds=5\n",
    ")\n",
    "res_top_25 = dr_select_best(\n",
    "    X_top, D_top, y_top, modely1_list, modely0_list, modeld_list, nfolds=5\n",
    ")\n",
    "table_all = summary(*res_all, X, D, y, name=\"select best IRM with semi cross fitting all samples\")\n",
    "table_bottom_25 = summary(*res_bottom_25, X_bottom, D_bottom, y_bottom, name=f\"select best IRM with semi cross fitting bottom 25\\% income\")\n",
    "table_top_25 = summary(*res_top_25, X_top, D_top, y_top, name=f\"select best IRM with semi cross fitting top 25\\% income\")\n",
    "\n",
    "table_final = pd.concat([table_all, table_bottom_25, table_top_25])\n",
    "table_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------Random Forest in EconML for PLR------------\n",
      "Coefficient Results:  X is None, please call intercept_inference to learn the constant!\n",
      "                        CATE Intercept Results                        \n",
      "======================================================================\n",
      "               point_estimate  stderr  zstat pvalue ci_lower  ci_upper\n",
      "----------------------------------------------------------------------\n",
      "cate_intercept        8603.31 1333.541 6.451    0.0 5989.619 11217.002\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "<sub>A linear parametric conditional average treatment effect (CATE) model was fitted:\n",
      "$Y = \\Theta(X)\\cdot T + g(X, W) + \\epsilon$\n",
      "where for every outcome $i$ and treatment $j$ the CATE $\\Theta_{ij}(X)$ has the form:\n",
      "$\\Theta_{ij}(X) = X' coef_{ij} + cate\\_intercept_{ij}$\n",
      "Coefficient Results table portrays the $coef_{ij}$ parameter vector for each outcome $i$ and treatment $j$. Intercept Results table portrays the $cate\\_intercept_{ij}$ parameter.</sub>\n",
      "-----------------Gradient Boosting in EconML for PLR------------\n",
      "Coefficient Results:  X is None, please call intercept_inference to learn the constant!\n",
      "                        CATE Intercept Results                       \n",
      "=====================================================================\n",
      "               point_estimate  stderr zstat pvalue ci_lower  ci_upper\n",
      "---------------------------------------------------------------------\n",
      "cate_intercept       9129.167 1379.02  6.62    0.0 6426.337 11831.997\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "<sub>A linear parametric conditional average treatment effect (CATE) model was fitted:\n",
      "$Y = \\Theta(X)\\cdot T + g(X, W) + \\epsilon$\n",
      "where for every outcome $i$ and treatment $j$ the CATE $\\Theta_{ij}(X)$ has the form:\n",
      "$\\Theta_{ij}(X) = X' coef_{ij} + cate\\_intercept_{ij}$\n",
      "Coefficient Results table portrays the $coef_{ij}$ parameter vector for each outcome $i$ and treatment $j$. Intercept Results table portrays the $cate\\_intercept_{ij}$ parameter.</sub>\n",
      "-----------------Decision Tree in EconML for PLR------------\n",
      "Coefficient Results:  X is None, please call intercept_inference to learn the constant!\n",
      "                        CATE Intercept Results                        \n",
      "======================================================================\n",
      "               point_estimate  stderr  zstat pvalue ci_lower  ci_upper\n",
      "----------------------------------------------------------------------\n",
      "cate_intercept       8772.347 1448.641 6.056    0.0 5933.064 11611.631\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "<sub>A linear parametric conditional average treatment effect (CATE) model was fitted:\n",
      "$Y = \\Theta(X)\\cdot T + g(X, W) + \\epsilon$\n",
      "where for every outcome $i$ and treatment $j$ the CATE $\\Theta_{ij}(X)$ has the form:\n",
      "$\\Theta_{ij}(X) = X' coef_{ij} + cate\\_intercept_{ij}$\n",
      "Coefficient Results table portrays the $coef_{ij}$ parameter vector for each outcome $i$ and treatment $j$. Intercept Results table portrays the $cate\\_intercept_{ij}$ parameter.</sub>\n",
      "-----------------Random Forest in DoubleML for PLR------------\n",
      "          coef     std err         t         P>|t|        2.5 %        97.5 %\n",
      "d  8522.938727  1346.13888  6.331396  2.429524e-10  5884.555005  11161.322449\n",
      "-----------------Decision Tree in DoubleML for PLR------------\n",
      "          coef      std err        t         P>|t|        2.5 %        97.5 %\n",
      "d  8733.751904  1455.331705  6.00121  1.958519e-09  5881.354176  11586.149631\n",
      "-----------------Gradient Boosting in DoubleML for PLR------------\n",
      "          coef      std err         t         P>|t|        2.5 %        97.5 %\n",
      "d  8834.460857  1366.032444  6.467241  9.980849e-11  6157.086466  11511.835249\n",
      "-----------------Random Forest in EconML for IRM------------\n",
      "Coefficient Results:  X is None, please call intercept_inference to learn the constant!\n",
      "                        CATE Intercept Results                       \n",
      "=====================================================================\n",
      "               point_estimate  stderr  zstat pvalue ci_lower ci_upper\n",
      "---------------------------------------------------------------------\n",
      "cate_intercept       8022.684 1120.937 7.157    0.0 5825.687 10219.68\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "<sub>A linear parametric conditional average treatment effect (CATE) model was fitted:\n",
      "$Y = \\Theta(X)\\cdot T + g(X, W) + \\epsilon$\n",
      "where $T$ is the one-hot-encoding of the discrete treatment and for every outcome $i$ and treatment $j$ the CATE $\\Theta_{ij}(X)$ has the form:\n",
      "$\\Theta_{ij}(X) = \\phi(X)' coef_{ij} + cate\\_intercept_{ij}$\n",
      "where $\\phi(X)$ is the output of the `featurizer` or $X$ if `featurizer`=None. Coefficient Results table portrays the $coef_{ij}$ parameter vector for each outcome $i$ and the designated treatment $j$ passed to summary. Intercept Results table portrays the $cate\\_intercept_{ij}$ parameter.</sub>\n",
      "-----------------Decision Tree in EconML for IRM------------\n",
      "Coefficient Results:  X is None, please call intercept_inference to learn the constant!\n",
      "                        CATE Intercept Results                        \n",
      "======================================================================\n",
      "               point_estimate  stderr  zstat pvalue ci_lower  ci_upper\n",
      "----------------------------------------------------------------------\n",
      "cate_intercept       8351.725 1250.165  6.68    0.0 5901.447 10802.002\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "<sub>A linear parametric conditional average treatment effect (CATE) model was fitted:\n",
      "$Y = \\Theta(X)\\cdot T + g(X, W) + \\epsilon$\n",
      "where $T$ is the one-hot-encoding of the discrete treatment and for every outcome $i$ and treatment $j$ the CATE $\\Theta_{ij}(X)$ has the form:\n",
      "$\\Theta_{ij}(X) = \\phi(X)' coef_{ij} + cate\\_intercept_{ij}$\n",
      "where $\\phi(X)$ is the output of the `featurizer` or $X$ if `featurizer`=None. Coefficient Results table portrays the $coef_{ij}$ parameter vector for each outcome $i$ and the designated treatment $j$ passed to summary. Intercept Results table portrays the $cate\\_intercept_{ij}$ parameter.</sub>\n",
      "-----------------Gradient Boosting in EconML for IRM------------\n",
      "Coefficient Results:  X is None, please call intercept_inference to learn the constant!\n",
      "                        CATE Intercept Results                        \n",
      "======================================================================\n",
      "               point_estimate  stderr  zstat pvalue ci_lower  ci_upper\n",
      "----------------------------------------------------------------------\n",
      "cate_intercept       8118.252 1135.288 7.151    0.0 5893.129 10343.376\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "<sub>A linear parametric conditional average treatment effect (CATE) model was fitted:\n",
      "$Y = \\Theta(X)\\cdot T + g(X, W) + \\epsilon$\n",
      "where $T$ is the one-hot-encoding of the discrete treatment and for every outcome $i$ and treatment $j$ the CATE $\\Theta_{ij}(X)$ has the form:\n",
      "$\\Theta_{ij}(X) = \\phi(X)' coef_{ij} + cate\\_intercept_{ij}$\n",
      "where $\\phi(X)$ is the output of the `featurizer` or $X$ if `featurizer`=None. Coefficient Results table portrays the $coef_{ij}$ parameter vector for each outcome $i$ and the designated treatment $j$ passed to summary. Intercept Results table portrays the $cate\\_intercept_{ij}$ parameter.</sub>\n",
      "-----------------Random Forest in DoubleML for IRM------------\n",
      "          coef      std err         t         P>|t|        2.5 %        97.5 %\n",
      "d  7845.776236  1130.115803  6.942453  3.853482e-12  5630.789963  10060.762509\n",
      "-----------------Decision Tree in DoubleML for IRM------------\n",
      "          coef      std err         t         P>|t|        2.5 %        97.5 %\n",
      "d  7714.625633  1237.942501  6.231813  4.610689e-10  5288.302916  10140.948349\n",
      "-----------------Gradient Boosting in DoubleML for IRM------------\n",
      "          coef      std err         t         P>|t|        2.5 %        97.5 %\n",
      "d  8421.633643  1139.766509  7.388911  1.480359e-13  6187.732334  10655.534952\n"
     ]
    }
   ],
   "source": [
    " #c.) \n",
    "W = StandardScaler().fit_transform(transformer.fit_transform(X))\n",
    "# PLR in econml\n",
    "# ! pip install econml\n",
    "from econml.dml import LinearDML\n",
    "\n",
    "# random forest in econml\n",
    "ldml_rf = LinearDML(\n",
    "    model_y=RandomForestRegressor(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "    model_t=RandomForestClassifier(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "    cv=5,\n",
    "    discrete_treatment=True,\n",
    "    random_state=123\n",
    ").fit(y, D, W=W)\n",
    "print(\"-----------------Random Forest in EconML for PLR------------\")\n",
    "print(ldml_rf.summary())\n",
    "\n",
    "# gradient boosting in econml\n",
    "ldml_gb = LinearDML(\n",
    "    model_y=GradientBoostingRegressor(max_depth=2, n_iter_no_change=5, random_state=123),\n",
    "    model_t=GradientBoostingClassifier(max_depth=2, n_iter_no_change=5, random_state=123),\n",
    "    cv=5,\n",
    "    discrete_treatment=True,\n",
    "    random_state=123\n",
    ").fit(y, D, W=W)\n",
    "print(\"-----------------Gradient Boosting in EconML for PLR------------\")\n",
    "print(ldml_gb.summary())\n",
    "\n",
    "# PLR with Decision Tree\n",
    "ldml_dt = LinearDML(\n",
    "    model_y=DecisionTreeRegressor(min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "    model_t=DecisionTreeClassifier(min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "    cv=5,\n",
    "    discrete_treatment=True,\n",
    "    random_state=123\n",
    ").fit(y, D, W=W)\n",
    "print(\"-----------------Decision Tree in EconML for PLR------------\")\n",
    "print(ldml_dt.summary())\n",
    "\n",
    "# plr in double ml\n",
    "# ! pip install doubleml\n",
    "from doubleml import DoubleMLData\n",
    "import doubleml as dbml\n",
    "\n",
    "dml_data = DoubleMLData.from_arrays(W, y, D)\n",
    "# random forest\n",
    "dml_plr_rf = dbml.DoubleMLPLR(\n",
    "    dml_data,\n",
    "    RandomForestRegressor(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "    RandomForestClassifier(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "    n_folds=5,\n",
    ")\n",
    "dml_plr_rf.fit()\n",
    "print(\"-----------------Random Forest in DoubleML for PLR------------\")\n",
    "print(dml_plr_rf.summary)\n",
    "\n",
    "# decision tree\n",
    "dml_plr_dt = dbml.DoubleMLPLR(\n",
    "    dml_data,\n",
    "    DecisionTreeRegressor(min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "    DecisionTreeClassifier(min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "    n_folds=5,\n",
    ")\n",
    "\n",
    "dml_plr_dt.fit()\n",
    "print(\"-----------------Decision Tree in DoubleML for PLR------------\")\n",
    "print(dml_plr_dt.summary)\n",
    "\n",
    "# gradient boosting\n",
    "\n",
    "dml_plr_gb = dbml.DoubleMLPLR(\n",
    "    dml_data,\n",
    "    GradientBoostingRegressor(max_depth=2, n_iter_no_change=5, random_state=123),\n",
    "    GradientBoostingClassifier(max_depth=2, n_iter_no_change=5, random_state=123),\n",
    "    n_folds=5,\n",
    ")\n",
    "\n",
    "dml_plr_gb.fit() \n",
    "print(\"-----------------Gradient Boosting in DoubleML for PLR------------\")\n",
    "print(dml_plr_gb.summary)\n",
    "\n",
    "\n",
    "# irm with econml\n",
    "\n",
    "from econml.dr import LinearDRLearner\n",
    "\n",
    "# random forest\n",
    "dr_forest = LinearDRLearner(\n",
    "    model_regression=RandomForestRegressor(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "    model_propensity=RandomForestClassifier(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "    cv=5,\n",
    ")\n",
    "dr_forest.fit(y, D, W=W)\n",
    "print(\"-----------------Random Forest in EconML for IRM------------\")\n",
    "print(dr_forest.summary(T=1))\n",
    "\n",
    "# decision tree\n",
    "dr_tree = LinearDRLearner(\n",
    "    model_regression=DecisionTreeRegressor(min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "    model_propensity=DecisionTreeClassifier(min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "    cv=5,\n",
    ")\n",
    "dr_tree.fit(y, D, W=W)\n",
    "print(\"-----------------Decision Tree in EconML for IRM------------\")\n",
    "print(dr_tree.summary(T=1))\n",
    "\n",
    "# gradient boosting\n",
    "dr_gb = LinearDRLearner(\n",
    "    model_regression=GradientBoostingRegressor(max_depth=2, n_iter_no_change=5, random_state=123),\n",
    "    model_propensity=GradientBoostingClassifier(max_depth=2, n_iter_no_change=5, random_state=123),\n",
    "    cv=5,\n",
    ")\n",
    "dr_gb.fit(y, D, W=W)\n",
    "print(\"-----------------Gradient Boosting in EconML for IRM------------\")\n",
    "print(dr_gb.summary(T=1))\n",
    "# irm with double ml\n",
    "\n",
    "# random forest\n",
    "dml_irm_rf = dbml.DoubleMLIRM(\n",
    "    dml_data,\n",
    "    RandomForestRegressor(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "    RandomForestClassifier(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "    n_folds=5,\n",
    ")\n",
    "dml_irm_rf.fit()\n",
    "print(\"-----------------Random Forest in DoubleML for IRM------------\")\n",
    "print(dml_irm_rf.summary)\n",
    "\n",
    "# decision tree\n",
    "\n",
    "dml_irm_dt = dbml.DoubleMLIRM(\n",
    "    dml_data,\n",
    "    DecisionTreeRegressor(min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "    DecisionTreeClassifier(min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "    n_folds=5,\n",
    ")\n",
    "dml_irm_dt.fit()\n",
    "print(\"-----------------Decision Tree in DoubleML for IRM------------\")\n",
    "print(dml_irm_dt.summary)\n",
    "# gradient boosting\n",
    "\n",
    "dml_irm_gb = dbml.DoubleMLIRM(\n",
    "    dml_data,\n",
    "    GradientBoostingRegressor(max_depth=2, n_iter_no_change=5, random_state=123),\n",
    "    GradientBoostingClassifier(max_depth=2, n_iter_no_change=5, \n",
    "                               \n",
    "                               random_state=123),\n",
    "    n_folds=5,\n",
    ")\n",
    "dml_irm_gb.fit()\n",
    "print(\"-----------------Gradient Boosting in DoubleML for IRM------------\")\n",
    "print(dml_irm_gb.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Econml can work with all the base learners (random forest, decision tree, boosted forest), as can doubleML. Both can work with an scikit-learn model in fact. Theoretically, one could also build a custom class that implements the scikit api for stacking or semi crossfitting with choosing the best model, but neither library can work directly with stacking or perform the semi cross-fitting with the custom implementations we built as far as I could tell from the documentation of the packages.   \n",
    "  \n",
    "As for the results, for PLR (note that results may change slightly upon rerunning/exporting due to randomness. Estimate first, then standard error in parentheses): \n",
    "\n",
    "random forest previous:  8905 (1357)  \n",
    "random forest econml:  8603 (1333)   \n",
    "random forest doubleML: 8523 (1346)  \n",
    "  \n",
    "decision tree previous: 9236 (1440)  \n",
    "decision tree econml: 8772 (1449)  \n",
    "decision tree doubleML: 8734 (1455)  \n",
    "  \n",
    "boosted forest previous: 8840 (1334)  \n",
    "boosted forest econml: 9129 (1379)  \n",
    "boosted forest doubleML: 8834 (1366)  \n",
    "\n",
    "For IRM:  \n",
    "random forest previous: 7699 (1159)  \n",
    "random forest econml: 8023 (1121)  \n",
    "random forest doubleML: 7805 (1155)  \n",
    "\n",
    "decision tree previous: 7836 (1255)  \n",
    "decision tree econml: 8352 (1250)  \n",
    "decision tree doubleML: 7714 (1238) \n",
    "\n",
    "boosted forest previous: 8593 (1157)  \n",
    "boosted forest econml: 8118 (1135)  \n",
    "boosted forest doubleML: 8683 (1258)  \n",
    "\n",
    "The results are broadly consistent across the different methods, including for the decision tree. Apart from the decision tree, the results are also consistent with the estimates from the custom methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True ATE in the semi-synthetic world:  7448.65797745209\n",
      "\n",
      "=== Semi-Synthetic Data with n=1000 ===\n",
      "** PLR Results **\n",
      "                             estimate       stderr        lower         upper  \\\n",
      "double lasso              8517.648973  4225.009545   236.630265  16798.667681   \n",
      "lasso/logistic            8272.461258  4309.573350  -174.302508  16719.225024   \n",
      "random forest            10280.090121  4048.857480  2344.329460  18215.850782   \n",
      "decision tree            13993.974502  3957.574687  6237.128114  21750.820889   \n",
      "boosted forest           10744.395104  4466.305859  1990.435621  19498.354588   \n",
      "automl (semi-cfit)       11299.890740  4173.275529  3120.270703  19479.510777   \n",
      "stacked (semi-cfit)       8459.513070  4121.148148   382.062700  16536.963441   \n",
      "select-best (semi-cfit)   8272.461258  4309.573350  -174.302508  16719.225024   \n",
      "\n",
      "                               rmse y    rmse D  accuracy D        error  \\\n",
      "double lasso             58417.628247  0.461692       0.643  1068.990996   \n",
      "lasso/logistic           58417.628247  0.460641       0.647   823.803281   \n",
      "random forest            59759.768020  0.463541       0.648  2831.432144   \n",
      "decision tree            66992.081776  0.519073       0.604  6545.316524   \n",
      "boosted forest           63428.842385  0.466237       0.624  3295.737127   \n",
      "automl (semi-cfit)       60357.047091  0.461395       0.647  3851.232762   \n",
      "stacked (semi-cfit)      57878.405348  0.458692       0.661  1010.855093   \n",
      "select-best (semi-cfit)  58417.628247  0.460641       0.647   823.803281   \n",
      "\n",
      "                         rmse E[y|D,X]  rmse E[D|X]  covered  \n",
      "double lasso              14263.844056     0.072905        1  \n",
      "lasso/logistic            14263.844056     0.079331        1  \n",
      "random forest             19076.366886     0.092830        1  \n",
      "decision tree             32599.221080     0.261179        1  \n",
      "boosted forest            26504.540650     0.101954        1  \n",
      "automl (semi-cfit)        20378.464731     0.078377        1  \n",
      "stacked (semi-cfit)       15588.081866     0.072947        1  \n",
      "select-best (semi-cfit)   14263.844056     0.079331        1  \n",
      "** IRM Results **\n",
      "                             estimate        stderr         lower  \\\n",
      "lasso/logistic            9162.611121   3967.123470   1387.049120   \n",
      "random forest             9460.352755   3927.264154   1762.915013   \n",
      "decision tree            10081.426461  21067.521821 -31210.916308   \n",
      "boosted forest            7937.519186   3967.618074    160.987761   \n",
      "automl (semi-cfit)       10088.590473   3589.528453   3053.114706   \n",
      "stacked (semi-cfit)       8715.068505   3803.604825   1260.003047   \n",
      "select-best (semi-cfit)   8213.246909   3997.528074    378.091884   \n",
      "\n",
      "                                upper        rmse y    rmse D  accuracy D  \\\n",
      "lasso/logistic           16938.173122  58715.553937  0.464368       0.637   \n",
      "random forest            17157.790496  59294.418350  0.461790       0.654   \n",
      "decision tree            51373.769231  62657.647472  0.518895       0.604   \n",
      "boosted forest           15714.050612  68915.905670  0.463604       0.634   \n",
      "automl (semi-cfit)       17124.066240  60266.606846  0.462159       0.647   \n",
      "stacked (semi-cfit)      16170.133963  58281.316536  0.458528       0.663   \n",
      "select-best (semi-cfit)  16048.401933  58804.552552  0.460641       0.647   \n",
      "\n",
      "                               error  rmse E[y|D,X]  rmse E[D|X]  covered  \n",
      "lasso/logistic           1713.953143   14827.780706     0.084054        1  \n",
      "random forest            2011.694777   19110.963134     0.091256        1  \n",
      "decision tree            2632.768484   27805.749907     0.258108        1  \n",
      "boosted forest            488.861209   39165.971935     0.088068        1  \n",
      "automl (semi-cfit)       2639.932496   18872.000088     0.092433        1  \n",
      "stacked (semi-cfit)      1266.410528   15326.756455     0.072159        1  \n",
      "select-best (semi-cfit)   764.588931   15583.377134     0.079331        1  \n"
     ]
    }
   ],
   "source": [
    "# d.) \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_predict, KFold\n",
    "from sklearn.base import clone\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class semisynth:\n",
    "    \n",
    "    def fit(self, X, D, y, transformer, random_state=None):\n",
    "        \"\"\"\n",
    "        X, D, y: the real data\n",
    "        transformer: any sklearn-compatible Transformer for pre-processing\n",
    "        \"\"\"\n",
    "        self.X_ = X.copy()\n",
    "\n",
    "        # Model for Y|D=0\n",
    "        self.est0_ = make_pipeline(transformer,\n",
    "                                   RandomForestRegressor(min_samples_leaf=20,\n",
    "                                                         ccp_alpha=0.001,\n",
    "                                                         random_state=random_state)\n",
    "                                  ).fit(X[D==0], y[D==0])\n",
    "        self.res0_ = y[D==0] - self.est0_.predict(X[D==0])\n",
    "        # De-mean the residual distribution\n",
    "        self.res0_ -= np.mean(self.res0_)\n",
    "\n",
    "        # Model for Y|D=1\n",
    "        self.est1_ = make_pipeline(transformer,\n",
    "                                   RandomForestRegressor(min_samples_leaf=20,\n",
    "                                                         ccp_alpha=0.001,\n",
    "                                                         random_state=random_state)\n",
    "                                  ).fit(X[D==1], y[D==1])\n",
    "        self.res1_ = y[D==1] - self.est1_.predict(X[D==1])\n",
    "        self.res1_ -= np.mean(self.res1_)\n",
    "\n",
    "        # Model for D|X\n",
    "        self.prop_ = make_pipeline(transformer,\n",
    "                                   RandomForestClassifier(min_samples_leaf=20,\n",
    "                                                          ccp_alpha=0.001,\n",
    "                                                          random_state=random_state)\n",
    "                                  ).fit(X, D)\n",
    "        return self\n",
    "\n",
    "    def generate_data(self, n):\n",
    "        \"\"\"\n",
    "        Returns (X, D, Y, Y1, Y0):\n",
    "          X, D, Y: the new sample\n",
    "          Y1, Y0: potential outcomes for each row\n",
    "        \"\"\"\n",
    "        # Resample X from the empirical distribution\n",
    "        X = self.X_.iloc[np.random.choice(self.X_.shape[0], n, replace=True)]\n",
    "        \n",
    "        # Simulate D ~ Bernoulli(\\hat{p}(X))\n",
    "        pX = self.prop_.predict_proba(X)[:, 1]\n",
    "        D = np.random.binomial(1, pX)\n",
    "\n",
    "        # Construct Y0, Y1 by re-sampling from residual distribution\n",
    "        y0 = self.est0_.predict(X) + self.res0_[np.random.choice(self.res0_.shape[0], n, replace=True)]\n",
    "        y1 = self.est1_.predict(X) + self.res1_[np.random.choice(self.res1_.shape[0], n, replace=True)]\n",
    "        \n",
    "        # Observed Y\n",
    "        y = y0*(1 - D) + y1*D\n",
    "        return X, D, y, y1, y0\n",
    "    \n",
    "    def y_cef(self, X, D):\n",
    "        \"\"\"\n",
    "        Returns the 'true' E[Y|X, D] in the semi-synthetic world\n",
    "        = the random forest predictions from the original data\n",
    "        \"\"\"\n",
    "        return self.est1_.predict(X)*D + self.est0_.predict(X)*(1 - D)\n",
    "    \n",
    "    def D_cef(self, X):\n",
    "        \"\"\"\n",
    "        Returns the 'true' E[D|X] in the semi-synthetic world\n",
    "        \"\"\"\n",
    "        return self.prop_.predict_proba(X)[:, 1]\n",
    "\n",
    "    @property\n",
    "    def true_ate(self):\n",
    "        \"\"\"\n",
    "        The 'true' ATE in the semi-synthetic world, i.e. E[f1(X) - f0(X)]\n",
    "        using the entire original X_ distribution.\n",
    "        \"\"\"\n",
    "        return np.mean(self.est1_.predict(self.X_) - self.est0_.predict(self.X_))\n",
    "\n",
    "\n",
    "def summary(\n",
    "    point, stderr,\n",
    "    yhat, Dhat,    # cross-fitted predictions for y and D\n",
    "    resy, resD,    # residuals y-yhat, D-Dhat\n",
    "    final_residual, # epsilon or drhat\n",
    "    X, D, y,\n",
    "    *,\n",
    "    name,\n",
    "    synth  # the semisynth object, so we can compare to the \"true\" functions\n",
    "):\n",
    "    true_ate = synth.true_ate\n",
    "    covered = (point - 1.96*stderr <= true_ate <= point + 1.96*stderr)\n",
    "\n",
    "    # We'll compute the \"true\" E[Y|D,X], E[D|X]\n",
    "    y_cef_true = synth.y_cef(X, D)\n",
    "    d_cef_true = synth.D_cef(X)\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'estimate':    [point],\n",
    "        'stderr':      [stderr],\n",
    "        'lower':       [point - 1.96*stderr],\n",
    "        'upper':       [point + 1.96*stderr],\n",
    "        'rmse y':      [np.sqrt(np.mean(resy**2))],  # RMSE vs. *observed* Y\n",
    "        'rmse D':      [np.sqrt(np.mean(resD**2))],  # RMSE vs. *observed* D\n",
    "        'accuracy D':  [np.mean(np.abs(resD) < 0.5)],# classification accuracy\n",
    "        # New columns:\n",
    "        'error':       [abs(point - true_ate)],    # how far from true\n",
    "        'rmse E[y|D,X]':[np.sqrt(np.mean((yhat - y_cef_true)**2))],\n",
    "        'rmse E[D|X]': [np.sqrt(np.mean((Dhat - d_cef_true)**2))],\n",
    "        'covered':     [1 if covered else 0]       # did CI cover true ATE?\n",
    "    }, index=[name])\n",
    "    \n",
    "    \n",
    "from copy import deepcopy\n",
    "\n",
    "synth = semisynth().fit(X, D, y, transformer, random_state=123)\n",
    "print(\"True ATE in the semi-synthetic world: \", synth.true_ate)\n",
    "\n",
    "\n",
    "def run_plr_methods(X_train, D_train, y_train, synth):\n",
    "    \"\"\"\n",
    "    X_train, D_train, y_train come from synth.generate_data(...)\n",
    "    We'll replicate your DML approach for partial linear model\n",
    "    with multiple model combos and stack them in a table.\n",
    "    \"\"\"\n",
    "    results_table = []\n",
    "\n",
    "    # 1) Double Lasso with cross-fitting\n",
    "    # (a) specify pipelines\n",
    "    lassoy_ = deepcopy(lassoy)\n",
    "    lassod_ = deepcopy(lassod)\n",
    "    # (b) run\n",
    "    point, stderr, yhat, Dhat, resy, resD, eps = dml(X_train, D_train, y_train, lassoy_, lassod_, nfolds=5)\n",
    "    # (c) summary\n",
    "    df_ = summary(point, stderr, yhat, Dhat, resy, resD, eps,\n",
    "                  X_train, D_train, y_train, name='double lasso', synth=synth)\n",
    "    results_table.append(df_)\n",
    "\n",
    "    # 2) lasso / logistic\n",
    "    lassoy_ = deepcopy(lassoy)\n",
    "    lgrd_   = deepcopy(lgrd)\n",
    "    point, stderr, yhat, Dhat, resy, resD, eps = dml(X_train, D_train, y_train,\n",
    "                                                     lassoy_, lgrd_, nfolds=5, classifier=True)\n",
    "    df_ = summary(point, stderr, yhat, Dhat, resy, resD, eps,\n",
    "                  X_train, D_train, y_train, name='lasso/logistic', synth=synth)\n",
    "    results_table.append(df_)\n",
    "\n",
    "    # 3) Random Forest\n",
    "    rfy_ = deepcopy(rfy)\n",
    "    rfd_ = deepcopy(rfd)\n",
    "    point, stderr, yhat, Dhat, resy, resD, eps = dml(X_train, D_train, y_train,\n",
    "                                                     rfy_, rfd_, nfolds=5, classifier=True)\n",
    "    df_ = summary(point, stderr, yhat, Dhat, resy, resD, eps,\n",
    "                  X_train, D_train, y_train, name='random forest', synth=synth)\n",
    "    results_table.append(df_)\n",
    "\n",
    "    # 4) Decision Tree\n",
    "    dtry_ = deepcopy(dtry)\n",
    "    dtrd_ = deepcopy(dtrd)\n",
    "    point, stderr, yhat, Dhat, resy, resD, eps = dml(X_train, D_train, y_train,\n",
    "                                                     dtry_, dtrd_, nfolds=5, classifier=True)\n",
    "    df_ = summary(point, stderr, yhat, Dhat, resy, resD, eps,\n",
    "                  X_train, D_train, y_train, name='decision tree', synth=synth)\n",
    "    results_table.append(df_)\n",
    "\n",
    "    # 5) Boosted trees\n",
    "    gbfy_ = deepcopy(gbfy)\n",
    "    gbfd_ = deepcopy(gbfd)\n",
    "    point, stderr, yhat, Dhat, resy, resD, eps = dml(X_train, D_train, y_train,\n",
    "                                                     gbfy_, gbfd_, nfolds=5, classifier=True)\n",
    "    df_ = summary(point, stderr, yhat, Dhat, resy, resD, eps,\n",
    "                  X_train, D_train, y_train, name='boosted forest', synth=synth)\n",
    "    results_table.append(df_)\n",
    "\n",
    "    # 6) automl (semi-cfit)\n",
    "    # Similarly for stacking (semi-cfit).\n",
    "    flamly_ = make_pipeline(transformer, AutoML(time_budget=50,\n",
    "                                                task='regression',\n",
    "                                                early_stop=True,\n",
    "                                                eval_method='cv',\n",
    "                                                n_splits=3,\n",
    "                                                metric='r2',\n",
    "                                                verbose=0))\n",
    "    flamld_ = make_pipeline(transformer, AutoML(time_budget=50,\n",
    "                                                task='classification',\n",
    "                                                early_stop=True,\n",
    "                                                eval_method='cv',\n",
    "                                                n_splits=3,\n",
    "                                                metric='r2',\n",
    "                                                verbose=0))\n",
    "    # Fit Y, D on entire X_train\n",
    "    flamly_.fit(X_train, y_train)\n",
    "    besty_model = flamly_[-1].best_model_for_estimator(flamly_[-1].best_estimator)\n",
    "    besty = make_pipeline(transformer, clone(besty_model))\n",
    "\n",
    "    flamld_.fit(X_train, D_train)\n",
    "    bestd_model = flamld_[-1].best_model_for_estimator(flamld_[-1].best_estimator)\n",
    "    bestd = make_pipeline(transformer, clone(bestd_model))\n",
    "\n",
    "    point, stderr, yhat, Dhat, resy, resD, eps = dml(X_train, D_train, y_train,\n",
    "                                                     besty, bestd, nfolds=5, classifier=True)\n",
    "    df_ = summary(point, stderr, yhat, Dhat, resy, resD, eps,\n",
    "                  X_train, D_train, y_train, name='automl (semi-cfit)', synth=synth)\n",
    "    results_table.append(df_)\n",
    "\n",
    "    # 7) stacking (semi-cfit)\n",
    "    point, stderr, yhat, Dhat, resy, resD, eps = dml_dirty(\n",
    "        X_train, D_train, y_train,\n",
    "        [lassoy, rfy, dtry, gbfy],\n",
    "        [lgrd, rfd, dtrd, gbfd],\n",
    "        nfolds=5, classifier=True\n",
    "    )\n",
    "    df_ = summary(point, stderr, yhat, Dhat, resy, resD, eps,\n",
    "                  X_train, D_train, y_train, name='stacked (semi-cfit)', synth=synth)\n",
    "    results_table.append(df_)\n",
    "    \n",
    "    #8 select best\n",
    "    point, stderr, yhat, Dhat, resy, resD, eps = dml_select_best(\n",
    "        X_train, D_train, y_train,\n",
    "        [lassoy, rfy, dtry, gbfy],\n",
    "        [lgrd, rfd, dtrd, gbfd],\n",
    "        nfolds=5, classifier=True\n",
    "    )\n",
    "    df_ = summary(point, stderr, yhat, Dhat, resy, resD, eps,\n",
    "                    X_train, D_train, y_train, name='select-best (semi-cfit)', synth=synth)\n",
    "    results_table.append(df_)\n",
    "\n",
    "    return pd.concat(results_table)\n",
    "\n",
    "\n",
    "def run_irm_methods(X_train, D_train, y_train, synth):\n",
    "    \"\"\"\n",
    "    X_train, D_train, y_train from synth.generate_data(...)\n",
    "    We'll replicate your IRM approach with multiple model combos and stack in a table.\n",
    "    \"\"\"\n",
    "    results_table = []\n",
    "\n",
    "    # 1) lasso-lasso, logistic repeated seeds + median aggregator\n",
    "    # We'll do a single run for demonstration:\n",
    "    lassoy_ = deepcopy(lassoytest)  # or define a pipeline as in the notebook\n",
    "    lgrd_   = deepcopy(lgrdtest)\n",
    "\n",
    "    point, stderr, yhat, Dhat, resy, resD, drhat = dr(X_train, D_train, y_train,\n",
    "                                                      lassoy_, lassoy_,\n",
    "                                                      lgrd_, nfolds=5)\n",
    "    df_ = summary(point, stderr, yhat, Dhat, resy, resD, drhat,\n",
    "                  X_train, D_train, y_train, name='lasso/logistic', synth=synth)\n",
    "    results_table.append(df_)\n",
    "\n",
    "    # 2) random forest\n",
    "    rfy_ = deepcopy(rfy)\n",
    "    rfd_ = deepcopy(rfd)\n",
    "    point, stderr, yhat, Dhat, resy, resD, drhat = dr(X_train, D_train, y_train,\n",
    "                                                      rfy_, rfy_, rfd_, nfolds=5)\n",
    "    df_ = summary(point, stderr, yhat, Dhat, resy, resD, drhat,\n",
    "                  X_train, D_train, y_train, name='random forest', synth=synth)\n",
    "    results_table.append(df_)\n",
    "\n",
    "    # 3) decision tree\n",
    "    dtry_ = deepcopy(dtry)\n",
    "    dtrd_ = deepcopy(dtrd)\n",
    "    point, stderr, yhat, Dhat, resy, resD, drhat = dr(X_train, D_train, y_train,\n",
    "                                                      dtry_, dtry_, dtrd_, nfolds=5)\n",
    "    df_ = summary(point, stderr, yhat, Dhat, resy, resD, drhat,\n",
    "                  X_train, D_train, y_train, name='decision tree', synth=synth)\n",
    "    results_table.append(df_)\n",
    "\n",
    "    # 4) boosted forest\n",
    "    gbfy_ = deepcopy(gbfy)\n",
    "    gbfd_ = deepcopy(gbfd)\n",
    "    point, stderr, yhat, Dhat, resy, resD, drhat = dr(X_train, D_train, y_train,\n",
    "                                                      gbfy_, gbfy_, gbfd_, nfolds=5)\n",
    "    df_ = summary(point, stderr, yhat, Dhat, resy, resD, drhat,\n",
    "                  X_train, D_train, y_train, name='boosted forest', synth=synth)\n",
    "    results_table.append(df_)\n",
    "\n",
    "    # 5) automl\n",
    "    flamly0_ = make_pipeline(transformer, AutoML(time_budget=30, task='regression', early_stop=True,\n",
    "                                                eval_method='cv', n_splits=3, metric='r2', verbose=0))\n",
    "    flamly1_ = make_pipeline(transformer, AutoML(time_budget=30, task='regression', early_stop=True,\n",
    "                                                eval_method='cv', n_splits=3, metric='r2', verbose=0))\n",
    "    flamld_  = make_pipeline(transformer, AutoML(time_budget=30, task='classification', early_stop=True,\n",
    "                                                eval_method='cv', n_splits=3, metric='r2', verbose=0))\n",
    "    # Fit for Y|D=0, Y|D=1\n",
    "    flamly0_.fit(X_train[D_train == 0], y_train[D_train == 0])\n",
    "    besty0_model = flamly0_[-1].best_model_for_estimator(flamly0_[-1].best_estimator)\n",
    "    besty0 = make_pipeline(transformer, clone(besty0_model))\n",
    "\n",
    "    flamly1_.fit(X_train[D_train == 1], y_train[D_train == 1])\n",
    "    besty1_model = flamly1_[-1].best_model_for_estimator(flamly1_[-1].best_estimator)\n",
    "    besty1 = make_pipeline(transformer, clone(besty1_model))\n",
    "\n",
    "    flamld_.fit(X_train, D_train)\n",
    "    bestd_model = flamld_[-1].best_model_for_estimator(flamld_[-1].best_estimator)\n",
    "    bestd = make_pipeline(transformer, clone(bestd_model))\n",
    "\n",
    "    point, stderr, yhat, Dhat, resy, resD, drhat = dr(X_train, D_train, y_train,\n",
    "                                                      besty0, besty1, bestd,\n",
    "                                                      nfolds=5)\n",
    "    df_ = summary(point, stderr, yhat, Dhat, resy, resD, drhat,\n",
    "                  X_train, D_train, y_train, name='automl (semi-cfit)', synth=synth)\n",
    "    results_table.append(df_)\n",
    "\n",
    "    # 6) stacking (semi-cfit):\n",
    "    lassoy_ = deepcopy(lassoy)\n",
    "    rfy_ = deepcopy(rfy)\n",
    "    dtry_ = deepcopy(dtry)\n",
    "    gbfy_ = deepcopy(gbfy)\n",
    "\n",
    "    lgrd_ = deepcopy(lgrd)\n",
    "    rfd_  = deepcopy(rfd)\n",
    "    dtrd_ = deepcopy(dtrd)\n",
    "    gbfd_ = deepcopy(gbfd)\n",
    "\n",
    "    point, stderr, yhat, Dhat, resy, resD, drhat = dr_dirty(\n",
    "        X_train, D_train, y_train,\n",
    "        [lassoy_, rfy_, dtry_, gbfy_],\n",
    "        [lassoy_, rfy_, dtry_, gbfy_],\n",
    "        [lgrd_, rfd_, dtrd_, gbfd_],\n",
    "        nfolds=5\n",
    "    )\n",
    "    df_ = summary(point, stderr, yhat, Dhat, resy, resD, drhat,\n",
    "                  X_train, D_train, y_train, name='stacked (semi-cfit)', synth=synth)\n",
    "    results_table.append(df_)\n",
    "    \n",
    "    # 7) select best\n",
    "    \n",
    "    point, stderr, yhat, Dhat, resy, resD, drhat = dr_select_best(\n",
    "        X_train, D_train, y_train,\n",
    "        [lassoy, rfy, dtry, gbfy],\n",
    "        [lassoy, rfy, dtry, gbfy],\n",
    "        [lgrd, rfd, dtrd, gbfd],\n",
    "        nfolds=5\n",
    "    )\n",
    "    df_ = summary(point, stderr, yhat, Dhat, resy, resD, drhat,\n",
    "                    X_train, D_train, y_train, name='select-best (semi-cfit)', synth=synth)\n",
    "    results_table.append(df_)\n",
    "\n",
    "    return pd.concat(results_table)\n",
    "\n",
    "\n",
    "n=1000\n",
    "\n",
    "print(f\"\\n=== Semi-Synthetic Data with n={n} ===\")\n",
    "X_synth, D_synth, y_synth, y1_synth, y0_synth = synth.generate_data(n)\n",
    "\n",
    "\n",
    "print(\"** PLR Results **\")\n",
    "table_plr = run_plr_methods(X_synth, D_synth, y_synth, synth)\n",
    "print(table_plr)\n",
    "\n",
    "print(\"** IRM Results **\")\n",
    "table_irm = run_irm_methods(X_synth, D_synth, y_synth, synth)\n",
    "print(table_irm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the PLR setting, lasso/logistic regression outperforms all other methods in terms of the estimate. \n",
    "\n",
    "In the IRM setting, the boosted forest outperforms all other methods in terms of the estimate. \n",
    "\n",
    "So neither automl nor stacking perform as well as the best model alone."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs288_alt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
