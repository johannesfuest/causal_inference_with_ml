{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1\n",
    "\n",
    "We start with the PLR part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTHONWARNINGS=ignore\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>estimate</th>\n",
       "      <th>stderr</th>\n",
       "      <th>lower</th>\n",
       "      <th>upper</th>\n",
       "      <th>rmse y</th>\n",
       "      <th>rmse D</th>\n",
       "      <th>accuracy D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>double lasso</th>\n",
       "      <td>9035.120004</td>\n",
       "      <td>1295.135748</td>\n",
       "      <td>6496.653938</td>\n",
       "      <td>11573.586070</td>\n",
       "      <td>54254.468883</td>\n",
       "      <td>0.443406</td>\n",
       "      <td>0.688553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lasso/logistic</th>\n",
       "      <td>9092.508157</td>\n",
       "      <td>1304.398170</td>\n",
       "      <td>6535.887743</td>\n",
       "      <td>11649.128571</td>\n",
       "      <td>54254.468883</td>\n",
       "      <td>0.444043</td>\n",
       "      <td>0.687847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random forest</th>\n",
       "      <td>8871.096519</td>\n",
       "      <td>1348.341577</td>\n",
       "      <td>6228.347029</td>\n",
       "      <td>11513.846010</td>\n",
       "      <td>54889.121007</td>\n",
       "      <td>0.444574</td>\n",
       "      <td>0.688553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>decision tree</th>\n",
       "      <td>9236.195678</td>\n",
       "      <td>1440.551643</td>\n",
       "      <td>6412.714457</td>\n",
       "      <td>12059.676898</td>\n",
       "      <td>59427.392172</td>\n",
       "      <td>0.446437</td>\n",
       "      <td>0.688048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>boosted forest</th>\n",
       "      <td>9110.713844</td>\n",
       "      <td>1343.747801</td>\n",
       "      <td>6476.968155</td>\n",
       "      <td>11744.459534</td>\n",
       "      <td>55657.022520</td>\n",
       "      <td>0.443533</td>\n",
       "      <td>0.690267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>automl (semi-cfit)</th>\n",
       "      <td>8877.435525</td>\n",
       "      <td>1300.756781</td>\n",
       "      <td>6327.952234</td>\n",
       "      <td>11426.918816</td>\n",
       "      <td>53908.841872</td>\n",
       "      <td>0.446314</td>\n",
       "      <td>0.689864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stacked (semi-cfit)</th>\n",
       "      <td>8985.081489</td>\n",
       "      <td>1310.988123</td>\n",
       "      <td>6415.544768</td>\n",
       "      <td>11554.618210</td>\n",
       "      <td>54030.500545</td>\n",
       "      <td>0.442871</td>\n",
       "      <td>0.688956</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        estimate       stderr        lower         upper  \\\n",
       "double lasso         9035.120004  1295.135748  6496.653938  11573.586070   \n",
       "lasso/logistic       9092.508157  1304.398170  6535.887743  11649.128571   \n",
       "random forest        8871.096519  1348.341577  6228.347029  11513.846010   \n",
       "decision tree        9236.195678  1440.551643  6412.714457  12059.676898   \n",
       "boosted forest       9110.713844  1343.747801  6476.968155  11744.459534   \n",
       "automl (semi-cfit)   8877.435525  1300.756781  6327.952234  11426.918816   \n",
       "stacked (semi-cfit)  8985.081489  1310.988123  6415.544768  11554.618210   \n",
       "\n",
       "                           rmse y    rmse D  accuracy D  \n",
       "double lasso         54254.468883  0.443406    0.688553  \n",
       "lasso/logistic       54254.468883  0.444043    0.687847  \n",
       "random forest        54889.121007  0.444574    0.688553  \n",
       "decision tree        59427.392172  0.446437    0.688048  \n",
       "boosted forest       55657.022520  0.443533    0.690267  \n",
       "automl (semi-cfit)   53908.841872  0.446314    0.689864  \n",
       "stacked (semi-cfit)  54030.500545  0.442871    0.688956  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%env PYTHONWARNINGS=ignore\n",
    "\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=ConvergenceWarning)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter('ignore')\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\".*did not converge.*\",\n",
    "    category=ConvergenceWarning\n",
    ")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_predict, KFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LassoCV, LinearRegression, LogisticRegressionCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.base import TransformerMixin, BaseEstimator, clone\n",
    "from formulaic import Formula\n",
    "from flaml.automl import AutoML\n",
    "np.random.seed(1234)\n",
    "# set random seed for all other libraries\n",
    "import random\n",
    "random.seed(1234)\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '1234'\n",
    "\n",
    "\n",
    "file = \"https://raw.githubusercontent.com/CausalAIBook/MetricsMLNotebooks/main/data/401k.csv\"\n",
    "data = pd.read_csv(file)\n",
    "y = data['net_tfa'].values\n",
    "D = data['e401'].values\n",
    "D2 = data['p401'].values\n",
    "D3 = data['a401'].values\n",
    "X = data.drop(['e401', 'p401', 'a401', 'tw', 'tfa', 'net_tfa', 'tfa_he',\n",
    "               'hval', 'hmort', 'hequity',\n",
    "               'nifa', 'net_nifa', 'net_n401', 'ira',\n",
    "               'dum91', 'icat', 'ecat', 'zhat',\n",
    "               'i1', 'i2', 'i3', 'i4', 'i5', 'i6', 'i7',\n",
    "               'a1', 'a2', 'a3', 'a4', 'a5'], axis=1)\n",
    "\n",
    "class FormulaTransformer(TransformerMixin, BaseEstimator):\n",
    "\n",
    "    def __init__(self, formula, array=False):\n",
    "        self.formula = formula\n",
    "        self.array = array\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        df = Formula(self.formula).get_model_matrix(X)\n",
    "        if self.array:\n",
    "            return df.values\n",
    "        return df\n",
    "transformer = FormulaTransformer(\"0 + poly(age, degree=6, raw=True) + poly(inc, degree=8, raw=True) \"\n",
    "                                 \"+ poly(educ, degree=4, raw=True) + poly(fsize, degree=2, raw=True) \"\n",
    "                                 \"+ male + marr + twoearn + db + pira + hown\", array=True)\n",
    "\n",
    "def dml(X, D, y, modely, modeld, *, nfolds, classifier=False):\n",
    "    '''\n",
    "    DML for the Partially Linear Model setting with cross-fitting\n",
    "\n",
    "    Input\n",
    "    -----\n",
    "    X: the controls\n",
    "    D: the treatment\n",
    "    y: the outcome\n",
    "    modely: the ML model for predicting the outcome y\n",
    "    modeld: the ML model for predicting the treatment D\n",
    "    nfolds: the number of folds in cross-fitting\n",
    "    classifier: bool, whether the modeld is a classifier or a regressor\n",
    "\n",
    "    Output\n",
    "    ------\n",
    "    point: the point estimate of the treatment effect of D on y\n",
    "    stderr: the standard error of the treatment effect\n",
    "    yhat: the cross-fitted predictions for the outcome y\n",
    "    Dhat: the cross-fitted predictions for the treatment D\n",
    "    resy: the outcome residuals\n",
    "    resD: the treatment residuals\n",
    "    epsilon: the final residual-on-residual OLS regression residual\n",
    "    '''\n",
    "    cv = KFold(n_splits=nfolds, shuffle=True, random_state=123)  # shuffled k-folds\n",
    "    yhat = cross_val_predict(modely, X, y, cv=cv, n_jobs=-1)  # out-of-fold predictions for y\n",
    "    # out-of-fold predictions for D\n",
    "    # use predict or predict_proba dependent on classifier or regressor for D\n",
    "    if classifier:\n",
    "        Dhat = cross_val_predict(modeld, X, D, cv=cv, method='predict_proba', n_jobs=-1)[:, 1]\n",
    "    else:\n",
    "        Dhat = cross_val_predict(modeld, X, D, cv=cv, n_jobs=-1)\n",
    "    # calculate outcome and treatment residuals\n",
    "    resy = y - yhat\n",
    "    resD = D - Dhat\n",
    "\n",
    "    # final stage ols based point estimate and standard error\n",
    "    point = np.mean(resy * resD) / np.mean(resD**2)\n",
    "    epsilon = resy - point * resD\n",
    "    var = np.mean(epsilon**2 * resD**2) / np.mean(resD**2)**2\n",
    "    stderr = np.sqrt(var / X.shape[0])\n",
    "\n",
    "    return point, stderr, yhat, Dhat, resy, resD, epsilon\n",
    "\n",
    "def summary(point, stderr, yhat, Dhat, resy, resD, epsilon, X, D, y, *, name):\n",
    "    '''\n",
    "    Convenience summary function that takes the results of the DML function\n",
    "    and summarizes several estimation quantities and performance metrics.\n",
    "    '''\n",
    "    return pd.DataFrame({'estimate': point,  # point estimate\n",
    "                         'stderr': stderr,  # standard error\n",
    "                         'lower': point - 1.96 * stderr,  # lower end of 95% confidence interval\n",
    "                         'upper': point + 1.96 * stderr,  # upper end of 95% confidence interval\n",
    "                         'rmse y': np.sqrt(np.mean(resy**2)),  # RMSE of model that predicts outcome y\n",
    "                         'rmse D': np.sqrt(np.mean(resD**2)),  # RMSE of model that predicts treatment D\n",
    "                         'accuracy D': np.mean(np.abs(resD) < .5),  # binary classification accuracy of model for D\n",
    "                         }, index=[name])\n",
    "# double lasso with cross-fitting\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "lassoy = make_pipeline(transformer, StandardScaler(), LassoCV(cv=cv))\n",
    "lassod = make_pipeline(transformer, StandardScaler(), LassoCV(cv=cv))\n",
    "result = dml(X, D, y, lassoy, lassod, nfolds=5)\n",
    "table = summary(*result, X, D, y, name='double lasso')\n",
    "\n",
    "# penalized logreg for D (default is l2 penalty)\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "lassoy = make_pipeline(transformer, StandardScaler(), LassoCV(cv=cv))\n",
    "lgrd = make_pipeline(transformer, StandardScaler(), LogisticRegressionCV(cv=cv))\n",
    "result = dml(X, D, y, lassoy, lgrd, nfolds=5, classifier=True)\n",
    "table = pd.concat([table, summary(*result, X, D, y, name='lasso/logistic')])\n",
    "\n",
    "# random forest\n",
    "rfy = make_pipeline(transformer, RandomForestRegressor(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001))\n",
    "rfd = make_pipeline(transformer, RandomForestClassifier(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001))\n",
    "result = dml(X, D, y, rfy, rfd, nfolds=5, classifier=True)\n",
    "table = pd.concat([table, summary(*result, X, D, y, name='random forest')])\n",
    "\n",
    "# decision tree\n",
    "dtry = make_pipeline(transformer, DecisionTreeRegressor(min_samples_leaf=10, ccp_alpha=.001))\n",
    "dtrd = make_pipeline(transformer, DecisionTreeClassifier(min_samples_leaf=10, ccp_alpha=.001))\n",
    "result = dml(X, D, y, dtry, dtrd, nfolds=5, classifier=True)\n",
    "table = pd.concat([table, summary(*result, X, D, y, name='decision tree')])\n",
    "\n",
    "# boosted trees\n",
    "gbfy = make_pipeline(transformer, GradientBoostingRegressor(max_depth=2, n_iter_no_change=5))\n",
    "gbfd = make_pipeline(transformer, GradientBoostingClassifier(max_depth=2, n_iter_no_change=5))\n",
    "result = dml(X, D, y, gbfy, gbfd, nfolds=5, classifier=True)\n",
    "table = pd.concat([table, summary(*result, X, D, y, name='boosted forest')])\n",
    "\n",
    "# semi cross fitting: To avoid the computational cost of performing model selection within each fold (assuming that we don't select among an exponential set of hyperparameters/models in the number of samples), it is ok to perform model selection using all the data and then perform cross-fitting with the selected model\n",
    "flamly = make_pipeline(transformer, AutoML(time_budget=100, task='regression', early_stop=True,\n",
    "                                           eval_method='cv', n_splits=3, metric='r2', verbose=0))\n",
    "flamld = make_pipeline(transformer, AutoML(time_budget=100, task='classification', early_stop=True,\n",
    "                                           eval_method='cv', n_splits=3, metric='r2', verbose=0))\n",
    "flamly.fit(X, y)\n",
    "besty = make_pipeline(transformer, clone(flamly[-1].best_model_for_estimator(flamly[-1].best_estimator)))\n",
    "flamld.fit(X, D)\n",
    "bestd = make_pipeline(transformer, clone(flamld[-1].best_model_for_estimator(flamld[-1].best_estimator)))\n",
    "result = dml(X, D, y, besty, bestd, nfolds=5, classifier=True)\n",
    "table = pd.concat([table, summary(*result, X, D, y, name='automl (semi-cfit)')])\n",
    "\n",
    "# semi cross fitting with stacking\n",
    "def dml_dirty(X, D, y, modely_list, modeld_list, *,\n",
    "              stacker=LinearRegression(), nfolds, classifier=False):\n",
    "    '''\n",
    "    DML for the Partially Linear Model setting with semi-cross-fitting\n",
    "\n",
    "    Input\n",
    "    -----\n",
    "    X: the controls\n",
    "    D: the treatment\n",
    "    y: the outcome\n",
    "    modely: the ML model for predicting the outcome y\n",
    "    modeld: the ML model for predicting the treatment D\n",
    "    stacker: model used to aggregate predictions of each of the base models\n",
    "    nfolds: the number of folds in cross-fitting\n",
    "    classifier: bool, whether the modeld is a classifier or a regressor\n",
    "\n",
    "    Output\n",
    "    ------\n",
    "    point: the point estimate of the treatment effect of D on y\n",
    "    stderr: the standard error of the treatment effect\n",
    "    yhat: the cross-fitted predictions for the outcome y\n",
    "    Dhat: the cross-fitted predictions for the treatment D\n",
    "    resy: the outcome residuals\n",
    "    resD: the treatment residuals\n",
    "    epsilon: the final residual-on-residual OLS regression residual\n",
    "    '''\n",
    "    # construct out-of-fold predictions for each model\n",
    "    cv = KFold(n_splits=nfolds, shuffle=True, random_state=123)\n",
    "    yhats = np.array([cross_val_predict(modely, X, y, cv=cv, n_jobs=-1) for modely in modely_list]).T\n",
    "    if classifier:\n",
    "        Dhats = np.array([cross_val_predict(modeld, X, D, cv=cv, method='predict_proba', n_jobs=-1)[:, 1]\n",
    "                         for modeld in modeld_list]).T\n",
    "    else:\n",
    "        Dhats = np.array([cross_val_predict(modeld, X, D, cv=cv, n_jobs=-1) for modeld in modeld_list]).T\n",
    "    # calculate stacked residuals by finding optimal coefficients\n",
    "    # and weigthing out-of-sample predictions by these coefficients\n",
    "    yhat = stacker.fit(yhats, y).predict(yhats)\n",
    "    Dhat = stacker.fit(Dhats, D).predict(Dhats)\n",
    "    resy = y - yhat\n",
    "    resD = D - Dhat\n",
    "    # go with the stacked residuals\n",
    "    point = np.mean(resy * resD) / np.mean(resD**2)\n",
    "    epsilon = resy - point * resD\n",
    "    var = np.mean(epsilon**2 * resD**2) / np.mean(resD**2)**2\n",
    "    stderr = np.sqrt(var / X.shape[0])\n",
    "    return point, stderr, yhat, Dhat, resy, resD, epsilon\n",
    "\n",
    "result = dml_dirty(X, D, y, [lassoy, rfy, dtry, gbfy], [lgrd, rfd, dtrd, gbfd],\n",
    "                   nfolds=5, classifier=True)\n",
    "table = pd.concat([table, summary(*result, X, D, y, name='stacked (semi-cfit)')])\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Bottom 25% Income Sample ===\n",
      "                                  estimate       stderr        lower  \\\n",
      "bottom25% double lasso         3769.763582  1092.945664  1627.590080   \n",
      "bottom25% lasso/logistic       3803.759038  1072.578658  1701.504869   \n",
      "bottom25% random forest        4391.212252  1092.834491  2249.256649   \n",
      "bottom25% decision tree        3470.960704  1032.092136  1448.060118   \n",
      "bottom25% boosted forest       3937.557332  1095.544442  1790.290227   \n",
      "bottom25% automl (semi-cfit)   3916.284487  1090.867865  1778.183471   \n",
      "bottom25% stacked (semi-cfit)  4020.224735  1085.316134  1893.005112   \n",
      "\n",
      "                                     upper        rmse y    rmse D  accuracy D  \n",
      "bottom25% double lasso         5911.937083  13400.361810  0.343801    0.846433  \n",
      "bottom25% lasso/logistic       5906.013207  13400.361810  0.354799    0.844015  \n",
      "bottom25% random forest        6533.167855  13504.626992  0.345757    0.846030  \n",
      "bottom25% decision tree        5493.861290  14728.635257  0.380351    0.804111  \n",
      "bottom25% boosted forest       6084.824438  13314.675396  0.345368    0.846030  \n",
      "bottom25% automl (semi-cfit)   6054.385502  13485.103617  0.346054    0.843611  \n",
      "bottom25% stacked (semi-cfit)  6147.444359  13348.318672  0.343124    0.845627  \n",
      "\n",
      "=== Top 25% Income Sample ===\n",
      "                                estimate       stderr         lower  \\\n",
      "top25% double lasso         17505.836955  3902.624431   9856.693070   \n",
      "top25% lasso/logistic       18204.194590  3867.405524  10624.079763   \n",
      "top25% random forest        17395.330900  3958.550025   9636.572851   \n",
      "top25% decision tree        15545.230278  3745.904001   8203.258437   \n",
      "top25% boosted forest       17199.833742  4019.987998   9320.657265   \n",
      "top25% automl (semi-cfit)   16710.661055  3858.944487   9147.129861   \n",
      "top25% stacked (semi-cfit)  18117.671222  3886.362885  10500.399967   \n",
      "\n",
      "                                   upper         rmse y    rmse D  accuracy D  \n",
      "top25% double lasso         25154.980841   91393.039963  0.483189    0.601049  \n",
      "top25% lasso/logistic       25784.309418   91393.039963  0.482708    0.601049  \n",
      "top25% random forest        25154.088949   93912.090755  0.484224    0.605889  \n",
      "top25% decision tree        22887.202119  101948.859766  0.544696    0.536103  \n",
      "top25% boosted forest       25079.010219   98327.079190  0.484232    0.596208  \n",
      "top25% automl (semi-cfit)   24274.192249   95109.874876  0.484471    0.602259  \n",
      "top25% stacked (semi-cfit)  25734.942477   91308.030002  0.481461    0.605083  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_bottom = data.loc[data['inc'] <= data['inc'].quantile(0.25)].copy()\n",
    "data_top = data.loc[data['inc'] >= data['inc'].quantile(0.75)].copy()\n",
    "\n",
    "# Define helper function to create X, D, y from a subset\n",
    "def extract_XDy(df):\n",
    "    \"\"\"Given a subset of the 401k data, produce X, D, and y \n",
    "    consistent with the main analysis.\"\"\"\n",
    "    y_ = df['net_tfa'].values\n",
    "    D_ = df['e401'].values\n",
    "    X_ = df.drop([\n",
    "        'e401', 'p401', 'a401', 'tw', 'tfa', 'net_tfa', 'tfa_he',\n",
    "        'hval', 'hmort', 'hequity', 'nifa', 'net_nifa', 'net_n401',\n",
    "        'ira', 'dum91', 'icat', 'ecat', 'zhat', 'i1', 'i2', 'i3',\n",
    "        'i4', 'i5', 'i6', 'i7', 'a1', 'a2', 'a3', 'a4', 'a5'\n",
    "    ], axis=1)\n",
    "    return X_, D_, y_\n",
    "\n",
    "X_bottom, D_bottom, y_bottom = extract_XDy(data_bottom)\n",
    "X_top, D_top, y_top = extract_XDy(data_top)\n",
    "\n",
    "\n",
    "# Create a helper function that runs all models and returns a summary table\n",
    "def run_all_estimators(X, D, y, name_prefix=''):\n",
    "    \"\"\"Runs the pipeline of estimators and returns a summary results table.\"\"\"\n",
    "    table_local = []\n",
    "\n",
    "    # cross-validation setup\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "    \n",
    "    # 1) double lasso\n",
    "    lassoy = make_pipeline(transformer, StandardScaler(), LassoCV(cv=cv))\n",
    "    lassod = make_pipeline(transformer, StandardScaler(), LassoCV(cv=cv))\n",
    "    result = dml(X, D, y, lassoy, lassod, nfolds=5)\n",
    "    table_local.append(summary(*result, X, D, y, name=f'{name_prefix} double lasso'))\n",
    "\n",
    "    # 2) lasso/logistic\n",
    "    lassoy = make_pipeline(transformer, StandardScaler(), LassoCV(cv=cv))\n",
    "    lgrd = make_pipeline(transformer, StandardScaler(), LogisticRegressionCV(cv=cv))\n",
    "    result = dml(X, D, y, lassoy, lgrd, nfolds=5, classifier=True)\n",
    "    table_local.append(summary(*result, X, D, y, name=f'{name_prefix} lasso/logistic'))\n",
    "\n",
    "    # 3) random forest\n",
    "    rfy = make_pipeline(transformer,\n",
    "                        RandomForestRegressor(n_estimators=100,\n",
    "                                              min_samples_leaf=10,\n",
    "                                              ccp_alpha=0.001))\n",
    "    rfd = make_pipeline(transformer,\n",
    "                        RandomForestClassifier(n_estimators=100,\n",
    "                                               min_samples_leaf=10,\n",
    "                                               ccp_alpha=0.001))\n",
    "    result = dml(X, D, y, rfy, rfd, nfolds=5, classifier=True)\n",
    "    table_local.append(summary(*result, X, D, y, name=f'{name_prefix} random forest'))\n",
    "\n",
    "    # 4) decision tree\n",
    "    dtry = make_pipeline(transformer,\n",
    "                         DecisionTreeRegressor(min_samples_leaf=10,\n",
    "                                               ccp_alpha=0.001))\n",
    "    dtrd = make_pipeline(transformer,\n",
    "                         DecisionTreeClassifier(min_samples_leaf=10,\n",
    "                                                ccp_alpha=0.001))\n",
    "    result = dml(X, D, y, dtry, dtrd, nfolds=5, classifier=True)\n",
    "    table_local.append(summary(*result, X, D, y, name=f'{name_prefix} decision tree'))\n",
    "\n",
    "    # 5) boosted trees\n",
    "    gbfy = make_pipeline(transformer,\n",
    "                         GradientBoostingRegressor(max_depth=2,\n",
    "                                                   n_iter_no_change=5))\n",
    "    gbfd = make_pipeline(transformer,\n",
    "                         GradientBoostingClassifier(max_depth=2,\n",
    "                                                    n_iter_no_change=5))\n",
    "    result = dml(X, D, y, gbfy, gbfd, nfolds=5, classifier=True)\n",
    "    table_local.append(summary(*result, X, D, y, name=f'{name_prefix} boosted forest'))\n",
    "\n",
    "    # 6) automl (semi cross-fitting)\n",
    "    flamly = make_pipeline(transformer,\n",
    "                           AutoML(time_budget=60,  # reduce if desired\n",
    "                                  task='regression',\n",
    "                                  early_stop=True,\n",
    "                                  eval_method='cv',\n",
    "                                  n_splits=3,\n",
    "                                  metric='r2',\n",
    "                                  verbose=0))\n",
    "    flamld = make_pipeline(transformer,\n",
    "                           AutoML(time_budget=60,\n",
    "                                  task='classification',\n",
    "                                  early_stop=True,\n",
    "                                  eval_method='cv',\n",
    "                                  n_splits=3,\n",
    "                                  metric='r2',\n",
    "                                  verbose=0))\n",
    "    # Fit once on entire data\n",
    "    flamly.fit(X, y)\n",
    "    besty_model = flamly[-1].best_model_for_estimator(flamly[-1].best_estimator)\n",
    "    besty = make_pipeline(transformer, clone(besty_model))\n",
    "\n",
    "    flamld.fit(X, D)\n",
    "    bestd_model = flamld[-1].best_model_for_estimator(flamld[-1].best_estimator)\n",
    "    bestd = make_pipeline(transformer, clone(bestd_model))\n",
    "\n",
    "    result = dml(X, D, y, besty, bestd, nfolds=5, classifier=True)\n",
    "    table_local.append(summary(*result, X, D, y, name=f'{name_prefix} automl (semi-cfit)'))\n",
    "\n",
    "    # 7) stacked (semi-cfit)\n",
    "    # Re-use the same base models we created, but put them into lists:\n",
    "    # - Notice we must re-create them fresh so they are unfitted before cross_val_predict\n",
    "    #   or cross_val_predict won't do what we expect.\n",
    "    lassoy_ = make_pipeline(transformer, StandardScaler(), LassoCV(cv=cv))\n",
    "    lgrd_ = make_pipeline(transformer, StandardScaler(), LogisticRegressionCV(cv=cv))\n",
    "    rfy_ = make_pipeline(transformer,\n",
    "                         RandomForestRegressor(n_estimators=100,\n",
    "                                               min_samples_leaf=10,\n",
    "                                               ccp_alpha=0.001))\n",
    "    rfd_ = make_pipeline(transformer,\n",
    "                         RandomForestClassifier(n_estimators=100,\n",
    "                                                min_samples_leaf=10,\n",
    "                                                ccp_alpha=0.001))\n",
    "    dtry_ = make_pipeline(transformer,\n",
    "                          DecisionTreeRegressor(min_samples_leaf=10,\n",
    "                                                ccp_alpha=0.001))\n",
    "    dtrd_ = make_pipeline(transformer,\n",
    "                          DecisionTreeClassifier(min_samples_leaf=10,\n",
    "                                                 ccp_alpha=0.001))\n",
    "    gbfy_ = make_pipeline(transformer,\n",
    "                          GradientBoostingRegressor(max_depth=2,\n",
    "                                                    n_iter_no_change=5))\n",
    "    gbfd_ = make_pipeline(transformer,\n",
    "                          GradientBoostingClassifier(max_depth=2,\n",
    "                                                     n_iter_no_change=5))\n",
    "\n",
    "    modely_list = [lassoy_, rfy_, dtry_, gbfy_]\n",
    "    modeld_list = [lgrd_, rfd_, dtrd_, gbfd_]\n",
    "\n",
    "    result = dml_dirty(X, D, y, modely_list, modeld_list,\n",
    "                       stacker=LinearRegression(),\n",
    "                       nfolds=5, classifier=True)\n",
    "    table_local.append(summary(*result, X, D, y, name=f'{name_prefix} stacked (semi-cfit)'))\n",
    "\n",
    "    # Concatenate all results\n",
    "    return pd.concat(table_local)\n",
    "\n",
    "\n",
    "table_bottom = run_all_estimators(X_bottom, D_bottom, y_bottom, name_prefix='bottom25%')\n",
    "table_top = run_all_estimators(X_top, D_top, y_top, name_prefix='top25%')\n",
    "\n",
    "print(\"=== Bottom 25% Income Sample ===\")\n",
    "print(table_bottom)\n",
    "\n",
    "print(\"\\n=== Top 25% Income Sample ===\")\n",
    "print(table_top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the PLR setting, there indeed seems to be heterogeneity in the treatment with respect to income, with the bottom 25% of earners seeing estimates around 3.8k and the top 25% seeing estimates of around 17.5k. The different machine learning models are broadly consistent across all three income groups.  \n",
    "  \n",
    "Next we more to the IRM part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>estimate</th>\n",
       "      <th>stderr</th>\n",
       "      <th>lower</th>\n",
       "      <th>upper</th>\n",
       "      <th>rmse y</th>\n",
       "      <th>rmse D</th>\n",
       "      <th>accuracy D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lasso/logistic</th>\n",
       "      <td>7726.585781</td>\n",
       "      <td>1159.322663</td>\n",
       "      <td>5454.313361</td>\n",
       "      <td>9998.858202</td>\n",
       "      <td>54060.702446</td>\n",
       "      <td>0.444041</td>\n",
       "      <td>0.687948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random forest</th>\n",
       "      <td>7777.909708</td>\n",
       "      <td>1146.768056</td>\n",
       "      <td>5530.244318</td>\n",
       "      <td>10025.575099</td>\n",
       "      <td>55592.303235</td>\n",
       "      <td>0.444747</td>\n",
       "      <td>0.689057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>decision tree</th>\n",
       "      <td>7846.333657</td>\n",
       "      <td>1255.457709</td>\n",
       "      <td>5385.636547</td>\n",
       "      <td>10307.030767</td>\n",
       "      <td>60491.245996</td>\n",
       "      <td>0.446437</td>\n",
       "      <td>0.688048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>boosted forest</th>\n",
       "      <td>8605.121529</td>\n",
       "      <td>1148.964341</td>\n",
       "      <td>6353.151421</td>\n",
       "      <td>10857.091638</td>\n",
       "      <td>55118.774981</td>\n",
       "      <td>0.443754</td>\n",
       "      <td>0.688855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>automl (semi-cfit)</th>\n",
       "      <td>8493.401447</td>\n",
       "      <td>1098.094834</td>\n",
       "      <td>6341.135573</td>\n",
       "      <td>10645.667321</td>\n",
       "      <td>53750.922780</td>\n",
       "      <td>0.448531</td>\n",
       "      <td>0.688250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stacked (semi-cfit)</th>\n",
       "      <td>7762.670926</td>\n",
       "      <td>1127.006260</td>\n",
       "      <td>5553.738656</td>\n",
       "      <td>9971.603197</td>\n",
       "      <td>53406.542825</td>\n",
       "      <td>0.442790</td>\n",
       "      <td>0.689662</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        estimate       stderr        lower         upper  \\\n",
       "lasso/logistic       7726.585781  1159.322663  5454.313361   9998.858202   \n",
       "random forest        7777.909708  1146.768056  5530.244318  10025.575099   \n",
       "decision tree        7846.333657  1255.457709  5385.636547  10307.030767   \n",
       "boosted forest       8605.121529  1148.964341  6353.151421  10857.091638   \n",
       "automl (semi-cfit)   8493.401447  1098.094834  6341.135573  10645.667321   \n",
       "stacked (semi-cfit)  7762.670926  1127.006260  5553.738656   9971.603197   \n",
       "\n",
       "                           rmse y    rmse D  accuracy D  \n",
       "lasso/logistic       54060.702446  0.444041    0.687948  \n",
       "random forest        55592.303235  0.444747    0.689057  \n",
       "decision tree        60491.245996  0.446437    0.688048  \n",
       "boosted forest       55118.774981  0.443754    0.688855  \n",
       "automl (semi-cfit)   53750.922780  0.448531    0.688250  \n",
       "stacked (semi-cfit)  53406.542825  0.442790    0.689662  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dr(X, D, y, modely0, modely1, modeld, *, trimming=0.01, nfolds):\n",
    "    '''\n",
    "    DML for the Interactive Regression Model setting (Doubly Robust Learning)\n",
    "    with cross-fitting\n",
    "\n",
    "    Input\n",
    "    -----\n",
    "    X: the controls\n",
    "    D: the treatment\n",
    "    y: the outcome\n",
    "    modely0: the ML model for predicting the outcome y in the control population\n",
    "    modely1: the ML model for predicting the outcome y in the treated population\n",
    "    modeld: the ML model for predicting the treatment D\n",
    "    trimming: threshold below which to trim propensities\n",
    "    nfolds: the number of folds in cross-fitting\n",
    "\n",
    "    Output\n",
    "    ------\n",
    "    point: the point estimate of the treatment effect of D on y\n",
    "    stderr: the standard error of the treatment effect\n",
    "    yhat: the cross-fitted predictions for the outcome y\n",
    "    Dhat: the cross-fitted predictions for the outcome D\n",
    "    resy: the outcome residuals\n",
    "    resD: the treatment residuals\n",
    "    drhat: the doubly robust quantity for each sample\n",
    "    '''\n",
    "    cv = KFold(n_splits=nfolds, shuffle=True, random_state=123)\n",
    "    yhat0, yhat1 = np.zeros(y.shape), np.zeros(y.shape)\n",
    "    # we will fit a model E[Y| D, X] by fitting a separate model for D==0\n",
    "    # and a separate model for D==1.\n",
    "    for train, test in cv.split(X, y):\n",
    "        # train a model on training data that received treatment zero and predict on all data in test set\n",
    "        yhat0[test] = clone(modely0).fit(X.iloc[train][D[train] == 0], y[train][D[train] == 0]).predict(X.iloc[test])\n",
    "        # train a model on training data that received treatment one and predict on all data in test set\n",
    "        yhat1[test] = clone(modely1).fit(X.iloc[train][D[train] == 1], y[train][D[train] == 1]).predict(X.iloc[test])\n",
    "    # prediction for observed treatment\n",
    "    yhat = yhat0 * (1 - D) + yhat1 * D\n",
    "    # propensity scores\n",
    "    Dhat = cross_val_predict(modeld, X, D, cv=cv, method='predict_proba', n_jobs=-1)[:, 1]\n",
    "    Dhat = np.clip(Dhat, trimming, 1 - trimming)\n",
    "    # doubly robust quantity for every sample\n",
    "    drhat = yhat1 - yhat0 + (y - yhat) * (D / Dhat - (1 - D) / (1 - Dhat))\n",
    "    point = np.mean(drhat)\n",
    "    var = np.var(drhat)\n",
    "    stderr = np.sqrt(var / X.shape[0])\n",
    "    return point, stderr, yhat, Dhat, y - yhat, D - Dhat, drhat\n",
    "\n",
    "v = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "lassoytest = make_pipeline(transformer, StandardScaler(), LassoCV(cv=cv))\n",
    "lgrdtest = make_pipeline(transformer, StandardScaler(), LogisticRegressionCV(cv=cv))\n",
    "result = dr(X, D, y, lassoytest, lassoytest, lgrdtest, nfolds=5)\n",
    "seed_estimates = summary(*result, X, D, y, name='lasso/logistic')\n",
    "\n",
    "for i in range(9):\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    lassoytest = make_pipeline(transformer, StandardScaler(), LassoCV(cv=cv))\n",
    "    lgrdtest = make_pipeline(transformer, StandardScaler(), LogisticRegressionCV(cv=cv))\n",
    "    result = dr(X, D, y, lassoytest, lassoytest, lgrdtest, nfolds=5)\n",
    "    seed_estimates = pd.concat([seed_estimates, summary(*result, X, D, y, name='lasso/logistic')])\n",
    "\n",
    "med_theta = np.median(seed_estimates.values[:, 0])\n",
    "se_med = np.sqrt(np.median((seed_estimates.values[:, 1])**2 + (seed_estimates.values[:, 0] - med_theta)**2))\n",
    "tabledr = pd.DataFrame({'estimate': med_theta,\n",
    "                        'stderr': se_med,\n",
    "                        'lower': med_theta - 1.96 * se_med,\n",
    "                        'upper': med_theta + 1.96 * se_med,\n",
    "                        'rmse y': np.median(seed_estimates.values[:, 4]),\n",
    "                        'rmse D': np.median(seed_estimates.values[:, 5]),\n",
    "                        'accuracy D': np.median(seed_estimates.values[:, 6]),\n",
    "                        }, index=['lasso/logistic'])\n",
    "\n",
    "rfy = make_pipeline(transformer, RandomForestRegressor(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001))\n",
    "rfd = make_pipeline(transformer, RandomForestClassifier(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001))\n",
    "result = dr(X, D, y, rfy, rfy, rfd, nfolds=5)\n",
    "tabledr = pd.concat([tabledr, summary(*result, X, D, y, name='random forest')])\n",
    "dtry = make_pipeline(transformer, DecisionTreeRegressor(min_samples_leaf=10, ccp_alpha=.001))\n",
    "dtrd = make_pipeline(transformer, DecisionTreeClassifier(min_samples_leaf=10, ccp_alpha=.001))\n",
    "result = dr(X, D, y, dtry, dtry, dtrd, nfolds=5)\n",
    "tabledr = pd.concat([tabledr, summary(*result, X, D, y, name='decision tree')])\n",
    "gbfy = make_pipeline(transformer, GradientBoostingRegressor(max_depth=2, n_iter_no_change=5))\n",
    "gbfd = make_pipeline(transformer, GradientBoostingClassifier(max_depth=2, n_iter_no_change=5))\n",
    "result = dr(X, D, y, gbfy, gbfy, gbfd, nfolds=5)\n",
    "tabledr = pd.concat([tabledr, summary(*result, X, D, y, name='boosted forest')])\n",
    "# semi cross-fitting\n",
    "flamly0 = make_pipeline(transformer, AutoML(time_budget=60, task='regression', early_stop=True,\n",
    "                                            eval_method='cv', n_splits=3, metric='r2', verbose=0))\n",
    "flamly1 = make_pipeline(transformer, AutoML(time_budget=60, task='regression', early_stop=True,\n",
    "                                            eval_method='cv', n_splits=3, metric='r2', verbose=0))\n",
    "flamld = make_pipeline(transformer, AutoML(time_budget=60, task='classification', early_stop=True,\n",
    "                                           eval_method='cv', n_splits=3, metric='r2', verbose=0))\n",
    "\n",
    "flamly0.fit(X[D == 0], y[D == 0])\n",
    "besty0 = make_pipeline(transformer, clone(flamly0[-1].best_model_for_estimator(flamly0[-1].best_estimator)))\n",
    "flamly1.fit(X[D == 1], y[D == 1])\n",
    "besty1 = make_pipeline(transformer, clone(flamly1[-1].best_model_for_estimator(flamly1[-1].best_estimator)))\n",
    "flamld.fit(X, D)\n",
    "bestd = make_pipeline(transformer, clone(flamld[-1].best_model_for_estimator(flamld[-1].best_estimator)))\n",
    "result = dr(X, D, y, besty0, besty1, bestd, nfolds=5)\n",
    "tabledr = pd.concat([tabledr, summary(*result, X, D, y, name='automl (semi-cfit)')])\n",
    "def dr_dirty(X, D, y, modely0_list, modely1_list, modeld_list, *,\n",
    "             stacker=LinearRegression(), trimming=0.01, nfolds):\n",
    "    '''\n",
    "    DML for the Interactive Regression Model setting (Doubly Robust Learning)\n",
    "    with cross-fitting\n",
    "\n",
    "    Input\n",
    "    -----\n",
    "    X: the controls\n",
    "    D: the treatment\n",
    "    y: the outcome\n",
    "    modely_list: list of ML models for predicting the outcome y\n",
    "    modeld_list: list of ML models for predicting the treatment D\n",
    "    stacker: model used to aggregate predictions of each of the base models\n",
    "    trimming: threshold below which to trim propensities\n",
    "    nfolds: the number of folds in cross-fitting\n",
    "\n",
    "    Output\n",
    "    ------\n",
    "    point: the point estimate of the treatment effect of D on y\n",
    "    stderr: the standard error of the treatment effect\n",
    "    yhat: the cross-fitted predictions for the outcome y\n",
    "    Dhat: the cross-fitted predictions for the outcome D\n",
    "    resy: the outcome residuals\n",
    "    resD: the treatment residuals\n",
    "    drhat: the doubly robust quantity for each sample\n",
    "    '''\n",
    "    cv = KFold(n_splits=nfolds, shuffle=True, random_state=123)\n",
    "\n",
    "    # we will fit a model E[Y| D, X] by fitting a separate model for D==0\n",
    "    # and a separate model for D==1. We do that for each model type in modely_list\n",
    "    yhats0, yhats1 = np.zeros((y.shape[0], len(modely0_list))), np.zeros((y.shape[0], len(modely1_list)))\n",
    "    for train, test in cv.split(X, y):\n",
    "        for it, modely0 in enumerate(modely0_list):\n",
    "            mdl = clone(modely0).fit(X.iloc[train][D[train] == 0], y[train][D[train] == 0])\n",
    "            yhats0[test, it] = mdl.predict(X.iloc[test])\n",
    "        for it, modely1 in enumerate(modely1_list):\n",
    "            mdl = clone(modely1).fit(X.iloc[train][D[train] == 1], y[train][D[train] == 1])\n",
    "            yhats1[test, it] = mdl.predict(X.iloc[test])\n",
    "\n",
    "    # calculate stacking weights for the outcome model for each population\n",
    "    # and combine the outcome model predictions\n",
    "    yhat0 = clone(stacker).fit(yhats0[D == 0], y[D == 0]).predict(yhats0)\n",
    "    yhat1 = clone(stacker).fit(yhats1[D == 1], y[D == 1]).predict(yhats1)\n",
    "\n",
    "    # prediction for observed treatment using the stacked model\n",
    "    yhat = yhat0 * (1 - D) + yhat1 * D\n",
    "\n",
    "    # propensity scores\n",
    "    Dhats = np.array([cross_val_predict(modeld, X, D, cv=cv, method='predict_proba', n_jobs=-1)[:, 1]\n",
    "                     for modeld in modeld_list]).T\n",
    "    # construct coefficients on each model based on stacker\n",
    "    Dhat = clone(stacker).fit(Dhats, D).predict(Dhats)\n",
    "    # trim propensities\n",
    "    Dhat = np.clip(Dhat, trimming, 1 - trimming)\n",
    "\n",
    "    # doubly robust quantity for every sample\n",
    "    drhat = yhat1 - yhat0 + (y - yhat) * (D / Dhat - (1 - D) / (1 - Dhat))\n",
    "    point = np.mean(drhat)\n",
    "    var = np.var(drhat)\n",
    "    stderr = np.sqrt(var / X.shape[0])\n",
    "    return point, stderr, yhat, Dhat, y - yhat, D - Dhat, drhat\n",
    "\n",
    "result = dr_dirty(X, D, y, [lassoy, rfy, dtry, gbfy], [lassoy, rfy, dtry, gbfy], [lgrd, rfd, dtrd, gbfd], nfolds=5)\n",
    "tabledr = pd.concat([tabledr, summary(*result, X, D, y, name='stacked (semi-cfit)')])\n",
    "tabledr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Doubly Robust Results: Bottom 25% Income ===\n",
      "                        estimate       stderr        lower         upper  \\\n",
      "lasso/logistic       4168.663113  1055.478209  2099.925824   6237.400403   \n",
      "random forest        3799.230032   991.820440  1855.261969   5743.198095   \n",
      "decision tree        6575.695867  3065.986166   566.362982  12585.028752   \n",
      "boosted forest       4743.873292  1050.278674  2685.327092   6802.419492   \n",
      "automl (semi-cfit)   4513.585430  1017.811332  2518.675220   6508.495641   \n",
      "stacked (semi-cfit)  4132.274210   993.706517  2184.609436   6079.938984   \n",
      "\n",
      "                           rmse y    rmse D  accuracy D  \n",
      "lasso/logistic       13391.064772  0.357524    0.841001  \n",
      "random forest        13456.578497  0.346027    0.845843  \n",
      "decision tree        14583.602725  0.370841    0.809927  \n",
      "boosted forest       13537.975130  0.344690    0.844633  \n",
      "automl (semi-cfit)   13170.419927  0.346929    0.839790  \n",
      "stacked (semi-cfit)  13227.455519  0.343537    0.844229  \n",
      "\n",
      "=== Doubly Robust Results: Top 25% Income ===\n",
      "                         estimate        stderr         lower         upper  \\\n",
      "lasso/logistic       17259.513464   3916.988487   9582.216029  24936.810898   \n",
      "random forest        16385.719656   4108.509640   8333.040763  24438.398550   \n",
      "decision tree        46206.960011  26502.839004  -5738.604437  98152.524459   \n",
      "boosted forest       16969.879793   3909.085243   9308.072716  24631.686871   \n",
      "automl (semi-cfit)   17037.483404   3747.395234   9692.588744  24382.378063   \n",
      "stacked (semi-cfit)  18650.016854   3803.165954  11195.811584  26104.222123   \n",
      "\n",
      "                            rmse y    rmse D  accuracy D  \n",
      "lasso/logistic        91534.931249  0.482861    0.603066  \n",
      "random forest         96587.383105  0.485947    0.603469  \n",
      "decision tree        103738.276387  0.544698    0.535700  \n",
      "boosted forest        95603.651415  0.484866    0.591771  \n",
      "automl (semi-cfit)    94421.196487  0.484810    0.599435  \n",
      "stacked (semi-cfit)   91019.403730  0.481500    0.608310  \n"
     ]
    }
   ],
   "source": [
    "data_bottom = data.query('inc < inc.quantile(.25)').copy()\n",
    "data_top    = data.query('inc > inc.quantile(.75)').copy()\n",
    "\n",
    "def extract_XDy(df):\n",
    "    \"\"\"Given a subset of the 401k data, produce X, D, and y \n",
    "    consistent with the main DR analysis.\"\"\"\n",
    "    y_ = df['net_tfa'].values\n",
    "    D_ = df['e401'].values\n",
    "    X_ = df.drop([\n",
    "        'e401', 'p401', 'a401', 'tw', 'tfa', 'net_tfa', 'tfa_he',\n",
    "        'hval', 'hmort', 'hequity', 'nifa', 'net_nifa', 'net_n401',\n",
    "        'ira', 'dum91', 'icat', 'ecat', 'zhat', 'i1', 'i2', 'i3',\n",
    "        'i4', 'i5', 'i6', 'i7', 'a1', 'a2', 'a3', 'a4', 'a5'\n",
    "    ], axis=1)\n",
    "    return X_, D_, y_\n",
    "\n",
    "X_bottom, D_bottom, y_bottom = extract_XDy(data_bottom)\n",
    "X_top, D_top, y_top          = extract_XDy(data_top)\n",
    "\n",
    "def dr(X, D, y, modely0, modely1, modeld, *, trimming=0.01, nfolds=5):\n",
    "    '''\n",
    "    DML for the Interactive Regression Model setting (Doubly Robust Learning)\n",
    "    with cross-fitting\n",
    "    '''\n",
    "    cv = KFold(n_splits=nfolds, shuffle=True, random_state=123)\n",
    "    yhat0, yhat1 = np.zeros(y.shape), np.zeros(y.shape)\n",
    "    for train, test in cv.split(X, y):\n",
    "        # Fit E[Y|D=0,X]\n",
    "        mdl0 = clone(modely0).fit(X.iloc[train][D[train] == 0], y[train][D[train] == 0])\n",
    "        yhat0[test] = mdl0.predict(X.iloc[test])\n",
    "        # Fit E[Y|D=1,X]\n",
    "        mdl1 = clone(modely1).fit(X.iloc[train][D[train] == 1], y[train][D[train] == 1])\n",
    "        yhat1[test] = mdl1.predict(X.iloc[test])\n",
    "\n",
    "    # Combine to get E[Y|D,X] predictions for the observed D\n",
    "    yhat = yhat0 * (1 - D) + yhat1 * D\n",
    "\n",
    "    # Propensity scores\n",
    "    Dhat = cross_val_predict(modeld, X, D, cv=cv, method='predict_proba', n_jobs=-1)[:, 1]\n",
    "    Dhat = np.clip(Dhat, trimming, 1 - trimming)\n",
    "\n",
    "    # Doubly robust quantity for each sample\n",
    "    drhat = yhat1 - yhat0 + (y - yhat) * (D / Dhat - (1 - D) / (1 - Dhat))\n",
    "    point = np.mean(drhat)\n",
    "    var   = np.var(drhat)\n",
    "    stderr= np.sqrt(var / X.shape[0])\n",
    "\n",
    "    # Return:\n",
    "    #   - point: the point estimate\n",
    "    #   - stderr: standard error\n",
    "    #   - yhat: cross-fitted predictions for the observed D\n",
    "    #   - Dhat: cross-fitted propensities\n",
    "    #   - (y - yhat): outcome residual\n",
    "    #   - (D - Dhat): treatment residual\n",
    "    #   - drhat: doubly robust terms\n",
    "    return point, stderr, yhat, Dhat, (y - yhat), (D - Dhat), drhat\n",
    "\n",
    "def dr_dirty(\n",
    "    X, D, y,\n",
    "    modely0_list, modely1_list, modeld_list,\n",
    "    stacker=LinearRegression(), trimming=0.01, nfolds=5\n",
    "):\n",
    "    '''\n",
    "    DML for the Interactive Regression Model setting (Doubly Robust Learning)\n",
    "    with cross-fitting and stacking\n",
    "    '''\n",
    "    cv = KFold(n_splits=nfolds, shuffle=True, random_state=123)\n",
    "\n",
    "    # Prepare space for multiple model predictions\n",
    "    yhats0 = np.zeros((X.shape[0], len(modely0_list)))\n",
    "    yhats1 = np.zeros((X.shape[0], len(modely1_list)))\n",
    "\n",
    "    # Cross-fitting: fit each base model E[Y|D=0,X], E[Y|D=1,X]\n",
    "    for train, test in cv.split(X, y):\n",
    "        for i_m0, m0 in enumerate(modely0_list):\n",
    "            m0_cl = clone(m0).fit(X.iloc[train][D[train] == 0], y[train][D[train] == 0])\n",
    "            yhats0[test, i_m0] = m0_cl.predict(X.iloc[test])\n",
    "        for i_m1, m1 in enumerate(modely1_list):\n",
    "            m1_cl = clone(m1).fit(X.iloc[train][D[train] == 1], y[train][D[train] == 1])\n",
    "            yhats1[test, i_m1] = m1_cl.predict(X.iloc[test])\n",
    "\n",
    "    # Stack them for D=0 predictions\n",
    "    yhat0 = clone(stacker).fit(yhats0[D == 0], y[D == 0]).predict(yhats0)\n",
    "    # Stack them for D=1 predictions\n",
    "    yhat1 = clone(stacker).fit(yhats1[D == 1], y[D == 1]).predict(yhats1)\n",
    "    # Combined model prediction for Y\n",
    "    yhat = yhat0 * (1 - D) + yhat1 * D\n",
    "\n",
    "    # Now do the same for propensity models\n",
    "    Dhats_list = []\n",
    "    for md in modeld_list:\n",
    "        Dhats_list.append(cross_val_predict(md, X, D, cv=cv, method='predict_proba', n_jobs=-1)[:, 1])\n",
    "    Dhats_arr = np.vstack(Dhats_list).T  # shape: (n_samples, n_models)\n",
    "\n",
    "    # Stack the predicted propensity\n",
    "    Dhat = clone(stacker).fit(Dhats_arr, D).predict(Dhats_arr)\n",
    "    Dhat = np.clip(Dhat, trimming, 1 - trimming)\n",
    "\n",
    "    # Doubly robust quantity\n",
    "    drhat = yhat1 - yhat0 + (y - yhat) * (D / Dhat - (1 - D) / (1 - Dhat))\n",
    "    point = np.mean(drhat)\n",
    "    var   = np.var(drhat)\n",
    "    stderr= np.sqrt(var / X.shape[0])\n",
    "\n",
    "    return point, stderr, yhat, Dhat, (y - yhat), (D - Dhat), drhat\n",
    "\n",
    "\n",
    "def summary(point, stderr, yhat, Dhat, resy, resD, drhat, X, D, y, *, name=''):\n",
    "    return pd.DataFrame({\n",
    "        'estimate': point,  # point estimate\n",
    "        'stderr': stderr,   # standard error\n",
    "        'lower': point - 1.96 * stderr,\n",
    "        'upper': point + 1.96 * stderr,\n",
    "        'rmse y': np.sqrt(np.mean(resy**2)),\n",
    "        'rmse D': np.sqrt(np.mean(resD**2)),\n",
    "        # classification accuracy for D, if it's binary:\n",
    "        'accuracy D': np.mean(np.abs(resD) < 0.5),\n",
    "    }, index=[name])\n",
    "\n",
    "\n",
    "def run_dr_analysis(X, D, y):\n",
    "    \"\"\"\n",
    "    Replicates the DR analysis shown in the original code:\n",
    "      - lasso/logistic with repeated seeds, then median-based estimate\n",
    "      - random forest\n",
    "      - decision tree\n",
    "      - boosted forest\n",
    "      - automl (semi-cfit)\n",
    "      - stacking\n",
    "    Returns a single table of results.\n",
    "    \"\"\"\n",
    "    # -- 1) Lasso/logistic repeated over seeds, then median-based estimate\n",
    "    seed_estimates = None\n",
    "\n",
    "    # We'll define a base 5-fold for the model pipelines:\n",
    "    cv_5fold = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "\n",
    "    # One example run with seed=123\n",
    "    lassoy = make_pipeline(transformer, StandardScaler(), LassoCV(cv=cv_5fold))\n",
    "    lgrd   = make_pipeline(transformer, StandardScaler(), LogisticRegressionCV(cv=cv_5fold))\n",
    "\n",
    "    # DR for single seed first\n",
    "    result = dr(X, D, y, lassoy, lassoy, lgrd, nfolds=5)\n",
    "    seed_estimates = summary(*result, X, D, y, name='lasso/logistic')\n",
    "\n",
    "    # Loop over multiple seeds\n",
    "    for i in range(9):\n",
    "        cv_seed = KFold(n_splits=5, shuffle=True, random_state=i)\n",
    "        lassoy_i = make_pipeline(transformer, StandardScaler(), LassoCV(cv=cv_seed))\n",
    "        lgrd_i   = make_pipeline(transformer, StandardScaler(), LogisticRegressionCV(cv=cv_seed))\n",
    "\n",
    "        result_i = dr(X, D, y, lassoy_i, lassoy_i, lgrd_i, nfolds=5)\n",
    "        seed_estimates = pd.concat([seed_estimates, summary(*result_i, X, D, y, name='lasso/logistic')])\n",
    "\n",
    "    # Compute median-based point estimate and standard error\n",
    "    med_theta  = np.median(seed_estimates['estimate'].values)\n",
    "    # sqrt(median(var_i) + var( point_i ))\n",
    "    # but the original code basically does:\n",
    "    se_med     = np.sqrt(\n",
    "        np.median(seed_estimates['stderr'].values ** 2)\n",
    "        + np.median((seed_estimates['estimate'].values - med_theta) ** 2)\n",
    "    )\n",
    "    tabledr = pd.DataFrame({\n",
    "        'estimate': med_theta,\n",
    "        'stderr':   se_med,\n",
    "        'lower':    med_theta - 1.96 * se_med,\n",
    "        'upper':    med_theta + 1.96 * se_med,\n",
    "        'rmse y':   np.median(seed_estimates['rmse y'].values),\n",
    "        'rmse D':   np.median(seed_estimates['rmse D'].values),\n",
    "        'accuracy D': np.median(seed_estimates['accuracy D'].values)\n",
    "    }, index=['lasso/logistic'])\n",
    "\n",
    "    # -- 2) Random Forest\n",
    "    rfy = make_pipeline(transformer, \n",
    "                        RandomForestRegressor(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001))\n",
    "    rfd = make_pipeline(transformer, \n",
    "                        RandomForestClassifier(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001))\n",
    "    result = dr(X, D, y, rfy, rfy, rfd, nfolds=5)\n",
    "    tabledr = pd.concat([tabledr, summary(*result, X, D, y, name='random forest')])\n",
    "\n",
    "    # -- 3) Decision Tree\n",
    "    dtry = make_pipeline(transformer,\n",
    "                         DecisionTreeRegressor(min_samples_leaf=10, ccp_alpha=.001))\n",
    "    dtrd = make_pipeline(transformer,\n",
    "                         DecisionTreeClassifier(min_samples_leaf=10, ccp_alpha=.001))\n",
    "    result = dr(X, D, y, dtry, dtry, dtrd, nfolds=5)\n",
    "    tabledr = pd.concat([tabledr, summary(*result, X, D, y, name='decision tree')])\n",
    "\n",
    "    # -- 4) Boosted Forest\n",
    "    gbfy = make_pipeline(transformer, \n",
    "                         GradientBoostingRegressor(max_depth=2, n_iter_no_change=5))\n",
    "    gbfd = make_pipeline(transformer, \n",
    "                         GradientBoostingClassifier(max_depth=2, n_iter_no_change=5))\n",
    "    result = dr(X, D, y, gbfy, gbfy, gbfd, nfolds=5)\n",
    "    tabledr = pd.concat([tabledr, summary(*result, X, D, y, name='boosted forest')])\n",
    "\n",
    "    # -- 5) AutoML (semi cross-fitting)\n",
    "    flamly0 = make_pipeline(transformer,\n",
    "                            AutoML(time_budget=60, task='regression', early_stop=True,\n",
    "                                   eval_method='cv', n_splits=3, metric='r2', verbose=0))\n",
    "    flamly1 = make_pipeline(transformer,\n",
    "                            AutoML(time_budget=60, task='regression', early_stop=True,\n",
    "                                   eval_method='cv', n_splits=3, metric='r2', verbose=0))\n",
    "    flamld  = make_pipeline(transformer,\n",
    "                            AutoML(time_budget=60, task='classification', early_stop=True,\n",
    "                                   eval_method='cv', n_splits=3, metric='r2', verbose=0))\n",
    "\n",
    "    # Fit Y|D=0, Y|D=1 on subsets\n",
    "    flamly0.fit(X[D == 0], y[D == 0])\n",
    "    besty0_model = flamly0[-1].best_model_for_estimator(flamly0[-1].best_estimator)\n",
    "    besty0 = make_pipeline(transformer, clone(besty0_model))\n",
    "\n",
    "    flamly1.fit(X[D == 1], y[D == 1])\n",
    "    besty1_model = flamly1[-1].best_model_for_estimator(flamly1[-1].best_estimator)\n",
    "    besty1 = make_pipeline(transformer, clone(besty1_model))\n",
    "\n",
    "    # Fit propensities on the full sample\n",
    "    flamld.fit(X, D)\n",
    "    bestd_model = flamld[-1].best_model_for_estimator(flamld[-1].best_estimator)\n",
    "    bestd = make_pipeline(transformer, clone(bestd_model))\n",
    "\n",
    "    result = dr(X, D, y, besty0, besty1, bestd, nfolds=5)\n",
    "    tabledr = pd.concat([tabledr, summary(*result, X, D, y, name='automl (semi-cfit)')])\n",
    "\n",
    "    # -- 6) Stacked (semi-cfit)\n",
    "    # Prepare lists of base models for Y|D=0, Y|D=1\n",
    "    lassoy_ = make_pipeline(transformer, StandardScaler(), LassoCV(cv=cv_5fold))\n",
    "    rfy_    = make_pipeline(transformer, \n",
    "                            RandomForestRegressor(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001))\n",
    "    dtry_   = make_pipeline(transformer, \n",
    "                            DecisionTreeRegressor(min_samples_leaf=10, ccp_alpha=.001))\n",
    "    gbfy_   = make_pipeline(transformer, \n",
    "                            GradientBoostingRegressor(max_depth=2, n_iter_no_change=5))\n",
    "\n",
    "    modely0_list = [lassoy_, rfy_, dtry_, gbfy_]  # for D=0\n",
    "    modely1_list = [lassoy_, rfy_, dtry_, gbfy_]  # for D=1\n",
    "\n",
    "    lgrd_   = make_pipeline(transformer, StandardScaler(), LogisticRegressionCV(cv=cv_5fold))\n",
    "    rfd_    = make_pipeline(transformer, \n",
    "                            RandomForestClassifier(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001))\n",
    "    dtrd_   = make_pipeline(transformer, \n",
    "                            DecisionTreeClassifier(min_samples_leaf=10, ccp_alpha=.001))\n",
    "    gbfd_   = make_pipeline(transformer, \n",
    "                            GradientBoostingClassifier(max_depth=2, n_iter_no_change=5))\n",
    "    modeld_list = [lgrd_, rfd_, dtrd_, gbfd_]\n",
    "\n",
    "    result = dr_dirty(X, D, y, modely0_list, modely1_list, modeld_list, stacker=LinearRegression(), nfolds=5)\n",
    "    tabledr = pd.concat([tabledr, summary(*result, X, D, y, name='stacked (semi-cfit)')])\n",
    "\n",
    "    return tabledr\n",
    "\n",
    "table_dr_bottom = run_dr_analysis(X_bottom, D_bottom, y_bottom)\n",
    "table_dr_top    = run_dr_analysis(X_top,    D_top,    y_top   )\n",
    "\n",
    "print(\"=== Doubly Robust Results: Bottom 25% Income ===\")\n",
    "print(table_dr_bottom)\n",
    "\n",
    "print(\"\\n=== Doubly Robust Results: Top 25% Income ===\")\n",
    "print(table_dr_top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the IRM setting, we again see heterogeneity in the treatment effect with respect to income, with the bottom 25% of earners seeing estimates around 4.5k and the top 25% seeing estimates of around 17k. The different machine learning models are broadly consistent across all three income groups with the exception of the decision tree, which has far higher estimates and standard errors than other methods.  \n",
    "  \n",
    "The IRM and PLR results differ slightly in terms of the estimate, but both find heterogeneity and are broadly in the same range of values.\n",
    "\n",
    "Now we move to implemetting semi-crossfitting with best model selection, first for PLR and then for IRM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>estimate</th>\n",
       "      <th>stderr</th>\n",
       "      <th>lower</th>\n",
       "      <th>upper</th>\n",
       "      <th>rmse y</th>\n",
       "      <th>rmse D</th>\n",
       "      <th>accuracy D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>select-best (semi-cfit) PLR</th>\n",
       "      <td>8834.698468</td>\n",
       "      <td>1321.460893</td>\n",
       "      <td>6244.635118</td>\n",
       "      <td>11424.761817</td>\n",
       "      <td>54254.468883</td>\n",
       "      <td>0.443139</td>\n",
       "      <td>0.691578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>select-best (semi-cfit) PLR (bottom 25%)</th>\n",
       "      <td>3999.925670</td>\n",
       "      <td>1124.172083</td>\n",
       "      <td>1796.548387</td>\n",
       "      <td>6203.302954</td>\n",
       "      <td>13398.184779</td>\n",
       "      <td>0.343868</td>\n",
       "      <td>0.844633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>select-best (semi-cfit) PLR (top 25%)</th>\n",
       "      <td>18204.194590</td>\n",
       "      <td>3867.405524</td>\n",
       "      <td>10624.079763</td>\n",
       "      <td>25784.309418</td>\n",
       "      <td>91393.039963</td>\n",
       "      <td>0.482708</td>\n",
       "      <td>0.601049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              estimate       stderr  \\\n",
       "select-best (semi-cfit) PLR                8834.698468  1321.460893   \n",
       "select-best (semi-cfit) PLR (bottom 25%)   3999.925670  1124.172083   \n",
       "select-best (semi-cfit) PLR (top 25%)     18204.194590  3867.405524   \n",
       "\n",
       "                                                 lower         upper  \\\n",
       "select-best (semi-cfit) PLR                6244.635118  11424.761817   \n",
       "select-best (semi-cfit) PLR (bottom 25%)   1796.548387   6203.302954   \n",
       "select-best (semi-cfit) PLR (top 25%)     10624.079763  25784.309418   \n",
       "\n",
       "                                                rmse y    rmse D  accuracy D  \n",
       "select-best (semi-cfit) PLR               54254.468883  0.443139    0.691578  \n",
       "select-best (semi-cfit) PLR (bottom 25%)  13398.184779  0.343868    0.844633  \n",
       "select-best (semi-cfit) PLR (top 25%)     91393.039963  0.482708    0.601049  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# b.)\n",
    "from sklearn.model_selection import cross_val_predict, KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from copy import deepcopy\n",
    "\n",
    "def get_oof_and_mse(model, X, y, cv, classifier=False):\n",
    "    \"\"\"\n",
    "    Returns out-of-fold predictions and the MSE (or 'regression MSE' if classifier=True).\n",
    "    If classifier=True, we use model.predict_proba(...).\n",
    "    \"\"\"\n",
    "    if classifier:\n",
    "        # For binary D, treat the problem as a regression on [0,1], \n",
    "        # so we measure MSE on predicted probabilities\n",
    "        preds = cross_val_predict(model, X, y, cv=cv, method='predict_proba')[:, 1]\n",
    "    else:\n",
    "        preds = cross_val_predict(model, X, y, cv=cv)\n",
    "    mse_val = mean_squared_error(y, preds)\n",
    "    return preds, mse_val\n",
    "\n",
    "\n",
    "def dml_select_best(X, D, y, modely_list, modeld_list, *, nfolds=5, classifier=False, trimming=0.01):\n",
    "    \"\"\"\n",
    "    Semi-cross-fitting for the Partially Linear Model (PLR) by selecting\n",
    "    the single best model for Y and single best model for D among user-provided lists.\n",
    "\n",
    "    Steps:\n",
    "      1) For each candidate in modely_list, get OOF predictions vs. y. Pick the best by MSE.\n",
    "      2) For each candidate in modeld_list, get OOF predictions vs. D. Pick the best by MSE.\n",
    "         If classifier=True, each model in modeld_list is treated as a classifier,\n",
    "         but we still measure MSE on the predicted probability vs. the true D.\n",
    "      3) Use the chosen best model's OOF predictions for yhat and Dhat.\n",
    "      4) Compute partial linear estimate as in the usual DML formula:\n",
    "            theta = E[(y - yhat)*(D - Dhat)] / E[(D - Dhat)^2]\n",
    "         and the standard error formula.\n",
    "\n",
    "    Returns:\n",
    "      point, stderr, yhat, Dhat, resy, resD, epsilon\n",
    "    \"\"\"\n",
    "    cv = KFold(n_splits=nfolds, shuffle=True, random_state=123)\n",
    "\n",
    "    # --- 1) Select best model for y\n",
    "    best_mse_y = np.inf\n",
    "    best_preds_y = None\n",
    "    best_model_y = None\n",
    "\n",
    "    for candidate in modely_list:\n",
    "        # Clone so we don't pollute the original pipeline with partial fits\n",
    "        cand = deepcopy(candidate)\n",
    "        preds, mse_val = get_oof_and_mse(cand, X, y, cv, classifier=False)\n",
    "        if mse_val < best_mse_y:\n",
    "            best_mse_y = mse_val\n",
    "            best_preds_y = preds\n",
    "            best_model_y = cand\n",
    "\n",
    "    # --- 2) Select best model for D\n",
    "    best_mse_d = np.inf\n",
    "    best_preds_d = None\n",
    "    best_model_d = None\n",
    "\n",
    "    for candidate in modeld_list:\n",
    "        # If classifier=True, we measure MSE on predicted probabilities\n",
    "        cand = deepcopy(candidate)\n",
    "        preds, mse_val = get_oof_and_mse(cand, X, D, cv, classifier=classifier)\n",
    "        if mse_val < best_mse_d:\n",
    "            best_mse_d = mse_val\n",
    "            best_preds_d = preds\n",
    "            best_model_d = cand\n",
    "\n",
    "    # Residuals\n",
    "    resy = y - best_preds_y\n",
    "    resD = D - best_preds_d\n",
    "\n",
    "    # Final partial linear estimate\n",
    "    point = np.mean(resy * resD) / np.mean(resD**2)\n",
    "    epsilon = resy - point * resD\n",
    "\n",
    "    var = np.mean(epsilon**2 * resD**2) / (np.mean(resD**2)**2)\n",
    "    stderr = np.sqrt(var / X.shape[0])\n",
    "\n",
    "    return point, stderr, best_preds_y, best_preds_d, resy, resD, epsilon\n",
    "\n",
    "modely_list = [lassoy, rfy, dtry, gbfy]\n",
    "modeld_list = [lgrd, rfd, dtrd, gbfd]\n",
    "\n",
    "point, stderr, yhat, Dhat, resy, resD, epsilon = dml_select_best(\n",
    "    X, D, y, modely_list, modeld_list, nfolds=5, classifier=True\n",
    ")\n",
    "\n",
    "table_select = summary(\n",
    "    point, stderr, yhat, Dhat, resy, resD, epsilon, X, D, y,\n",
    "    name='select-best (semi-cfit) PLR'\n",
    ")\n",
    "point_bottom, stderr_bottom, yhat_bottom, Dhat_bottom, resy_bottom, resD_bottom, epsilon_bottom = dml_select_best(\n",
    "    X_bottom, D_bottom, y_bottom, modely_list, modeld_list, nfolds=5, classifier=True\n",
    ")\n",
    "table_select_bottom = summary(\n",
    "    point_bottom, stderr_bottom, yhat_bottom, Dhat_bottom, resy_bottom, resD_bottom, epsilon_bottom, X_bottom, D_bottom, y_bottom,\n",
    "    name='select-best (semi-cfit) PLR (bottom 25%)'\n",
    ")\n",
    "table_select = pd.concat([table_select, table_select_bottom])\n",
    "point_top, stderr_top, yhat_top, Dhat_top, resy_top, resD_top, epsilon_top = dml_select_best(\n",
    "    X_top, D_top, y_top, modely_list, modeld_list, nfolds=5, classifier=True\n",
    ")\n",
    "table_select_top = summary(\n",
    "    point_top, stderr_top, yhat_top, Dhat_top, resy_top, resD_top, epsilon_top, X_top, D_top, y_top,\n",
    "    name='select-best (semi-cfit) PLR (top 25%)'\n",
    ")\n",
    "table_select = pd.concat([table_select, table_select_top])\n",
    "table_select"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For IRM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>estimate</th>\n",
       "      <th>stderr</th>\n",
       "      <th>lower</th>\n",
       "      <th>upper</th>\n",
       "      <th>rmse y</th>\n",
       "      <th>rmse D</th>\n",
       "      <th>accuracy D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>select best IRM with semi cross fitting all samples</th>\n",
       "      <td>7718.844897</td>\n",
       "      <td>1119.692846</td>\n",
       "      <td>5524.246919</td>\n",
       "      <td>9913.442874</td>\n",
       "      <td>54026.928780</td>\n",
       "      <td>0.443322</td>\n",
       "      <td>0.689864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>select best IRM with semi cross fitting bottom 25\\% income</th>\n",
       "      <td>4243.618356</td>\n",
       "      <td>1107.527383</td>\n",
       "      <td>2072.864686</td>\n",
       "      <td>6414.372026</td>\n",
       "      <td>13378.218781</td>\n",
       "      <td>0.344798</td>\n",
       "      <td>0.845036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>select best IRM with semi cross fitting top 25\\% income</th>\n",
       "      <td>18393.721922</td>\n",
       "      <td>3820.594388</td>\n",
       "      <td>10905.356922</td>\n",
       "      <td>25882.086922</td>\n",
       "      <td>91533.229313</td>\n",
       "      <td>0.482708</td>\n",
       "      <td>0.601049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        estimate       stderr  \\\n",
       "select best IRM with semi cross fitting all sam...   7718.844897  1119.692846   \n",
       "select best IRM with semi cross fitting bottom ...   4243.618356  1107.527383   \n",
       "select best IRM with semi cross fitting top 25\\...  18393.721922  3820.594388   \n",
       "\n",
       "                                                           lower  \\\n",
       "select best IRM with semi cross fitting all sam...   5524.246919   \n",
       "select best IRM with semi cross fitting bottom ...   2072.864686   \n",
       "select best IRM with semi cross fitting top 25\\...  10905.356922   \n",
       "\n",
       "                                                           upper  \\\n",
       "select best IRM with semi cross fitting all sam...   9913.442874   \n",
       "select best IRM with semi cross fitting bottom ...   6414.372026   \n",
       "select best IRM with semi cross fitting top 25\\...  25882.086922   \n",
       "\n",
       "                                                          rmse y    rmse D  \\\n",
       "select best IRM with semi cross fitting all sam...  54026.928780  0.443322   \n",
       "select best IRM with semi cross fitting bottom ...  13378.218781  0.344798   \n",
       "select best IRM with semi cross fitting top 25\\...  91533.229313  0.482708   \n",
       "\n",
       "                                                    accuracy D  \n",
       "select best IRM with semi cross fitting all sam...    0.689864  \n",
       "select best IRM with semi cross fitting bottom ...    0.845036  \n",
       "select best IRM with semi cross fitting top 25\\...    0.601049  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_oof_and_mse_irm(model, X, y, D, which_d, cv):\n",
    "    \"\"\"\n",
    "    For IRM, we measure OOF performance only on the subset where D == which_d.\n",
    "    We do cross-fitting: train on all 'train' points that have D==which_d, \n",
    "    predict on the entire test fold, but compute MSE only for the test fold \n",
    "    members that also have D==which_d.\n",
    "    \n",
    "    Returns OOF predictions (full length, but only truly valid for D==which_d),\n",
    "    plus MSE measured on that subset.\n",
    "    \"\"\"\n",
    "    preds_full = np.zeros(len(y), dtype=float)\n",
    "    for train_idx, test_idx in cv.split(X, y):\n",
    "        # filter the training portion to only those with D==which_d\n",
    "        train_sub = train_idx[D[train_idx] == which_d]\n",
    "        # Fit on (X, y) for that sub-sample\n",
    "        model_cl = deepcopy(model).fit(X.iloc[train_sub], y[train_sub])\n",
    "        # Predict on the entire test fold\n",
    "        preds_full[test_idx] = model_cl.predict(X.iloc[test_idx])\n",
    "    # MSE only on the subset that has D==which_d\n",
    "    mse_val = mean_squared_error(y[D==which_d], preds_full[D==which_d])\n",
    "    return preds_full, mse_val\n",
    "\n",
    "def dr_select_best(X, D, y, modely0_list, modely1_list, modeld_list,\n",
    "                   trimming=0.01, nfolds=5):\n",
    "    \"\"\"\n",
    "    Doubly-Robust (IRM) with semi-cross-fitting:\n",
    "    Select single best model for Y|D=0 from modely0_list,\n",
    "    single best model for Y|D=1 from modely1_list,\n",
    "    single best model for the propensity from modeld_list (all data).\n",
    "    \n",
    "    Then run the standard cross-fitting formula to construct:\n",
    "      yhat0, yhat1, Dhat, drhat,\n",
    "    and output the usual IRM results.\n",
    "    \"\"\"\n",
    "    cv = KFold(n_splits=nfolds, shuffle=True, random_state=123)\n",
    "\n",
    "    # --- 1) find best model for Y|D=0\n",
    "    best_mse_y0 = np.inf\n",
    "    best_preds_y0 = None\n",
    "    best_model_y0 = None\n",
    "    for candidate in modely0_list:\n",
    "        preds0, mse0 = get_oof_and_mse_irm(candidate, X, y, D, which_d=0, cv=cv)\n",
    "        if mse0 < best_mse_y0:\n",
    "            best_mse_y0 = mse0\n",
    "            best_preds_y0 = preds0\n",
    "            best_model_y0 = candidate\n",
    "\n",
    "    # --- 2) find best model for Y|D=1\n",
    "    best_mse_y1 = np.inf\n",
    "    best_preds_y1 = None\n",
    "    best_model_y1 = None\n",
    "    for candidate in modely1_list:\n",
    "        preds1, mse1 = get_oof_and_mse_irm(candidate, X, y, D, which_d=1, cv=cv)\n",
    "        if mse1 < best_mse_y1:\n",
    "            best_mse_y1 = mse1\n",
    "            best_preds_y1 = preds1\n",
    "            best_model_y1 = candidate\n",
    "\n",
    "    # --- 3) find best model for D (propensity), measure MSE on entire sample\n",
    "    #         but we treat D as binary, using predicted probability\n",
    "    cv2 = KFold(n_splits=nfolds, shuffle=True, random_state=123)\n",
    "    best_mse_d = np.inf\n",
    "    best_preds_d = None\n",
    "    best_model_d = None\n",
    "    for candidate in modeld_list:\n",
    "        preds_d, mse_d = get_oof_and_mse(candidate, X, D, cv2, classifier=True)\n",
    "        if mse_d < best_mse_d:\n",
    "            best_mse_d = mse_d\n",
    "            best_preds_d = preds_d\n",
    "            best_model_d = candidate\n",
    "\n",
    "    # --- 4) IRM formula\n",
    "    # We already have cross-fitted yhat0, yhat1, Dhat = best_preds_d\n",
    "    yhat = best_preds_y0*(1 - D) + best_preds_y1*D\n",
    "    Dhat = np.clip(best_preds_d, trimming, 1 - trimming)\n",
    "\n",
    "    # DR score\n",
    "    drhat = best_preds_y1 - best_preds_y0 + (y - yhat) * (\n",
    "        D / Dhat - (1 - D)/(1 - Dhat)\n",
    "    )\n",
    "    point = np.mean(drhat)\n",
    "    var = np.var(drhat)\n",
    "    stderr = np.sqrt(var / X.shape[0])\n",
    "\n",
    "    return (\n",
    "        point,\n",
    "        stderr,\n",
    "        yhat,                # cross-fitted E[Y|D=observed, X]\n",
    "        Dhat,                # cross-fitted e(X)\n",
    "        (y - yhat),          # residual in Y space\n",
    "        (D - Dhat),          # residual in D space\n",
    "        drhat\n",
    "    )\n",
    "    \n",
    "modely0_list = [lassoy, rfy, dtry, gbfy]\n",
    "modely1_list = [lassoy, rfy, dtry, gbfy]\n",
    "modeld_list  = [lgrd,  rfd,  dtrd, gbfd]\n",
    "\n",
    "res_all = dr_select_best(\n",
    "    X, D, y, modely0_list, modely1_list, modeld_list, nfolds=5\n",
    ")\n",
    "res_bottom_25 = dr_select_best(\n",
    "    X_bottom, D_bottom, y_bottom, modely0_list, modely1_list, modeld_list, nfolds=5\n",
    ")\n",
    "res_top_25 = dr_select_best(\n",
    "    X_top, D_top, y_top, modely1_list, modely0_list, modeld_list, nfolds=5\n",
    ")\n",
    "table_all = summary(*res_all, X, D, y, name=\"select best IRM with semi cross fitting all samples\")\n",
    "table_bottom_25 = summary(*res_bottom_25, X_bottom, D_bottom, y_bottom, name=f\"select best IRM with semi cross fitting bottom 25\\% income\")\n",
    "table_top_25 = summary(*res_top_25, X_top, D_top, y_top, name=f\"select best IRM with semi cross fitting top 25\\% income\")\n",
    "\n",
    "table_final = pd.concat([table_all, table_bottom_25, table_top_25])\n",
    "table_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- Double Lasso in EconML for PLR------------\n",
      "Coefficient Results:  X is None, please call intercept_inference to learn the constant!\n",
      "                        CATE Intercept Results                        \n",
      "======================================================================\n",
      "               point_estimate  stderr  zstat pvalue ci_lower  ci_upper\n",
      "----------------------------------------------------------------------\n",
      "cate_intercept       8608.589 1426.197 6.036    0.0 5813.295 11403.884\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "<sub>A linear parametric conditional average treatment effect (CATE) model was fitted:\n",
      "$Y = \\Theta(X)\\cdot T + g(X, W) + \\epsilon$\n",
      "where for every outcome $i$ and treatment $j$ the CATE $\\Theta_{ij}(X)$ has the form:\n",
      "$\\Theta_{ij}(X) = X' coef_{ij} + cate\\_intercept_{ij}$\n",
      "Coefficient Results table portrays the $coef_{ij}$ parameter vector for each outcome $i$ and treatment $j$. Intercept Results table portrays the $cate\\_intercept_{ij}$ parameter.</sub>\n",
      "-----------------Random Forest in EconML for PLR------------\n",
      "Coefficient Results:  X is None, please call intercept_inference to learn the constant!\n",
      "                        CATE Intercept Results                        \n",
      "======================================================================\n",
      "               point_estimate  stderr  zstat pvalue ci_lower  ci_upper\n",
      "----------------------------------------------------------------------\n",
      "cate_intercept        8603.31 1333.541 6.451    0.0 5989.619 11217.002\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "<sub>A linear parametric conditional average treatment effect (CATE) model was fitted:\n",
      "$Y = \\Theta(X)\\cdot T + g(X, W) + \\epsilon$\n",
      "where for every outcome $i$ and treatment $j$ the CATE $\\Theta_{ij}(X)$ has the form:\n",
      "$\\Theta_{ij}(X) = X' coef_{ij} + cate\\_intercept_{ij}$\n",
      "Coefficient Results table portrays the $coef_{ij}$ parameter vector for each outcome $i$ and treatment $j$. Intercept Results table portrays the $cate\\_intercept_{ij}$ parameter.</sub>\n",
      "-----------------Gradient Boosting in EconML for PLR------------\n",
      "Coefficient Results:  X is None, please call intercept_inference to learn the constant!\n",
      "                        CATE Intercept Results                       \n",
      "=====================================================================\n",
      "               point_estimate  stderr zstat pvalue ci_lower  ci_upper\n",
      "---------------------------------------------------------------------\n",
      "cate_intercept       9129.167 1379.02  6.62    0.0 6426.337 11831.997\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "<sub>A linear parametric conditional average treatment effect (CATE) model was fitted:\n",
      "$Y = \\Theta(X)\\cdot T + g(X, W) + \\epsilon$\n",
      "where for every outcome $i$ and treatment $j$ the CATE $\\Theta_{ij}(X)$ has the form:\n",
      "$\\Theta_{ij}(X) = X' coef_{ij} + cate\\_intercept_{ij}$\n",
      "Coefficient Results table portrays the $coef_{ij}$ parameter vector for each outcome $i$ and treatment $j$. Intercept Results table portrays the $cate\\_intercept_{ij}$ parameter.</sub>\n",
      "-----------------Decision Tree in EconML for PLR------------\n",
      "Coefficient Results:  X is None, please call intercept_inference to learn the constant!\n",
      "                        CATE Intercept Results                        \n",
      "======================================================================\n",
      "               point_estimate  stderr  zstat pvalue ci_lower  ci_upper\n",
      "----------------------------------------------------------------------\n",
      "cate_intercept       8772.347 1448.641 6.056    0.0 5933.064 11611.631\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "<sub>A linear parametric conditional average treatment effect (CATE) model was fitted:\n",
      "$Y = \\Theta(X)\\cdot T + g(X, W) + \\epsilon$\n",
      "where for every outcome $i$ and treatment $j$ the CATE $\\Theta_{ij}(X)$ has the form:\n",
      "$\\Theta_{ij}(X) = X' coef_{ij} + cate\\_intercept_{ij}$\n",
      "Coefficient Results table portrays the $coef_{ij}$ parameter vector for each outcome $i$ and treatment $j$. Intercept Results table portrays the $cate\\_intercept_{ij}$ parameter.</sub>\n",
      "-----------------Double Lasso in DoubleML for PLR------------\n",
      "-----------------Random Forest in DoubleML for PLR------------\n",
      "          coef      std err        t         P>|t|       2.5 %        97.5 %\n",
      "d  8833.162108  1345.946257  6.56279  5.281034e-11  6195.15592  11471.168296\n",
      "-----------------Decision Tree in DoubleML for PLR------------\n",
      "          coef      std err         t         P>|t|        2.5 %        97.5 %\n",
      "d  9412.772225  1421.249172  6.622887  3.522510e-11  6627.175035  12198.369415\n",
      "-----------------Gradient Boosting in DoubleML for PLR------------\n",
      "          coef      std err         t         P>|t|        2.5 %        97.5 %\n",
      "d  8974.079387  1361.310988  6.592233  4.332595e-11  6305.958879  11642.199896\n",
      "-----------------Random Forest in EconML for IRM------------\n",
      "Coefficient Results:  X is None, please call intercept_inference to learn the constant!\n",
      "                        CATE Intercept Results                       \n",
      "=====================================================================\n",
      "               point_estimate  stderr  zstat pvalue ci_lower ci_upper\n",
      "---------------------------------------------------------------------\n",
      "cate_intercept       7713.469 1136.741 6.786    0.0 5485.497 9941.441\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "<sub>A linear parametric conditional average treatment effect (CATE) model was fitted:\n",
      "$Y = \\Theta(X)\\cdot T + g(X, W) + \\epsilon$\n",
      "where $T$ is the one-hot-encoding of the discrete treatment and for every outcome $i$ and treatment $j$ the CATE $\\Theta_{ij}(X)$ has the form:\n",
      "$\\Theta_{ij}(X) = \\phi(X)' coef_{ij} + cate\\_intercept_{ij}$\n",
      "where $\\phi(X)$ is the output of the `featurizer` or $X$ if `featurizer`=None. Coefficient Results table portrays the $coef_{ij}$ parameter vector for each outcome $i$ and the designated treatment $j$ passed to summary. Intercept Results table portrays the $cate\\_intercept_{ij}$ parameter.</sub>\n",
      "-----------------Random Forest in EconML for IRM (separate models)------------\n",
      "Coefficient Results:  X is None, please call intercept_inference to learn the constant!\n",
      "                        CATE Intercept Results                       \n",
      "=====================================================================\n",
      "               point_estimate  stderr  zstat pvalue ci_lower ci_upper\n",
      "---------------------------------------------------------------------\n",
      "cate_intercept       7564.389 1139.643 6.638    0.0 5330.729 9798.048\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "<sub>A linear parametric conditional average treatment effect (CATE) model was fitted:\n",
      "$Y = \\Theta(X)\\cdot T + g(X, W) + \\epsilon$\n",
      "where $T$ is the one-hot-encoding of the discrete treatment and for every outcome $i$ and treatment $j$ the CATE $\\Theta_{ij}(X)$ has the form:\n",
      "$\\Theta_{ij}(X) = \\phi(X)' coef_{ij} + cate\\_intercept_{ij}$\n",
      "where $\\phi(X)$ is the output of the `featurizer` or $X$ if `featurizer`=None. Coefficient Results table portrays the $coef_{ij}$ parameter vector for each outcome $i$ and the designated treatment $j$ passed to summary. Intercept Results table portrays the $cate\\_intercept_{ij}$ parameter.</sub>\n",
      "-----------------Decision Tree in EconML for IRM------------\n",
      "Coefficient Results:  X is None, please call intercept_inference to learn the constant!\n",
      "                        CATE Intercept Results                        \n",
      "======================================================================\n",
      "               point_estimate  stderr  zstat pvalue ci_lower  ci_upper\n",
      "----------------------------------------------------------------------\n",
      "cate_intercept       8521.921 1264.001 6.742    0.0 6044.524 10999.318\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "<sub>A linear parametric conditional average treatment effect (CATE) model was fitted:\n",
      "$Y = \\Theta(X)\\cdot T + g(X, W) + \\epsilon$\n",
      "where $T$ is the one-hot-encoding of the discrete treatment and for every outcome $i$ and treatment $j$ the CATE $\\Theta_{ij}(X)$ has the form:\n",
      "$\\Theta_{ij}(X) = \\phi(X)' coef_{ij} + cate\\_intercept_{ij}$\n",
      "where $\\phi(X)$ is the output of the `featurizer` or $X$ if `featurizer`=None. Coefficient Results table portrays the $coef_{ij}$ parameter vector for each outcome $i$ and the designated treatment $j$ passed to summary. Intercept Results table portrays the $cate\\_intercept_{ij}$ parameter.</sub>\n",
      "-----------------Decision Tree in EconML for IRM (separate models)------------\n",
      "Coefficient Results:  X is None, please call intercept_inference to learn the constant!\n",
      "                        CATE Intercept Results                       \n",
      "=====================================================================\n",
      "               point_estimate  stderr zstat pvalue ci_lower  ci_upper\n",
      "---------------------------------------------------------------------\n",
      "cate_intercept       8380.877 1193.28 7.023    0.0 6042.092 10719.662\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "<sub>A linear parametric conditional average treatment effect (CATE) model was fitted:\n",
      "$Y = \\Theta(X)\\cdot T + g(X, W) + \\epsilon$\n",
      "where $T$ is the one-hot-encoding of the discrete treatment and for every outcome $i$ and treatment $j$ the CATE $\\Theta_{ij}(X)$ has the form:\n",
      "$\\Theta_{ij}(X) = \\phi(X)' coef_{ij} + cate\\_intercept_{ij}$\n",
      "where $\\phi(X)$ is the output of the `featurizer` or $X$ if `featurizer`=None. Coefficient Results table portrays the $coef_{ij}$ parameter vector for each outcome $i$ and the designated treatment $j$ passed to summary. Intercept Results table portrays the $cate\\_intercept_{ij}$ parameter.</sub>\n",
      "-----------------Gradient Boosting in EconML for IRM------------\n",
      "Coefficient Results:  X is None, please call intercept_inference to learn the constant!\n",
      "                        CATE Intercept Results                        \n",
      "======================================================================\n",
      "               point_estimate  stderr  zstat pvalue ci_lower  ci_upper\n",
      "----------------------------------------------------------------------\n",
      "cate_intercept       8158.627 1139.372 7.161    0.0 5925.499 10391.756\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "<sub>A linear parametric conditional average treatment effect (CATE) model was fitted:\n",
      "$Y = \\Theta(X)\\cdot T + g(X, W) + \\epsilon$\n",
      "where $T$ is the one-hot-encoding of the discrete treatment and for every outcome $i$ and treatment $j$ the CATE $\\Theta_{ij}(X)$ has the form:\n",
      "$\\Theta_{ij}(X) = \\phi(X)' coef_{ij} + cate\\_intercept_{ij}$\n",
      "where $\\phi(X)$ is the output of the `featurizer` or $X$ if `featurizer`=None. Coefficient Results table portrays the $coef_{ij}$ parameter vector for each outcome $i$ and the designated treatment $j$ passed to summary. Intercept Results table portrays the $cate\\_intercept_{ij}$ parameter.</sub>\n",
      "-----------------Gradient Boosting in EconML for IRM (separate models)------------\n",
      "Coefficient Results:  X is None, please call intercept_inference to learn the constant!\n",
      "                        CATE Intercept Results                        \n",
      "======================================================================\n",
      "               point_estimate  stderr  zstat pvalue ci_lower  ci_upper\n",
      "----------------------------------------------------------------------\n",
      "cate_intercept       8400.842 1145.037 7.337    0.0 6156.611 10645.073\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "<sub>A linear parametric conditional average treatment effect (CATE) model was fitted:\n",
      "$Y = \\Theta(X)\\cdot T + g(X, W) + \\epsilon$\n",
      "where $T$ is the one-hot-encoding of the discrete treatment and for every outcome $i$ and treatment $j$ the CATE $\\Theta_{ij}(X)$ has the form:\n",
      "$\\Theta_{ij}(X) = \\phi(X)' coef_{ij} + cate\\_intercept_{ij}$\n",
      "where $\\phi(X)$ is the output of the `featurizer` or $X$ if `featurizer`=None. Coefficient Results table portrays the $coef_{ij}$ parameter vector for each outcome $i$ and the designated treatment $j$ passed to summary. Intercept Results table portrays the $cate\\_intercept_{ij}$ parameter.</sub>\n",
      "-----------------Random Forest in DoubleML for IRM------------\n",
      "          coef      std err         t         P>|t|        2.5 %      97.5 %\n",
      "d  7592.885026  1139.897519  6.661024  2.719258e-11  5358.726942  9827.04311\n",
      "-----------------Decision Tree in DoubleML for IRM------------\n",
      "          coef      std err         t         P>|t|        2.5 %        97.5 %\n",
      "d  8141.012135  1212.869077  6.712194  1.917200e-11  5763.832427  10518.191843\n",
      "-----------------Gradient Boosting in DoubleML for IRM------------\n",
      "         coef      std err         t         P>|t|        2.5 %        97.5 %\n",
      "d  8302.57187  1134.872208  7.315865  2.557287e-13  6078.263216  10526.880524\n"
     ]
    }
   ],
   "source": [
    " #c.) \n",
    "W = StandardScaler().fit_transform(transformer.fit_transform(X))\n",
    "# PLR in econml\n",
    "# ! pip install econml\n",
    "from econml.dml import LinearDML\n",
    "\n",
    "\n",
    "# double lasso in econml\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "ldml_lasso = LinearDML(\n",
    "    model_y=LassoCV(cv=cv),\n",
    "    model_t=LassoCV(cv=cv),\n",
    ").fit(y, D, W=W)\n",
    "print(\"----------------- Double Lasso in EconML for PLR------------\")\n",
    "print(ldml_lasso.summary())\n",
    "\n",
    "# random forest in econml\n",
    "ldml_rf = LinearDML(\n",
    "    model_y=RandomForestRegressor(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "    model_t=RandomForestClassifier(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "    cv=5,\n",
    "    discrete_treatment=True,\n",
    "    random_state=123\n",
    ").fit(y, D, W=W)\n",
    "print(\"-----------------Random Forest in EconML for PLR------------\")\n",
    "print(ldml_rf.summary())\n",
    "\n",
    "# gradient boosting in econml\n",
    "ldml_gb = LinearDML(\n",
    "    model_y=GradientBoostingRegressor(max_depth=2, n_iter_no_change=5, random_state=123),\n",
    "    model_t=GradientBoostingClassifier(max_depth=2, n_iter_no_change=5, random_state=123),\n",
    "    cv=5,\n",
    "    discrete_treatment=True,\n",
    "    random_state=123\n",
    ").fit(y, D, W=W)\n",
    "print(\"-----------------Gradient Boosting in EconML for PLR------------\")\n",
    "print(ldml_gb.summary())\n",
    "\n",
    "# PLR with Decision Tree\n",
    "ldml_dt = LinearDML(\n",
    "    model_y=DecisionTreeRegressor(min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "    model_t=DecisionTreeClassifier(min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "    cv=5,\n",
    "    discrete_treatment=True,\n",
    "    random_state=123\n",
    ").fit(y, D, W=W)\n",
    "print(\"-----------------Decision Tree in EconML for PLR------------\")\n",
    "print(ldml_dt.summary())\n",
    "\n",
    "# plr in double ml\n",
    "# ! pip install doubleml\n",
    "from doubleml import DoubleMLData\n",
    "import doubleml as dbml\n",
    "\n",
    "\n",
    "dml_data = DoubleMLData.from_arrays(W, y, D)\n",
    "\n",
    "try:\n",
    "    # double lasso\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "    dml_plr_lasso = dbml.DoubleMLPLR(\n",
    "        dml_data,\n",
    "        LassoCV(cv=cv),\n",
    "        LassoCV(cv=cv),\n",
    "        n_folds=5,\n",
    "    )\n",
    "    dml_plr_lasso.fit()\n",
    "    print(\"-----------------Double Lasso in DoubleML for PLR------------\")\n",
    "    # random forest\n",
    "    dml_plr_rf = dbml.DoubleMLPLR(\n",
    "        dml_data,\n",
    "        RandomForestRegressor(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "        RandomForestClassifier(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "        n_folds=5,\n",
    "    )\n",
    "    dml_plr_rf.fit()\n",
    "    print(\"-----------------Random Forest in DoubleML for PLR------------\")\n",
    "    print(dml_plr_rf.summary)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(\"DoubleML failed to run the PLR models\")\n",
    "\n",
    "# decision tree\n",
    "dml_plr_dt = dbml.DoubleMLPLR(\n",
    "    dml_data,\n",
    "    DecisionTreeRegressor(min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "    DecisionTreeClassifier(min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "    n_folds=5,\n",
    ")\n",
    "\n",
    "dml_plr_dt.fit()\n",
    "print(\"-----------------Decision Tree in DoubleML for PLR------------\")\n",
    "print(dml_plr_dt.summary)\n",
    "\n",
    "# gradient boosting\n",
    "\n",
    "dml_plr_gb = dbml.DoubleMLPLR(\n",
    "    dml_data,\n",
    "    GradientBoostingRegressor(max_depth=2, n_iter_no_change=5, random_state=123),\n",
    "    GradientBoostingClassifier(max_depth=2, n_iter_no_change=5, random_state=123),\n",
    "    n_folds=5,\n",
    ")\n",
    "\n",
    "dml_plr_gb.fit() \n",
    "print(\"-----------------Gradient Boosting in DoubleML for PLR------------\")\n",
    "print(dml_plr_gb.summary)\n",
    "\n",
    "\n",
    "# irm with econml\n",
    "\n",
    "from econml.dr import LinearDRLearner   \n",
    "from econml.utilities import SeparateModel\n",
    "\n",
    "# random forest\n",
    "dr_forest = LinearDRLearner(\n",
    "    model_regression=RandomForestRegressor(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "    model_propensity=RandomForestClassifier(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "    cv=5,\n",
    ")\n",
    "dr_forest.fit(y, D, W=W)\n",
    "print(\"-----------------Random Forest in EconML for IRM------------\")\n",
    "print(dr_forest.summary(T=1))\n",
    "\n",
    "# random forest using seperate models for model_regression\n",
    "dr_forest_sep = LinearDRLearner(\n",
    "    model_regression=SeparateModel(\n",
    "        RandomForestRegressor(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "        RandomForestRegressor(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "    ),\n",
    "    model_propensity=RandomForestClassifier(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "    cv=5,\n",
    ")\n",
    "dr_forest_sep.fit(y, D, W=W)\n",
    "print(\"-----------------Random Forest in EconML for IRM (separate models)------------\")\n",
    "print(dr_forest_sep.summary(T=1))\n",
    "\n",
    "# decision tree\n",
    "dr_tree = LinearDRLearner(\n",
    "    model_regression=DecisionTreeRegressor(min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "    model_propensity=DecisionTreeClassifier(min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "    cv=5,\n",
    ")\n",
    "dr_tree.fit(y, D, W=W)\n",
    "print(\"-----------------Decision Tree in EconML for IRM------------\")\n",
    "print(dr_tree.summary(T=1))\n",
    "\n",
    "# decision tree using seperate models for model_regression\n",
    "dr_tree_sep = LinearDRLearner(\n",
    "    model_regression=SeparateModel(\n",
    "        DecisionTreeRegressor(min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "        DecisionTreeRegressor(min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "    ),\n",
    "    model_propensity=DecisionTreeClassifier(min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "    cv=5,\n",
    ")\n",
    "dr_tree_sep.fit(y, D, W=W)\n",
    "print(\"-----------------Decision Tree in EconML for IRM (separate models)------------\")\n",
    "print(dr_tree_sep.summary(T=1))\n",
    "\n",
    "# gradient boosting\n",
    "dr_gb = LinearDRLearner(\n",
    "    model_regression=GradientBoostingRegressor(max_depth=2, n_iter_no_change=5, random_state=123),\n",
    "    model_propensity=GradientBoostingClassifier(max_depth=2, n_iter_no_change=5, random_state=123),\n",
    "    cv=5,\n",
    ")\n",
    "dr_gb.fit(y, D, W=W)\n",
    "print(\"-----------------Gradient Boosting in EconML for IRM------------\")\n",
    "print(dr_gb.summary(T=1))\n",
    "\n",
    "# gradient boosting using seperate models for model_regression\n",
    "dr_gb_sep = LinearDRLearner(\n",
    "    model_regression=SeparateModel(\n",
    "        GradientBoostingRegressor(max_depth=2, n_iter_no_change=5, random_state=123),\n",
    "        GradientBoostingRegressor(max_depth=2, n_iter_no_change=5, random_state=123),\n",
    "    ),\n",
    "    model_propensity=GradientBoostingClassifier(max_depth=2, n_iter_no_change=5, random_state=123),\n",
    "    cv=5,\n",
    ")\n",
    "dr_gb_sep.fit(y, D, W=W)\n",
    "print(\"-----------------Gradient Boosting in EconML for IRM (separate models)------------\")\n",
    "print(dr_gb_sep.summary(T=1))\n",
    "# irm with double ml\n",
    "\n",
    "# random forest\n",
    "dml_irm_rf = dbml.DoubleMLIRM(\n",
    "    dml_data,\n",
    "    RandomForestRegressor(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "    RandomForestClassifier(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "    n_folds=5,\n",
    ")\n",
    "dml_irm_rf.fit()\n",
    "print(\"-----------------Random Forest in DoubleML for IRM------------\")\n",
    "print(dml_irm_rf.summary)\n",
    "\n",
    "# decision tree\n",
    "\n",
    "dml_irm_dt = dbml.DoubleMLIRM(\n",
    "    dml_data,\n",
    "    DecisionTreeRegressor(min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "    DecisionTreeClassifier(min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "    n_folds=5,\n",
    ")\n",
    "dml_irm_dt.fit()\n",
    "print(\"-----------------Decision Tree in DoubleML for IRM------------\")\n",
    "print(dml_irm_dt.summary)\n",
    "# gradient boosting\n",
    "\n",
    "dml_irm_gb = dbml.DoubleMLIRM(\n",
    "    dml_data,\n",
    "    GradientBoostingRegressor(max_depth=2, n_iter_no_change=5, random_state=123),\n",
    "    GradientBoostingClassifier(max_depth=2, n_iter_no_change=5, \n",
    "                               \n",
    "                               random_state=123),\n",
    "    n_folds=5,\n",
    ")\n",
    "dml_irm_gb.fit()\n",
    "print(\"-----------------Gradient Boosting in DoubleML for IRM------------\")\n",
    "print(dml_irm_gb.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Econml can work with most of the base learners (random forest, decision tree, boosted forest), as can doubleML. Both can work with any scikit-learn model in fact, so long as they implement fit() and predict() functions, according to the documentation. Theoretically, one could thus also build a custom class that implements the scikit api for stacking or semi crossfitting with choosing the best model, but neither library can work directly with stacking or perform the semi cross-fitting with the custom implementations we built as far as I could tell from the documentation of the packages. In theory one could write custom scikit-learn compatible interfaces for the custom implementations, but that would be a lot of work.  \n",
    "  \n",
    "In practice however, I found that double lasso did not work with the doubleML library. Upon closer examination, I found a section of the econml docs that explains that it specifically handles cases of models that do hyperparameter searches internally across folds. This is not the case for the doubleML library, which needs all folds to be treated strictly independently of each other under the hood and thus fails when LassoCV is used. This is a fundamental difference in the way the two libraries are built, and it means that doubleML cannot be used with double lasso as long as cross validation is used. One could however use a theoretically chosen penalty with lasso with scikit interface and then use that with doubleML.\n",
    "   \n",
    "As for the results (note that results may change slightly upon rerunning/exporting due to randomness. Please see output for full table, this is just for your convenience while grading. Estimate first, then standard error in parentheses, see above for other metrics): \n",
    "  \n",
    "For PLR:  \n",
    "\n",
    "double lasso previous: 9035\t(1295)\n",
    "double lasso econml: 8609 (1426)\n",
    "double lasso doubleML:  n/a\n",
    "  \n",
    "random forest previous:  8905 (1357)  \n",
    "random forest econml:  8603 (1333)   \n",
    "random forest doubleML: 8523 (1346)  \n",
    "  \n",
    "decision tree previous: 9236 (1440)  \n",
    "decision tree econml: 8772 (1449)  \n",
    "decision tree doubleML: 8734 (1455)  \n",
    "  \n",
    "boosted forest previous: 8840 (1334)  \n",
    "boosted forest econml: 9129 (1379)  \n",
    "boosted forest doubleML: 8834 (1366)  \n",
    "\n",
    "For IRM:  \n",
    "random forest previous: 7699 (1159)  \n",
    "random forest econml: 8023 (1121)  \n",
    "random forest doubleML: 7805 (1155)  \n",
    "\n",
    "decision tree previous: 7836 (1255)  \n",
    "decision tree econml: 8352 (1250)  \n",
    "decision tree doubleML: 7714 (1238) \n",
    "\n",
    "boosted forest previous: 8593 (1157)  \n",
    "boosted forest econml: 8118 (1135)  \n",
    "boosted forest doubleML: 8683 (1258)  \n",
    "\n",
    "The results are broadly consistent across the different methods and libraries, including for the decision tree. Apart from the decision tree in IRM, the results are also consistent with the estimates from the custom methods. For EconML and Double ML, the decision tree in the IRM setting is more in line with the other models than for the custom implementation, where it differs significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Semi-Synthetic Data with n=1000 ===\n",
      "True ATE in the semi-synthetic world:  7448.65797745209\n",
      "** PLR Results n = 1000 **\n",
      "True ATE in the semi-synthetic world:  7448.65797745209\n",
      "                            estimate       stderr        lower         upper  \\\n",
      "double lasso             4473.600355  3541.878591 -2468.481682  11415.682393   \n",
      "lasso/logistic           5057.826381  3454.633691 -1713.255652  11828.908415   \n",
      "random forest            6414.514167  3524.069645  -492.662337  13321.690672   \n",
      "decision tree            2139.271255  3316.734745 -4361.528845   8640.071355   \n",
      "boosted forest           3788.528551  3949.094321 -3951.696319  11528.753421   \n",
      "automl (semi-cfit)       5084.189070  3397.308487 -1574.535564  11742.913704   \n",
      "stacked (semi-cfit)      5085.361916  3473.896692 -1723.475600  11894.199432   \n",
      "select-best (semi-cfit)  4810.734877  3625.171173 -2294.600623  11916.070377   \n",
      "\n",
      "                               rmse y    rmse D  accuracy D        error  \\\n",
      "double lasso             55909.753842  0.471968       0.630  2975.057622   \n",
      "lasso/logistic           55909.753842  0.471921       0.622  2390.831596   \n",
      "random forest            56839.162836  0.475375       0.599  1034.143810   \n",
      "decision tree            58734.670442  0.528052       0.573  5309.386723   \n",
      "boosted forest           59146.491600  0.468753       0.613  3660.129426   \n",
      "automl (semi-cfit)       55368.942690  0.470150       0.620  2364.468907   \n",
      "stacked (semi-cfit)      55561.566008  0.468421       0.622  2363.296062   \n",
      "select-best (semi-cfit)  55909.753842  0.468823       0.620  2637.923101   \n",
      "\n",
      "                         rmse E[y|D,X]  rmse E[D|X]  covered  \n",
      "double lasso              15949.787495     0.082278        1  \n",
      "lasso/logistic            15949.787495     0.088301        1  \n",
      "random forest             18633.701348     0.088242        1  \n",
      "decision tree             24410.839670     0.270391        1  \n",
      "boosted forest            25133.177226     0.084430        1  \n",
      "automl (semi-cfit)        14550.081893     0.093434        1  \n",
      "stacked (semi-cfit)       14657.917946     0.079570        1  \n",
      "select-best (semi-cfit)   15949.787495     0.085958        1  \n",
      "** IRM Results n = 1000**\n",
      "                             estimate        stderr         lower  \\\n",
      "lasso/logistic            4267.826547   3482.062659  -2557.016266   \n",
      "random forest             7091.566844   4010.876659   -769.751407   \n",
      "decision tree           -15265.151737  17683.133754 -49924.093895   \n",
      "boosted forest            4628.684877   3860.449340  -2937.795830   \n",
      "automl (semi-cfit)        4902.492308   3502.494694  -1962.397292   \n",
      "stacked (semi-cfit)       4430.349320   3718.231617  -2857.384649   \n",
      "select-best (semi-cfit)   4524.568205   3580.853612  -2493.904874   \n",
      "\n",
      "                                upper        rmse y    rmse D  accuracy D  \\\n",
      "lasso/logistic           11092.669359  55886.066323  0.473062       0.626   \n",
      "random forest            14952.885095  57705.941213  0.474810       0.593   \n",
      "decision tree            19393.790421  61125.364616  0.525316       0.577   \n",
      "boosted forest           12195.165583  57631.979261  0.468528       0.616   \n",
      "automl (semi-cfit)       11767.381908  56447.099637  0.470150       0.620   \n",
      "stacked (semi-cfit)      11718.083288  55632.741007  0.467619       0.625   \n",
      "select-best (semi-cfit)  11543.041284  55847.774582  0.468984       0.627   \n",
      "\n",
      "                                error  rmse E[y|D,X]  rmse E[D|X]  covered  \n",
      "lasso/logistic            3180.831431   14976.276764     0.088047        1  \n",
      "random forest              357.091133   20592.196707     0.089207        1  \n",
      "decision tree            22713.809715   28969.546808     0.269341        1  \n",
      "boosted forest            2819.973101   20866.200869     0.082295        1  \n",
      "automl (semi-cfit)        2546.165669   17754.671196     0.093434        1  \n",
      "stacked (semi-cfit)       3018.308658   14247.009341     0.080707        1  \n",
      "select-best (semi-cfit)   2924.089772   14908.193207     0.082939        1  \n",
      "\n",
      "=== Semi-Synthetic Data with n=10000 ===\n",
      "True ATE in the semi-synthetic world:  7448.65797745209\n",
      "** PLR Results n = 10000 **\n",
      "True ATE in the semi-synthetic world:  7448.65797745209\n",
      "                            estimate       stderr        lower        upper  \\\n",
      "double lasso             5591.811198  1135.847404  3365.550287  7818.072109   \n",
      "lasso/logistic           5717.664057  1142.629811  3478.109629  7957.218486   \n",
      "random forest            5462.603671  1155.187186  3198.436786  7726.770556   \n",
      "decision tree            5927.860341  1232.087052  3512.969719  8342.750963   \n",
      "boosted forest           5535.447767  1163.535978  3254.917250  7815.978285   \n",
      "automl (semi-cfit)       5655.144066  1153.228134  3394.816924  7915.471209   \n",
      "stacked (semi-cfit)      5559.127340  1132.786809  3338.865194  7779.389485   \n",
      "select-best (semi-cfit)  5454.825375  1153.264592  3194.426776  7715.223975   \n",
      "\n",
      "                               rmse y    rmse D  accuracy D        error  \\\n",
      "double lasso             56463.233700  0.455572      0.6640  1856.846779   \n",
      "lasso/logistic           56463.233700  0.455998      0.6606  1730.993920   \n",
      "random forest            56055.636561  0.456158      0.6612  1986.054307   \n",
      "decision tree            59562.248417  0.456525      0.6649  1520.797636   \n",
      "boosted forest           56573.024212  0.455372      0.6608  1913.210210   \n",
      "automl (semi-cfit)       55569.068682  0.455615      0.6586  1793.513911   \n",
      "stacked (semi-cfit)      55724.218159  0.454879      0.6649  1889.530638   \n",
      "select-best (semi-cfit)  56021.296095  0.455482      0.6591  1993.832602   \n",
      "\n",
      "                         rmse E[y|D,X]  rmse E[D|X]  covered  \n",
      "double lasso              14023.054967     0.039495        1  \n",
      "lasso/logistic            14023.054967     0.043640        1  \n",
      "random forest             12852.647262     0.051854        1  \n",
      "decision tree             23280.237684     0.051042        1  \n",
      "boosted forest            14816.217457     0.040456        1  \n",
      "automl (semi-cfit)        11024.703573     0.048685        1  \n",
      "stacked (semi-cfit)       11182.511730     0.032285        1  \n",
      "select-best (semi-cfit)   12756.862093     0.042391        1  \n",
      "** IRM Results n = 10000**\n",
      "                            estimate       stderr        lower        upper  \\\n",
      "lasso/logistic           5666.571090  1225.025634  3265.520847  8067.621334   \n",
      "random forest            4760.032848  1125.066249  2554.903001  6965.162695   \n",
      "decision tree            4891.562451  1197.374615  2544.708207  7238.416696   \n",
      "boosted forest           5223.322746  1120.552172  3027.040490  7419.605002   \n",
      "automl (semi-cfit)       5162.087476  1117.317251  2972.145664  7352.029288   \n",
      "stacked (semi-cfit)      4957.967202  1136.824989  2729.790224  7186.144180   \n",
      "select-best (semi-cfit)  4733.058852  1126.398189  2525.318401  6940.799304   \n",
      "\n",
      "                               rmse y    rmse D  accuracy D        error  \\\n",
      "lasso/logistic           56409.465270  0.455551      0.6628  1782.086887   \n",
      "random forest            55581.662163  0.456039      0.6593  2688.625129   \n",
      "decision tree            58658.663610  0.456525      0.6649  2557.095526   \n",
      "boosted forest           56746.847835  0.455266      0.6600  2225.335231   \n",
      "automl (semi-cfit)       55522.121091  0.455615      0.6586  2286.570502   \n",
      "stacked (semi-cfit)      55393.228679  0.454846      0.6653  2490.690775   \n",
      "select-best (semi-cfit)  55571.896914  0.455849      0.6585  2715.599125   \n",
      "\n",
      "                         rmse E[y|D,X]  rmse E[D|X]  covered  \n",
      "lasso/logistic            13478.856856     0.040065        1  \n",
      "random forest             10129.043730     0.051081        0  \n",
      "decision tree             21774.892994     0.051042        0  \n",
      "boosted forest            14666.040811     0.040427        0  \n",
      "automl (semi-cfit)         9272.125247     0.048685        0  \n",
      "stacked (semi-cfit)        8833.740356     0.032088        0  \n",
      "select-best (semi-cfit)   10134.826077     0.042700        0  \n",
      "\n",
      "=== Semi-Synthetic Data with n=50000 ===\n",
      "True ATE in the semi-synthetic world:  7448.65797745209\n",
      "** PLR Results n = 50000 **\n",
      "True ATE in the semi-synthetic world:  7448.65797745209\n",
      "                            estimate      stderr        lower         upper  \\\n",
      "double lasso             9090.487472  587.320817  7939.338671  10241.636274   \n",
      "lasso/logistic           9120.032842  588.349563  7966.867699  10273.197986   \n",
      "random forest            8942.920812  583.369742  7799.516118  10086.325506   \n",
      "decision tree            8605.693824  596.877605  7435.813719   9775.573930   \n",
      "boosted forest           9015.824002  580.890973  7877.277695  10154.370308   \n",
      "automl (semi-cfit)       8953.917150  577.189340  7822.626043  10085.208257   \n",
      "stacked (semi-cfit)      8997.087074  577.568809  7865.052208  10129.121940   \n",
      "select-best (semi-cfit)  9002.059977  578.954674  7867.308815  10136.811138   \n",
      "\n",
      "                               rmse y    rmse D  accuracy D        error  \\\n",
      "double lasso             55173.277513  0.453581     0.67252  1641.829495   \n",
      "lasso/logistic           55173.277513  0.453525     0.67326  1671.374865   \n",
      "random forest            55170.505859  0.454489     0.67258  1494.262834   \n",
      "decision tree            57324.911461  0.454562     0.67204  1157.035847   \n",
      "boosted forest           54445.865152  0.452750     0.67530  1567.166024   \n",
      "automl (semi-cfit)       54246.531110  0.452865     0.67476  1505.259173   \n",
      "stacked (semi-cfit)      54340.404360  0.452720     0.67402  1548.429097   \n",
      "select-best (semi-cfit)  54426.791967  0.452851     0.67460  1553.401999   \n",
      "\n",
      "                         rmse E[y|D,X]  rmse E[D|X]  covered  \n",
      "double lasso              13522.387959     0.034659        0  \n",
      "lasso/logistic            13522.387959     0.034886        0  \n",
      "random forest             13925.351653     0.043921        0  \n",
      "decision tree             20782.858276     0.045949        1  \n",
      "boosted forest            10433.141831     0.022476        0  \n",
      "automl (semi-cfit)         9657.890497     0.022968        0  \n",
      "stacked (semi-cfit)       10026.821794     0.021964        0  \n",
      "select-best (semi-cfit)   10357.756793     0.023018        0  \n",
      "** IRM Results n = 50000**\n",
      "                            estimate      stderr        lower        upper  \\\n",
      "lasso/logistic           8583.125193  600.985353  7405.193901  9761.056485   \n",
      "random forest            8336.287754  561.779437  7235.200056  9437.375451   \n",
      "decision tree            8182.658924  627.004678  6953.729756  9411.588093   \n",
      "boosted forest           8388.313395  579.168656  7253.142830  9523.483960   \n",
      "automl (semi-cfit)       8354.155181  569.024404  7238.867349  9469.443013   \n",
      "stacked (semi-cfit)      8262.375120  589.710989  7106.541581  9418.208659   \n",
      "select-best (semi-cfit)  8338.481881  580.852446  7200.011086  9476.952676   \n",
      "\n",
      "                               rmse y    rmse D  accuracy D        error  \\\n",
      "lasso/logistic           54919.308540  0.453535     0.67304  1134.467215   \n",
      "random forest            54385.604112  0.454477     0.67238   887.629776   \n",
      "decision tree            57441.654225  0.454562     0.67204   734.000947   \n",
      "boosted forest           54107.500834  0.452840     0.67482   939.655418   \n",
      "automl (semi-cfit)       53777.461632  0.452865     0.67476   905.497204   \n",
      "stacked (semi-cfit)      53829.950210  0.452725     0.67478   813.717143   \n",
      "select-best (semi-cfit)  54120.944618  0.452850     0.67420   889.823904   \n",
      "\n",
      "                         rmse E[y|D,X]  rmse E[D|X]  covered  \n",
      "lasso/logistic            12661.066653     0.034964        1  \n",
      "random forest             10407.371951     0.043111        1  \n",
      "decision tree             20859.821285     0.045949        1  \n",
      "boosted forest             8664.569095     0.023052        1  \n",
      "automl (semi-cfit)         6630.654183     0.022968        1  \n",
      "stacked (semi-cfit)        7122.040203     0.022085        1  \n",
      "select-best (semi-cfit)    8724.212484     0.023064        1  \n"
     ]
    }
   ],
   "source": [
    "# d.) \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_predict, KFold\n",
    "from sklearn.base import clone\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class semisynth:\n",
    "    \n",
    "    def fit(self, X, D, y, transformer, random_state=None):\n",
    "        \"\"\"\n",
    "        X, D, y: the real data\n",
    "        transformer: any sklearn-compatible Transformer for pre-processing\n",
    "        \"\"\"\n",
    "        self.X_ = X.copy()\n",
    "\n",
    "        # Model for Y|D=0\n",
    "        self.est0_ = make_pipeline(transformer,\n",
    "                                   RandomForestRegressor(min_samples_leaf=20,\n",
    "                                                         ccp_alpha=0.001,\n",
    "                                                         random_state=random_state)\n",
    "                                  ).fit(X[D==0], y[D==0])\n",
    "        self.res0_ = y[D==0] - self.est0_.predict(X[D==0])\n",
    "        # De-mean the residual distribution\n",
    "        self.res0_ -= np.mean(self.res0_)\n",
    "\n",
    "        # Model for Y|D=1\n",
    "        self.est1_ = make_pipeline(transformer,\n",
    "                                   RandomForestRegressor(min_samples_leaf=20,\n",
    "                                                         ccp_alpha=0.001,\n",
    "                                                         random_state=random_state)\n",
    "                                  ).fit(X[D==1], y[D==1])\n",
    "        self.res1_ = y[D==1] - self.est1_.predict(X[D==1])\n",
    "        self.res1_ -= np.mean(self.res1_)\n",
    "\n",
    "        # Model for D|X\n",
    "        self.prop_ = make_pipeline(transformer,\n",
    "                                   RandomForestClassifier(min_samples_leaf=20,\n",
    "                                                          ccp_alpha=0.001,\n",
    "                                                          random_state=random_state)\n",
    "                                  ).fit(X, D)\n",
    "        return self\n",
    "\n",
    "    def generate_data(self, n):\n",
    "        \"\"\"\n",
    "        Returns (X, D, Y, Y1, Y0):\n",
    "          X, D, Y: the new sample\n",
    "          Y1, Y0: potential outcomes for each row\n",
    "        \"\"\"\n",
    "        # Resample X from the empirical distribution\n",
    "        X = self.X_.iloc[np.random.choice(self.X_.shape[0], n, replace=True)]\n",
    "        \n",
    "        # Simulate D ~ Bernoulli(\\hat{p}(X))\n",
    "        pX = self.prop_.predict_proba(X)[:, 1]\n",
    "        D = np.random.binomial(1, pX)\n",
    "\n",
    "        # Construct Y0, Y1 by re-sampling from residual distribution\n",
    "        y0 = self.est0_.predict(X) + self.res0_[np.random.choice(self.res0_.shape[0], n, replace=True)]\n",
    "        y1 = self.est1_.predict(X) + self.res1_[np.random.choice(self.res1_.shape[0], n, replace=True)]\n",
    "        \n",
    "        # Observed Y\n",
    "        y = y0*(1 - D) + y1*D\n",
    "        return X, D, y, y1, y0\n",
    "    \n",
    "    def y_cef(self, X, D):\n",
    "        \"\"\"\n",
    "        Returns the 'true' E[Y|X, D] in the semi-synthetic world\n",
    "        = the random forest predictions from the original data\n",
    "        \"\"\"\n",
    "        return self.est1_.predict(X)*D + self.est0_.predict(X)*(1 - D)\n",
    "    \n",
    "    def D_cef(self, X):\n",
    "        \"\"\"\n",
    "        Returns the 'true' E[D|X] in the semi-synthetic world\n",
    "        \"\"\"\n",
    "        return self.prop_.predict_proba(X)[:, 1]\n",
    "\n",
    "    @property\n",
    "    def true_ate(self):\n",
    "        \"\"\"\n",
    "        The 'true' ATE in the semi-synthetic world, i.e. E[f1(X) - f0(X)]\n",
    "        using the entire original X_ distribution.\n",
    "        \"\"\"\n",
    "        return np.mean(self.est1_.predict(self.X_) - self.est0_.predict(self.X_))\n",
    "\n",
    "\n",
    "def summary(\n",
    "    point, stderr,\n",
    "    yhat, Dhat,    # cross-fitted predictions for y and D\n",
    "    resy, resD,    # residuals y-yhat, D-Dhat\n",
    "    final_residual, # epsilon or drhat\n",
    "    X, D, y,\n",
    "    *,\n",
    "    name,\n",
    "    synth  # the semisynth object, so we can compare to the \"true\" functions\n",
    "):\n",
    "    true_ate = synth.true_ate\n",
    "    covered = (point - 1.96*stderr <= true_ate <= point + 1.96*stderr)\n",
    "\n",
    "    # We'll compute the \"true\" E[Y|D,X], E[D|X]\n",
    "    y_cef_true = synth.y_cef(X, D)\n",
    "    d_cef_true = synth.D_cef(X)\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'estimate':    [point],\n",
    "        'stderr':      [stderr],\n",
    "        'lower':       [point - 1.96*stderr],\n",
    "        'upper':       [point + 1.96*stderr],\n",
    "        'rmse y':      [np.sqrt(np.mean(resy**2))],  # RMSE vs. *observed* Y\n",
    "        'rmse D':      [np.sqrt(np.mean(resD**2))],  # RMSE vs. *observed* D\n",
    "        'accuracy D':  [np.mean(np.abs(resD) < 0.5)],# classification accuracy\n",
    "        # New columns:\n",
    "        'error':       [abs(point - true_ate)],    # how far from true\n",
    "        'rmse E[y|D,X]':[np.sqrt(np.mean((yhat - y_cef_true)**2))],\n",
    "        'rmse E[D|X]': [np.sqrt(np.mean((Dhat - d_cef_true)**2))],\n",
    "        'covered':     [1 if covered else 0]       # did CI cover true ATE?\n",
    "    }, index=[name])\n",
    "    \n",
    "    \n",
    "from copy import deepcopy\n",
    "\n",
    "synth = semisynth().fit(X, D, y, transformer, random_state=123)\n",
    "\n",
    "\n",
    "\n",
    "def run_plr_methods(X_train, D_train, y_train, synth):\n",
    "    \"\"\"\n",
    "    X_train, D_train, y_train come from synth.generate_data(...)\n",
    "    We'll replicate your DML approach for partial linear model\n",
    "    with multiple model combos and stack them in a table.\n",
    "    \"\"\"\n",
    "    results_table = []\n",
    "\n",
    "    # 1) Double Lasso with cross-fitting\n",
    "    # (a) specify pipelines\n",
    "    lassoy_ = deepcopy(lassoy)\n",
    "    lassod_ = deepcopy(lassod)\n",
    "    # (b) run\n",
    "    point, stderr, yhat, Dhat, resy, resD, eps = dml(X_train, D_train, y_train, lassoy_, lassod_, nfolds=5)\n",
    "    # (c) summary\n",
    "    df_ = summary(point, stderr, yhat, Dhat, resy, resD, eps,\n",
    "                  X_train, D_train, y_train, name='double lasso', synth=synth)\n",
    "    results_table.append(df_)\n",
    "\n",
    "    # 2) lasso / logistic\n",
    "    lassoy_ = deepcopy(lassoy)\n",
    "    lgrd_   = deepcopy(lgrd)\n",
    "    point, stderr, yhat, Dhat, resy, resD, eps = dml(X_train, D_train, y_train,\n",
    "                                                     lassoy_, lgrd_, nfolds=5, classifier=True)\n",
    "    df_ = summary(point, stderr, yhat, Dhat, resy, resD, eps,\n",
    "                  X_train, D_train, y_train, name='lasso/logistic', synth=synth)\n",
    "    results_table.append(df_)\n",
    "\n",
    "    # 3) Random Forest\n",
    "    rfy_ = deepcopy(rfy)\n",
    "    rfd_ = deepcopy(rfd)\n",
    "    point, stderr, yhat, Dhat, resy, resD, eps = dml(X_train, D_train, y_train,\n",
    "                                                     rfy_, rfd_, nfolds=5, classifier=True)\n",
    "    df_ = summary(point, stderr, yhat, Dhat, resy, resD, eps,\n",
    "                  X_train, D_train, y_train, name='random forest', synth=synth)\n",
    "    results_table.append(df_)\n",
    "\n",
    "    # 4) Decision Tree\n",
    "    dtry_ = deepcopy(dtry)\n",
    "    dtrd_ = deepcopy(dtrd)\n",
    "    point, stderr, yhat, Dhat, resy, resD, eps = dml(X_train, D_train, y_train,\n",
    "                                                     dtry_, dtrd_, nfolds=5, classifier=True)\n",
    "    df_ = summary(point, stderr, yhat, Dhat, resy, resD, eps,\n",
    "                  X_train, D_train, y_train, name='decision tree', synth=synth)\n",
    "    results_table.append(df_)\n",
    "\n",
    "    # 5) Boosted trees\n",
    "    gbfy_ = deepcopy(gbfy)\n",
    "    gbfd_ = deepcopy(gbfd)\n",
    "    point, stderr, yhat, Dhat, resy, resD, eps = dml(X_train, D_train, y_train,\n",
    "                                                     gbfy_, gbfd_, nfolds=5, classifier=True)\n",
    "    df_ = summary(point, stderr, yhat, Dhat, resy, resD, eps,\n",
    "                  X_train, D_train, y_train, name='boosted forest', synth=synth)\n",
    "    results_table.append(df_)\n",
    "\n",
    "    # 6) automl (semi-cfit)\n",
    "    # Similarly for stacking (semi-cfit).\n",
    "    flamly_ = make_pipeline(transformer, AutoML(time_budget=50,\n",
    "                                                task='regression',\n",
    "                                                early_stop=True,\n",
    "                                                eval_method='cv',\n",
    "                                                n_splits=3,\n",
    "                                                metric='r2',\n",
    "                                                verbose=0))\n",
    "    flamld_ = make_pipeline(transformer, AutoML(time_budget=50,\n",
    "                                                task='classification',\n",
    "                                                early_stop=True,\n",
    "                                                eval_method='cv',\n",
    "                                                n_splits=3,\n",
    "                                                metric='r2',\n",
    "                                                verbose=0))\n",
    "    # Fit Y, D on entire X_train\n",
    "    flamly_.fit(X_train, y_train)\n",
    "    besty_model = flamly_[-1].best_model_for_estimator(flamly_[-1].best_estimator)\n",
    "    besty = make_pipeline(transformer, clone(besty_model))\n",
    "\n",
    "    flamld_.fit(X_train, D_train)\n",
    "    bestd_model = flamld_[-1].best_model_for_estimator(flamld_[-1].best_estimator)\n",
    "    bestd = make_pipeline(transformer, clone(bestd_model))\n",
    "\n",
    "    point, stderr, yhat, Dhat, resy, resD, eps = dml(X_train, D_train, y_train,\n",
    "                                                     besty, bestd, nfolds=5, classifier=True)\n",
    "    df_ = summary(point, stderr, yhat, Dhat, resy, resD, eps,\n",
    "                  X_train, D_train, y_train, name='automl (semi-cfit)', synth=synth)\n",
    "    results_table.append(df_)\n",
    "\n",
    "    # 7) stacking (semi-cfit)\n",
    "    point, stderr, yhat, Dhat, resy, resD, eps = dml_dirty(\n",
    "        X_train, D_train, y_train,\n",
    "        [lassoy, rfy, dtry, gbfy],\n",
    "        [lgrd, rfd, dtrd, gbfd],\n",
    "        nfolds=5, classifier=True\n",
    "    )\n",
    "    df_ = summary(point, stderr, yhat, Dhat, resy, resD, eps,\n",
    "                  X_train, D_train, y_train, name='stacked (semi-cfit)', synth=synth)\n",
    "    results_table.append(df_)\n",
    "    \n",
    "    #8 select best\n",
    "    point, stderr, yhat, Dhat, resy, resD, eps = dml_select_best(\n",
    "        X_train, D_train, y_train,\n",
    "        [lassoy, rfy, dtry, gbfy],\n",
    "        [lgrd, rfd, dtrd, gbfd],\n",
    "        nfolds=5, classifier=True\n",
    "    )\n",
    "    df_ = summary(point, stderr, yhat, Dhat, resy, resD, eps,\n",
    "                    X_train, D_train, y_train, name='select-best (semi-cfit)', synth=synth)\n",
    "    results_table.append(df_)\n",
    "\n",
    "    return pd.concat(results_table)\n",
    "\n",
    "\n",
    "def run_irm_methods(X_train, D_train, y_train, synth):\n",
    "    \"\"\"\n",
    "    X_train, D_train, y_train from synth.generate_data(...)\n",
    "    We'll replicate your IRM approach with multiple model combos and stack in a table.\n",
    "    \"\"\"\n",
    "    results_table = []\n",
    "\n",
    "    # 1) lasso-lasso, logistic repeated seeds + median aggregator\n",
    "    # We'll do a single run for demonstration:\n",
    "    lassoy_ = deepcopy(lassoytest)  # or define a pipeline as in the notebook\n",
    "    lgrd_   = deepcopy(lgrdtest)\n",
    "\n",
    "    point, stderr, yhat, Dhat, resy, resD, drhat = dr(X_train, D_train, y_train,\n",
    "                                                      lassoy_, lassoy_,\n",
    "                                                      lgrd_, nfolds=5)\n",
    "    df_ = summary(point, stderr, yhat, Dhat, resy, resD, drhat,\n",
    "                  X_train, D_train, y_train, name='lasso/logistic', synth=synth)\n",
    "    results_table.append(df_)\n",
    "\n",
    "    # 2) random forest\n",
    "    rfy_ = deepcopy(rfy)\n",
    "    rfd_ = deepcopy(rfd)\n",
    "    point, stderr, yhat, Dhat, resy, resD, drhat = dr(X_train, D_train, y_train,\n",
    "                                                      rfy_, rfy_, rfd_, nfolds=5)\n",
    "    df_ = summary(point, stderr, yhat, Dhat, resy, resD, drhat,\n",
    "                  X_train, D_train, y_train, name='random forest', synth=synth)\n",
    "    results_table.append(df_)\n",
    "\n",
    "    # 3) decision tree\n",
    "    dtry_ = deepcopy(dtry)\n",
    "    dtrd_ = deepcopy(dtrd)\n",
    "    point, stderr, yhat, Dhat, resy, resD, drhat = dr(X_train, D_train, y_train,\n",
    "                                                      dtry_, dtry_, dtrd_, nfolds=5)\n",
    "    df_ = summary(point, stderr, yhat, Dhat, resy, resD, drhat,\n",
    "                  X_train, D_train, y_train, name='decision tree', synth=synth)\n",
    "    results_table.append(df_)\n",
    "\n",
    "    # 4) boosted forest\n",
    "    gbfy_ = deepcopy(gbfy)\n",
    "    gbfd_ = deepcopy(gbfd)\n",
    "    point, stderr, yhat, Dhat, resy, resD, drhat = dr(X_train, D_train, y_train,\n",
    "                                                      gbfy_, gbfy_, gbfd_, nfolds=5)\n",
    "    df_ = summary(point, stderr, yhat, Dhat, resy, resD, drhat,\n",
    "                  X_train, D_train, y_train, name='boosted forest', synth=synth)\n",
    "    results_table.append(df_)\n",
    "\n",
    "    # 5) automl\n",
    "    flamly0_ = make_pipeline(transformer, AutoML(time_budget=30, task='regression', early_stop=True,\n",
    "                                                eval_method='cv', n_splits=3, metric='r2', verbose=0))\n",
    "    flamly1_ = make_pipeline(transformer, AutoML(time_budget=30, task='regression', early_stop=True,\n",
    "                                                eval_method='cv', n_splits=3, metric='r2', verbose=0))\n",
    "    flamld_  = make_pipeline(transformer, AutoML(time_budget=30, task='classification', early_stop=True,\n",
    "                                                eval_method='cv', n_splits=3, metric='r2', verbose=0))\n",
    "    # Fit for Y|D=0, Y|D=1\n",
    "    flamly0_.fit(X_train[D_train == 0], y_train[D_train == 0])\n",
    "    besty0_model = flamly0_[-1].best_model_for_estimator(flamly0_[-1].best_estimator)\n",
    "    besty0 = make_pipeline(transformer, clone(besty0_model))\n",
    "\n",
    "    flamly1_.fit(X_train[D_train == 1], y_train[D_train == 1])\n",
    "    besty1_model = flamly1_[-1].best_model_for_estimator(flamly1_[-1].best_estimator)\n",
    "    besty1 = make_pipeline(transformer, clone(besty1_model))\n",
    "\n",
    "    flamld_.fit(X_train, D_train)\n",
    "    bestd_model = flamld_[-1].best_model_for_estimator(flamld_[-1].best_estimator)\n",
    "    bestd = make_pipeline(transformer, clone(bestd_model))\n",
    "\n",
    "    point, stderr, yhat, Dhat, resy, resD, drhat = dr(X_train, D_train, y_train,\n",
    "                                                      besty0, besty1, bestd,\n",
    "                                                      nfolds=5)\n",
    "    df_ = summary(point, stderr, yhat, Dhat, resy, resD, drhat,\n",
    "                  X_train, D_train, y_train, name='automl (semi-cfit)', synth=synth)\n",
    "    results_table.append(df_)\n",
    "\n",
    "    # 6) stacking (semi-cfit):\n",
    "    lassoy_ = deepcopy(lassoy)\n",
    "    rfy_ = deepcopy(rfy)\n",
    "    dtry_ = deepcopy(dtry)\n",
    "    gbfy_ = deepcopy(gbfy)\n",
    "\n",
    "    lgrd_ = deepcopy(lgrd)\n",
    "    rfd_  = deepcopy(rfd)\n",
    "    dtrd_ = deepcopy(dtrd)\n",
    "    gbfd_ = deepcopy(gbfd)\n",
    "\n",
    "    point, stderr, yhat, Dhat, resy, resD, drhat = dr_dirty(\n",
    "        X_train, D_train, y_train,\n",
    "        [lassoy_, rfy_, dtry_, gbfy_],\n",
    "        [lassoy_, rfy_, dtry_, gbfy_],\n",
    "        [lgrd_, rfd_, dtrd_, gbfd_],\n",
    "        nfolds=5\n",
    "    )\n",
    "    df_ = summary(point, stderr, yhat, Dhat, resy, resD, drhat,\n",
    "                  X_train, D_train, y_train, name='stacked (semi-cfit)', synth=synth)\n",
    "    results_table.append(df_)\n",
    "    \n",
    "    # 7) select best\n",
    "    \n",
    "    point, stderr, yhat, Dhat, resy, resD, drhat = dr_select_best(\n",
    "        X_train, D_train, y_train,\n",
    "        [lassoy, rfy, dtry, gbfy],\n",
    "        [lassoy, rfy, dtry, gbfy],\n",
    "        [lgrd, rfd, dtrd, gbfd],\n",
    "        nfolds=5\n",
    "    )\n",
    "    df_ = summary(point, stderr, yhat, Dhat, resy, resD, drhat,\n",
    "                    X_train, D_train, y_train, name='select-best (semi-cfit)', synth=synth)\n",
    "    results_table.append(df_)\n",
    "\n",
    "    return pd.concat(results_table)\n",
    "\n",
    "\n",
    "for n in [1000, 10000, 50000]:\n",
    "    print(f\"\\n=== Semi-Synthetic Data with n={n} ===\")\n",
    "    print(\"True ATE in the semi-synthetic world: \", synth.true_ate)\n",
    "    X_synth, D_synth, y_synth, y1_synth, y0_synth = synth.generate_data(n)\n",
    "\n",
    "\n",
    "    print(f\"** PLR Results n = {n} **\")\n",
    "    print(\"True ATE in the semi-synthetic world: \", synth.true_ate)\n",
    "    table_plr = run_plr_methods(X_synth, D_synth, y_synth, synth)\n",
    "    print(table_plr)\n",
    "\n",
    "    print(f\"** IRM Results n = {n}**\")\n",
    "    table_irm = run_irm_methods(X_synth, D_synth, y_synth, synth)\n",
    "    print(table_irm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For n = 1000:    \n",
    "In the PLR setting, random forest outperforms all other methods in terms of the estimate. \n",
    "In the IRM setting, the random forest outperforms all other methods in terms of the estimate.   \n",
    "So neither automl nor stacking perform as well as the best model alone.  \n",
    "\n",
    "For n = 10000:  \n",
    "\n",
    "In the PLR setting, the decision tree performs best in terms of the estimate.     \n",
    "In the IRM setting, lasso / logistic performs best in terms of the estimate.   \n",
    "So neither stacking nor automl perform as well as the best model alone.   \n",
    "  \n",
    "For n = 50000:  \n",
    "  \n",
    "In the PLR setting, Decision tree performs best in terms of the estimate.   \n",
    "In the IRM setting, Decision tree performs best in terms of the estimate.  \n",
    "Neither automl nor stacking perform as well as the best model alone.  \n",
    "\n",
    "  \n",
    "Please note that running the code repeatedly showed that the results can vary due to randomness. The results above are from a single run and I saw other models perform best in some cases in other runs.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs288_alt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
