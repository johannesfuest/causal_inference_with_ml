{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1\n",
    "\n",
    "We start with the PLR part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTHONWARNINGS=ignore\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>estimate</th>\n",
       "      <th>stderr</th>\n",
       "      <th>lower</th>\n",
       "      <th>upper</th>\n",
       "      <th>rmse y</th>\n",
       "      <th>rmse D</th>\n",
       "      <th>accuracy D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>double lasso</th>\n",
       "      <td>9035.120004</td>\n",
       "      <td>1295.135748</td>\n",
       "      <td>6496.653938</td>\n",
       "      <td>11573.586070</td>\n",
       "      <td>54254.468883</td>\n",
       "      <td>0.443406</td>\n",
       "      <td>0.688553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lasso/logistic</th>\n",
       "      <td>9092.508157</td>\n",
       "      <td>1304.398170</td>\n",
       "      <td>6535.887743</td>\n",
       "      <td>11649.128571</td>\n",
       "      <td>54254.468883</td>\n",
       "      <td>0.444043</td>\n",
       "      <td>0.687847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random forest</th>\n",
       "      <td>8814.219477</td>\n",
       "      <td>1353.875804</td>\n",
       "      <td>6160.622901</td>\n",
       "      <td>11467.816052</td>\n",
       "      <td>54941.244922</td>\n",
       "      <td>0.444556</td>\n",
       "      <td>0.688452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>decision tree</th>\n",
       "      <td>9236.195678</td>\n",
       "      <td>1440.551643</td>\n",
       "      <td>6412.714457</td>\n",
       "      <td>12059.676898</td>\n",
       "      <td>59427.392172</td>\n",
       "      <td>0.446437</td>\n",
       "      <td>0.688048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>boosted forest</th>\n",
       "      <td>9205.781342</td>\n",
       "      <td>1349.593254</td>\n",
       "      <td>6560.578565</td>\n",
       "      <td>11850.984119</td>\n",
       "      <td>55720.228404</td>\n",
       "      <td>0.443291</td>\n",
       "      <td>0.690267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>automl (semi-cfit)</th>\n",
       "      <td>8884.424774</td>\n",
       "      <td>1306.176253</td>\n",
       "      <td>6324.319318</td>\n",
       "      <td>11444.530230</td>\n",
       "      <td>53908.841872</td>\n",
       "      <td>0.443595</td>\n",
       "      <td>0.690973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stacked (semi-cfit)</th>\n",
       "      <td>8910.869879</td>\n",
       "      <td>1304.427306</td>\n",
       "      <td>6354.192359</td>\n",
       "      <td>11467.547398</td>\n",
       "      <td>53979.491147</td>\n",
       "      <td>0.442801</td>\n",
       "      <td>0.690671</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        estimate       stderr        lower         upper  \\\n",
       "double lasso         9035.120004  1295.135748  6496.653938  11573.586070   \n",
       "lasso/logistic       9092.508157  1304.398170  6535.887743  11649.128571   \n",
       "random forest        8814.219477  1353.875804  6160.622901  11467.816052   \n",
       "decision tree        9236.195678  1440.551643  6412.714457  12059.676898   \n",
       "boosted forest       9205.781342  1349.593254  6560.578565  11850.984119   \n",
       "automl (semi-cfit)   8884.424774  1306.176253  6324.319318  11444.530230   \n",
       "stacked (semi-cfit)  8910.869879  1304.427306  6354.192359  11467.547398   \n",
       "\n",
       "                           rmse y    rmse D  accuracy D  \n",
       "double lasso         54254.468883  0.443406    0.688553  \n",
       "lasso/logistic       54254.468883  0.444043    0.687847  \n",
       "random forest        54941.244922  0.444556    0.688452  \n",
       "decision tree        59427.392172  0.446437    0.688048  \n",
       "boosted forest       55720.228404  0.443291    0.690267  \n",
       "automl (semi-cfit)   53908.841872  0.443595    0.690973  \n",
       "stacked (semi-cfit)  53979.491147  0.442801    0.690671  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%env PYTHONWARNINGS=ignore\n",
    "\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=ConvergenceWarning)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter('ignore')\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\".*did not converge.*\",\n",
    "    category=ConvergenceWarning\n",
    ")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_predict, KFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LassoCV, LinearRegression, LogisticRegressionCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.base import TransformerMixin, BaseEstimator, clone\n",
    "from formulaic import Formula\n",
    "from flaml.automl import AutoML\n",
    "np.random.seed(1234)\n",
    "# set random seed for all other libraries\n",
    "import random\n",
    "random.seed(1234)\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '1234'\n",
    "\n",
    "\n",
    "file = \"https://raw.githubusercontent.com/CausalAIBook/MetricsMLNotebooks/main/data/401k.csv\"\n",
    "data = pd.read_csv(file)\n",
    "y = data['net_tfa'].values\n",
    "D = data['e401'].values\n",
    "D2 = data['p401'].values\n",
    "D3 = data['a401'].values\n",
    "X = data.drop(['e401', 'p401', 'a401', 'tw', 'tfa', 'net_tfa', 'tfa_he',\n",
    "               'hval', 'hmort', 'hequity',\n",
    "               'nifa', 'net_nifa', 'net_n401', 'ira',\n",
    "               'dum91', 'icat', 'ecat', 'zhat',\n",
    "               'i1', 'i2', 'i3', 'i4', 'i5', 'i6', 'i7',\n",
    "               'a1', 'a2', 'a3', 'a4', 'a5'], axis=1)\n",
    "\n",
    "class FormulaTransformer(TransformerMixin, BaseEstimator):\n",
    "\n",
    "    def __init__(self, formula, array=False):\n",
    "        self.formula = formula\n",
    "        self.array = array\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        df = Formula(self.formula).get_model_matrix(X)\n",
    "        if self.array:\n",
    "            return df.values\n",
    "        return df\n",
    "transformer = FormulaTransformer(\"0 + poly(age, degree=6, raw=True) + poly(inc, degree=8, raw=True) \"\n",
    "                                 \"+ poly(educ, degree=4, raw=True) + poly(fsize, degree=2, raw=True) \"\n",
    "                                 \"+ male + marr + twoearn + db + pira + hown\", array=True)\n",
    "\n",
    "def dml(X, D, y, modely, modeld, *, nfolds, classifier=False):\n",
    "    '''\n",
    "    DML for the Partially Linear Model setting with cross-fitting\n",
    "\n",
    "    Input\n",
    "    -----\n",
    "    X: the controls\n",
    "    D: the treatment\n",
    "    y: the outcome\n",
    "    modely: the ML model for predicting the outcome y\n",
    "    modeld: the ML model for predicting the treatment D\n",
    "    nfolds: the number of folds in cross-fitting\n",
    "    classifier: bool, whether the modeld is a classifier or a regressor\n",
    "\n",
    "    Output\n",
    "    ------\n",
    "    point: the point estimate of the treatment effect of D on y\n",
    "    stderr: the standard error of the treatment effect\n",
    "    yhat: the cross-fitted predictions for the outcome y\n",
    "    Dhat: the cross-fitted predictions for the treatment D\n",
    "    resy: the outcome residuals\n",
    "    resD: the treatment residuals\n",
    "    epsilon: the final residual-on-residual OLS regression residual\n",
    "    '''\n",
    "    cv = KFold(n_splits=nfolds, shuffle=True, random_state=123)  # shuffled k-folds\n",
    "    yhat = cross_val_predict(modely, X, y, cv=cv, n_jobs=-1)  # out-of-fold predictions for y\n",
    "    # out-of-fold predictions for D\n",
    "    # use predict or predict_proba dependent on classifier or regressor for D\n",
    "    if classifier:\n",
    "        Dhat = cross_val_predict(modeld, X, D, cv=cv, method='predict_proba', n_jobs=-1)[:, 1]\n",
    "    else:\n",
    "        Dhat = cross_val_predict(modeld, X, D, cv=cv, n_jobs=-1)\n",
    "    # calculate outcome and treatment residuals\n",
    "    resy = y - yhat\n",
    "    resD = D - Dhat\n",
    "\n",
    "    # final stage ols based point estimate and standard error\n",
    "    point = np.mean(resy * resD) / np.mean(resD**2)\n",
    "    epsilon = resy - point * resD\n",
    "    var = np.mean(epsilon**2 * resD**2) / np.mean(resD**2)**2\n",
    "    stderr = np.sqrt(var / X.shape[0])\n",
    "\n",
    "    return point, stderr, yhat, Dhat, resy, resD, epsilon\n",
    "\n",
    "def summary(point, stderr, yhat, Dhat, resy, resD, epsilon, X, D, y, *, name):\n",
    "    '''\n",
    "    Convenience summary function that takes the results of the DML function\n",
    "    and summarizes several estimation quantities and performance metrics.\n",
    "    '''\n",
    "    return pd.DataFrame({'estimate': point,  # point estimate\n",
    "                         'stderr': stderr,  # standard error\n",
    "                         'lower': point - 1.96 * stderr,  # lower end of 95% confidence interval\n",
    "                         'upper': point + 1.96 * stderr,  # upper end of 95% confidence interval\n",
    "                         'rmse y': np.sqrt(np.mean(resy**2)),  # RMSE of model that predicts outcome y\n",
    "                         'rmse D': np.sqrt(np.mean(resD**2)),  # RMSE of model that predicts treatment D\n",
    "                         'accuracy D': np.mean(np.abs(resD) < .5),  # binary classification accuracy of model for D\n",
    "                         }, index=[name])\n",
    "# double lasso with cross-fitting\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "lassoy = make_pipeline(transformer, StandardScaler(), LassoCV(cv=cv))\n",
    "lassod = make_pipeline(transformer, StandardScaler(), LassoCV(cv=cv))\n",
    "result = dml(X, D, y, lassoy, lassod, nfolds=5)\n",
    "table = summary(*result, X, D, y, name='double lasso')\n",
    "\n",
    "# penalized logreg for D (default is l2 penalty)\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "lassoy = make_pipeline(transformer, StandardScaler(), LassoCV(cv=cv))\n",
    "lgrd = make_pipeline(transformer, StandardScaler(), LogisticRegressionCV(cv=cv))\n",
    "result = dml(X, D, y, lassoy, lgrd, nfolds=5, classifier=True)\n",
    "table = pd.concat([table, summary(*result, X, D, y, name='lasso/logistic')])\n",
    "\n",
    "# random forest\n",
    "rfy = make_pipeline(transformer, RandomForestRegressor(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001))\n",
    "rfd = make_pipeline(transformer, RandomForestClassifier(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001))\n",
    "result = dml(X, D, y, rfy, rfd, nfolds=5, classifier=True)\n",
    "table = pd.concat([table, summary(*result, X, D, y, name='random forest')])\n",
    "\n",
    "# decision tree\n",
    "dtry = make_pipeline(transformer, DecisionTreeRegressor(min_samples_leaf=10, ccp_alpha=.001))\n",
    "dtrd = make_pipeline(transformer, DecisionTreeClassifier(min_samples_leaf=10, ccp_alpha=.001))\n",
    "result = dml(X, D, y, dtry, dtrd, nfolds=5, classifier=True)\n",
    "table = pd.concat([table, summary(*result, X, D, y, name='decision tree')])\n",
    "\n",
    "# boosted trees\n",
    "gbfy = make_pipeline(transformer, GradientBoostingRegressor(max_depth=2, n_iter_no_change=5))\n",
    "gbfd = make_pipeline(transformer, GradientBoostingClassifier(max_depth=2, n_iter_no_change=5))\n",
    "result = dml(X, D, y, gbfy, gbfd, nfolds=5, classifier=True)\n",
    "table = pd.concat([table, summary(*result, X, D, y, name='boosted forest')])\n",
    "\n",
    "# semi cross fitting: To avoid the computational cost of performing model selection within each fold (assuming that we don't select among an exponential set of hyperparameters/models in the number of samples), it is ok to perform model selection using all the data and then perform cross-fitting with the selected model\n",
    "flamly = make_pipeline(transformer, AutoML(time_budget=100, task='regression', early_stop=True,\n",
    "                                           eval_method='cv', n_splits=3, metric='r2', verbose=0))\n",
    "flamld = make_pipeline(transformer, AutoML(time_budget=100, task='classification', early_stop=True,\n",
    "                                           eval_method='cv', n_splits=3, metric='r2', verbose=0))\n",
    "flamly.fit(X, y)\n",
    "besty = make_pipeline(transformer, clone(flamly[-1].best_model_for_estimator(flamly[-1].best_estimator)))\n",
    "flamld.fit(X, D)\n",
    "bestd = make_pipeline(transformer, clone(flamld[-1].best_model_for_estimator(flamld[-1].best_estimator)))\n",
    "result = dml(X, D, y, besty, bestd, nfolds=5, classifier=True)\n",
    "table = pd.concat([table, summary(*result, X, D, y, name='automl (semi-cfit)')])\n",
    "\n",
    "# semi cross fitting with stacking\n",
    "def dml_dirty(X, D, y, modely_list, modeld_list, *,\n",
    "              stacker=LinearRegression(), nfolds, classifier=False):\n",
    "    '''\n",
    "    DML for the Partially Linear Model setting with semi-cross-fitting\n",
    "\n",
    "    Input\n",
    "    -----\n",
    "    X: the controls\n",
    "    D: the treatment\n",
    "    y: the outcome\n",
    "    modely: the ML model for predicting the outcome y\n",
    "    modeld: the ML model for predicting the treatment D\n",
    "    stacker: model used to aggregate predictions of each of the base models\n",
    "    nfolds: the number of folds in cross-fitting\n",
    "    classifier: bool, whether the modeld is a classifier or a regressor\n",
    "\n",
    "    Output\n",
    "    ------\n",
    "    point: the point estimate of the treatment effect of D on y\n",
    "    stderr: the standard error of the treatment effect\n",
    "    yhat: the cross-fitted predictions for the outcome y\n",
    "    Dhat: the cross-fitted predictions for the treatment D\n",
    "    resy: the outcome residuals\n",
    "    resD: the treatment residuals\n",
    "    epsilon: the final residual-on-residual OLS regression residual\n",
    "    '''\n",
    "    # construct out-of-fold predictions for each model\n",
    "    cv = KFold(n_splits=nfolds, shuffle=True, random_state=123)\n",
    "    yhats = np.array([cross_val_predict(modely, X, y, cv=cv, n_jobs=-1) for modely in modely_list]).T\n",
    "    if classifier:\n",
    "        Dhats = np.array([cross_val_predict(modeld, X, D, cv=cv, method='predict_proba', n_jobs=-1)[:, 1]\n",
    "                         for modeld in modeld_list]).T\n",
    "    else:\n",
    "        Dhats = np.array([cross_val_predict(modeld, X, D, cv=cv, n_jobs=-1) for modeld in modeld_list]).T\n",
    "    # calculate stacked residuals by finding optimal coefficients\n",
    "    # and weigthing out-of-sample predictions by these coefficients\n",
    "    yhat = stacker.fit(yhats, y).predict(yhats)\n",
    "    Dhat = stacker.fit(Dhats, D).predict(Dhats)\n",
    "    resy = y - yhat\n",
    "    resD = D - Dhat\n",
    "    # go with the stacked residuals\n",
    "    point = np.mean(resy * resD) / np.mean(resD**2)\n",
    "    epsilon = resy - point * resD\n",
    "    var = np.mean(epsilon**2 * resD**2) / np.mean(resD**2)**2\n",
    "    stderr = np.sqrt(var / X.shape[0])\n",
    "    return point, stderr, yhat, Dhat, resy, resD, epsilon\n",
    "\n",
    "result = dml_dirty(X, D, y, [lassoy, rfy, dtry, gbfy], [lgrd, rfd, dtrd, gbfd],\n",
    "                   nfolds=5, classifier=True)\n",
    "table = pd.concat([table, summary(*result, X, D, y, name='stacked (semi-cfit)')])\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Bottom 25% Income Sample ===\n",
      "                                  estimate       stderr        lower  \\\n",
      "bottom25% double lasso         3769.763582  1092.945664  1627.590080   \n",
      "bottom25% lasso/logistic       3803.759038  1072.578658  1701.504869   \n",
      "bottom25% random forest        4262.063214  1086.618469  2132.291016   \n",
      "bottom25% decision tree        3513.823970  1032.823896  1489.489135   \n",
      "bottom25% boosted forest       3798.466838  1102.222171  1638.111382   \n",
      "bottom25% automl (semi-cfit)   3818.527486  1094.432778  1673.439240   \n",
      "bottom25% stacked (semi-cfit)  3962.500707  1098.589461  1809.265363   \n",
      "\n",
      "                                     upper        rmse y    rmse D  accuracy D  \n",
      "bottom25% double lasso         5911.937083  13400.361810  0.343801    0.846433  \n",
      "bottom25% lasso/logistic       5906.013207  13400.361810  0.354799    0.844015  \n",
      "bottom25% random forest        6391.835413  13546.377469  0.345900    0.844418  \n",
      "bottom25% decision tree        5538.158806  14728.635257  0.380372    0.804111  \n",
      "bottom25% boosted forest       5958.822294  13525.775549  0.345385    0.844418  \n",
      "bottom25% automl (semi-cfit)   5963.615731  13480.512942  0.351340    0.835953  \n",
      "bottom25% stacked (semi-cfit)  6115.736052  13337.755903  0.342429    0.847239  \n",
      "\n",
      "=== Top 25% Income Sample ===\n",
      "                                estimate       stderr         lower  \\\n",
      "top25% double lasso         17505.836955  3902.624431   9856.693070   \n",
      "top25% lasso/logistic       18204.194590  3867.405524  10624.079763   \n",
      "top25% random forest        17330.631463  4019.408211   9452.591368   \n",
      "top25% decision tree        15270.539604  3774.945121   7871.647166   \n",
      "top25% boosted forest       17839.759721  3885.269506  10224.631489   \n",
      "top25% automl (semi-cfit)   17387.173148  3834.332285   9871.881869   \n",
      "top25% stacked (semi-cfit)  17879.380132  3856.952206  10319.753807   \n",
      "\n",
      "                                   upper         rmse y    rmse D  accuracy D  \n",
      "top25% double lasso         25154.980841   91393.039963  0.483189    0.601049  \n",
      "top25% lasso/logistic       25784.309418   91393.039963  0.482708    0.601049  \n",
      "top25% random forest        25208.671557   94139.908813  0.485149    0.602259  \n",
      "top25% decision tree        22669.432041  101948.859766  0.542915    0.538927  \n",
      "top25% boosted forest       25454.887954   94738.263333  0.484598    0.592981  \n",
      "top25% automl (semi-cfit)   24902.464427   95109.874876  0.484810    0.599435  \n",
      "top25% stacked (semi-cfit)  25439.006456   91352.535711  0.481764    0.612747  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_bottom = data.loc[data['inc'] < data['inc'].quantile(0.25)].copy()\n",
    "data_top = data.loc[data['inc'] > data['inc'].quantile(0.75)].copy()\n",
    "\n",
    "# Define helper function to create X, D, y from a subset\n",
    "def extract_XDy(df):\n",
    "    \"\"\"Given a subset of the 401k data, produce X, D, and y \n",
    "    consistent with the main analysis.\"\"\"\n",
    "    y_ = df['net_tfa'].values\n",
    "    D_ = df['e401'].values\n",
    "    X_ = df.drop([\n",
    "        'e401', 'p401', 'a401', 'tw', 'tfa', 'net_tfa', 'tfa_he',\n",
    "        'hval', 'hmort', 'hequity', 'nifa', 'net_nifa', 'net_n401',\n",
    "        'ira', 'dum91', 'icat', 'ecat', 'zhat', 'i1', 'i2', 'i3',\n",
    "        'i4', 'i5', 'i6', 'i7', 'a1', 'a2', 'a3', 'a4', 'a5'\n",
    "    ], axis=1)\n",
    "    return X_, D_, y_\n",
    "\n",
    "X_bottom, D_bottom, y_bottom = extract_XDy(data_bottom)\n",
    "X_top, D_top, y_top = extract_XDy(data_top)\n",
    "\n",
    "\n",
    "# Create a helper function that runs all models and returns a summary table\n",
    "def run_all_estimators(X, D, y, name_prefix=''):\n",
    "    \"\"\"Runs the pipeline of estimators and returns a summary results table.\"\"\"\n",
    "    table_local = []\n",
    "\n",
    "    # cross-validation setup\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "    \n",
    "    # 1) double lasso\n",
    "    lassoy = make_pipeline(transformer, StandardScaler(), LassoCV(cv=cv))\n",
    "    lassod = make_pipeline(transformer, StandardScaler(), LassoCV(cv=cv))\n",
    "    result = dml(X, D, y, lassoy, lassod, nfolds=5)\n",
    "    table_local.append(summary(*result, X, D, y, name=f'{name_prefix} double lasso'))\n",
    "\n",
    "    # 2) lasso/logistic\n",
    "    lassoy = make_pipeline(transformer, StandardScaler(), LassoCV(cv=cv))\n",
    "    lgrd = make_pipeline(transformer, StandardScaler(), LogisticRegressionCV(cv=cv))\n",
    "    result = dml(X, D, y, lassoy, lgrd, nfolds=5, classifier=True)\n",
    "    table_local.append(summary(*result, X, D, y, name=f'{name_prefix} lasso/logistic'))\n",
    "\n",
    "    # 3) random forest\n",
    "    rfy = make_pipeline(transformer,\n",
    "                        RandomForestRegressor(n_estimators=100,\n",
    "                                              min_samples_leaf=10,\n",
    "                                              ccp_alpha=0.001))\n",
    "    rfd = make_pipeline(transformer,\n",
    "                        RandomForestClassifier(n_estimators=100,\n",
    "                                               min_samples_leaf=10,\n",
    "                                               ccp_alpha=0.001))\n",
    "    result = dml(X, D, y, rfy, rfd, nfolds=5, classifier=True)\n",
    "    table_local.append(summary(*result, X, D, y, name=f'{name_prefix} random forest'))\n",
    "\n",
    "    # 4) decision tree\n",
    "    dtry = make_pipeline(transformer,\n",
    "                         DecisionTreeRegressor(min_samples_leaf=10,\n",
    "                                               ccp_alpha=0.001))\n",
    "    dtrd = make_pipeline(transformer,\n",
    "                         DecisionTreeClassifier(min_samples_leaf=10,\n",
    "                                                ccp_alpha=0.001))\n",
    "    result = dml(X, D, y, dtry, dtrd, nfolds=5, classifier=True)\n",
    "    table_local.append(summary(*result, X, D, y, name=f'{name_prefix} decision tree'))\n",
    "\n",
    "    # 5) boosted trees\n",
    "    gbfy = make_pipeline(transformer,\n",
    "                         GradientBoostingRegressor(max_depth=2,\n",
    "                                                   n_iter_no_change=5))\n",
    "    gbfd = make_pipeline(transformer,\n",
    "                         GradientBoostingClassifier(max_depth=2,\n",
    "                                                    n_iter_no_change=5))\n",
    "    result = dml(X, D, y, gbfy, gbfd, nfolds=5, classifier=True)\n",
    "    table_local.append(summary(*result, X, D, y, name=f'{name_prefix} boosted forest'))\n",
    "\n",
    "    # 6) automl (semi cross-fitting)\n",
    "    flamly = make_pipeline(transformer,\n",
    "                           AutoML(time_budget=60,  # reduce if desired\n",
    "                                  task='regression',\n",
    "                                  early_stop=True,\n",
    "                                  eval_method='cv',\n",
    "                                  n_splits=3,\n",
    "                                  metric='r2',\n",
    "                                  verbose=0))\n",
    "    flamld = make_pipeline(transformer,\n",
    "                           AutoML(time_budget=60,\n",
    "                                  task='classification',\n",
    "                                  early_stop=True,\n",
    "                                  eval_method='cv',\n",
    "                                  n_splits=3,\n",
    "                                  metric='r2',\n",
    "                                  verbose=0))\n",
    "    # Fit once on entire data\n",
    "    flamly.fit(X, y)\n",
    "    besty_model = flamly[-1].best_model_for_estimator(flamly[-1].best_estimator)\n",
    "    besty = make_pipeline(transformer, clone(besty_model))\n",
    "\n",
    "    flamld.fit(X, D)\n",
    "    bestd_model = flamld[-1].best_model_for_estimator(flamld[-1].best_estimator)\n",
    "    bestd = make_pipeline(transformer, clone(bestd_model))\n",
    "\n",
    "    result = dml(X, D, y, besty, bestd, nfolds=5, classifier=True)\n",
    "    table_local.append(summary(*result, X, D, y, name=f'{name_prefix} automl (semi-cfit)'))\n",
    "\n",
    "    # 7) stacked (semi-cfit)\n",
    "    # Re-use the same base models we created, but put them into lists:\n",
    "    # - Notice we must re-create them fresh so they are unfitted before cross_val_predict\n",
    "    #   or cross_val_predict won't do what we expect.\n",
    "    lassoy_ = make_pipeline(transformer, StandardScaler(), LassoCV(cv=cv))\n",
    "    lgrd_ = make_pipeline(transformer, StandardScaler(), LogisticRegressionCV(cv=cv))\n",
    "    rfy_ = make_pipeline(transformer,\n",
    "                         RandomForestRegressor(n_estimators=100,\n",
    "                                               min_samples_leaf=10,\n",
    "                                               ccp_alpha=0.001))\n",
    "    rfd_ = make_pipeline(transformer,\n",
    "                         RandomForestClassifier(n_estimators=100,\n",
    "                                                min_samples_leaf=10,\n",
    "                                                ccp_alpha=0.001))\n",
    "    dtry_ = make_pipeline(transformer,\n",
    "                          DecisionTreeRegressor(min_samples_leaf=10,\n",
    "                                                ccp_alpha=0.001))\n",
    "    dtrd_ = make_pipeline(transformer,\n",
    "                          DecisionTreeClassifier(min_samples_leaf=10,\n",
    "                                                 ccp_alpha=0.001))\n",
    "    gbfy_ = make_pipeline(transformer,\n",
    "                          GradientBoostingRegressor(max_depth=2,\n",
    "                                                    n_iter_no_change=5))\n",
    "    gbfd_ = make_pipeline(transformer,\n",
    "                          GradientBoostingClassifier(max_depth=2,\n",
    "                                                     n_iter_no_change=5))\n",
    "\n",
    "    modely_list = [lassoy_, rfy_, dtry_, gbfy_]\n",
    "    modeld_list = [lgrd_, rfd_, dtrd_, gbfd_]\n",
    "\n",
    "    result = dml_dirty(X, D, y, modely_list, modeld_list,\n",
    "                       stacker=LinearRegression(),\n",
    "                       nfolds=5, classifier=True)\n",
    "    table_local.append(summary(*result, X, D, y, name=f'{name_prefix} stacked (semi-cfit)'))\n",
    "\n",
    "    # Concatenate all results\n",
    "    return pd.concat(table_local)\n",
    "\n",
    "\n",
    "table_bottom = run_all_estimators(X_bottom, D_bottom, y_bottom, name_prefix='bottom25%')\n",
    "table_top = run_all_estimators(X_top, D_top, y_top, name_prefix='top25%')\n",
    "\n",
    "print(\"=== Bottom 25% Income Sample ===\")\n",
    "print(table_bottom)\n",
    "\n",
    "print(\"\\n=== Top 25% Income Sample ===\")\n",
    "print(table_top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the PLR setting, there indeed seems to be heterogeneity in the treatment with respect to income, with the bottom 25% of earners seeing estimates around 3.8k and the top 25% seeing estimates of around 17.5k. The different machine learning models are broadly consistent across all three income groups.  \n",
    "  \n",
    "Next we more to the IRM part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>estimate</th>\n",
       "      <th>stderr</th>\n",
       "      <th>lower</th>\n",
       "      <th>upper</th>\n",
       "      <th>rmse y</th>\n",
       "      <th>rmse D</th>\n",
       "      <th>accuracy D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lasso/logistic</th>\n",
       "      <td>7726.585781</td>\n",
       "      <td>1159.322663</td>\n",
       "      <td>5454.313361</td>\n",
       "      <td>9998.858202</td>\n",
       "      <td>54060.702446</td>\n",
       "      <td>0.444041</td>\n",
       "      <td>0.687948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random forest</th>\n",
       "      <td>7742.468553</td>\n",
       "      <td>1149.265905</td>\n",
       "      <td>5489.907380</td>\n",
       "      <td>9995.029726</td>\n",
       "      <td>55769.733587</td>\n",
       "      <td>0.444408</td>\n",
       "      <td>0.689057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>decision tree</th>\n",
       "      <td>7841.494336</td>\n",
       "      <td>1255.483231</td>\n",
       "      <td>5380.747203</td>\n",
       "      <td>10302.241469</td>\n",
       "      <td>60491.461599</td>\n",
       "      <td>0.446437</td>\n",
       "      <td>0.688048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>boosted forest</th>\n",
       "      <td>8564.372729</td>\n",
       "      <td>1144.760614</td>\n",
       "      <td>6320.641926</td>\n",
       "      <td>10808.103532</td>\n",
       "      <td>56222.278647</td>\n",
       "      <td>0.443718</td>\n",
       "      <td>0.688754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>automl (semi-cfit)</th>\n",
       "      <td>8129.709678</td>\n",
       "      <td>1132.995563</td>\n",
       "      <td>5909.038374</td>\n",
       "      <td>10350.380982</td>\n",
       "      <td>55338.022903</td>\n",
       "      <td>0.443595</td>\n",
       "      <td>0.690973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stacked (semi-cfit)</th>\n",
       "      <td>7631.637137</td>\n",
       "      <td>1129.338246</td>\n",
       "      <td>5418.134175</td>\n",
       "      <td>9845.140100</td>\n",
       "      <td>53711.207460</td>\n",
       "      <td>0.442716</td>\n",
       "      <td>0.689864</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        estimate       stderr        lower         upper  \\\n",
       "lasso/logistic       7726.585781  1159.322663  5454.313361   9998.858202   \n",
       "random forest        7742.468553  1149.265905  5489.907380   9995.029726   \n",
       "decision tree        7841.494336  1255.483231  5380.747203  10302.241469   \n",
       "boosted forest       8564.372729  1144.760614  6320.641926  10808.103532   \n",
       "automl (semi-cfit)   8129.709678  1132.995563  5909.038374  10350.380982   \n",
       "stacked (semi-cfit)  7631.637137  1129.338246  5418.134175   9845.140100   \n",
       "\n",
       "                           rmse y    rmse D  accuracy D  \n",
       "lasso/logistic       54060.702446  0.444041    0.687948  \n",
       "random forest        55769.733587  0.444408    0.689057  \n",
       "decision tree        60491.461599  0.446437    0.688048  \n",
       "boosted forest       56222.278647  0.443718    0.688754  \n",
       "automl (semi-cfit)   55338.022903  0.443595    0.690973  \n",
       "stacked (semi-cfit)  53711.207460  0.442716    0.689864  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dr(X, D, y, modely0, modely1, modeld, *, trimming=0.01, nfolds):\n",
    "    '''\n",
    "    DML for the Interactive Regression Model setting (Doubly Robust Learning)\n",
    "    with cross-fitting\n",
    "\n",
    "    Input\n",
    "    -----\n",
    "    X: the controls\n",
    "    D: the treatment\n",
    "    y: the outcome\n",
    "    modely0: the ML model for predicting the outcome y in the control population\n",
    "    modely1: the ML model for predicting the outcome y in the treated population\n",
    "    modeld: the ML model for predicting the treatment D\n",
    "    trimming: threshold below which to trim propensities\n",
    "    nfolds: the number of folds in cross-fitting\n",
    "\n",
    "    Output\n",
    "    ------\n",
    "    point: the point estimate of the treatment effect of D on y\n",
    "    stderr: the standard error of the treatment effect\n",
    "    yhat: the cross-fitted predictions for the outcome y\n",
    "    Dhat: the cross-fitted predictions for the outcome D\n",
    "    resy: the outcome residuals\n",
    "    resD: the treatment residuals\n",
    "    drhat: the doubly robust quantity for each sample\n",
    "    '''\n",
    "    cv = KFold(n_splits=nfolds, shuffle=True, random_state=123)\n",
    "    yhat0, yhat1 = np.zeros(y.shape), np.zeros(y.shape)\n",
    "    # we will fit a model E[Y| D, X] by fitting a separate model for D==0\n",
    "    # and a separate model for D==1.\n",
    "    for train, test in cv.split(X, y):\n",
    "        # train a model on training data that received treatment zero and predict on all data in test set\n",
    "        yhat0[test] = clone(modely0).fit(X.iloc[train][D[train] == 0], y[train][D[train] == 0]).predict(X.iloc[test])\n",
    "        # train a model on training data that received treatment one and predict on all data in test set\n",
    "        yhat1[test] = clone(modely1).fit(X.iloc[train][D[train] == 1], y[train][D[train] == 1]).predict(X.iloc[test])\n",
    "    # prediction for observed treatment\n",
    "    yhat = yhat0 * (1 - D) + yhat1 * D\n",
    "    # propensity scores\n",
    "    Dhat = cross_val_predict(modeld, X, D, cv=cv, method='predict_proba', n_jobs=-1)[:, 1]\n",
    "    Dhat = np.clip(Dhat, trimming, 1 - trimming)\n",
    "    # doubly robust quantity for every sample\n",
    "    drhat = yhat1 - yhat0 + (y - yhat) * (D / Dhat - (1 - D) / (1 - Dhat))\n",
    "    point = np.mean(drhat)\n",
    "    var = np.var(drhat)\n",
    "    stderr = np.sqrt(var / X.shape[0])\n",
    "    return point, stderr, yhat, Dhat, y - yhat, D - Dhat, drhat\n",
    "\n",
    "v = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "lassoytest = make_pipeline(transformer, StandardScaler(), LassoCV(cv=cv))\n",
    "lgrdtest = make_pipeline(transformer, StandardScaler(), LogisticRegressionCV(cv=cv))\n",
    "result = dr(X, D, y, lassoytest, lassoytest, lgrdtest, nfolds=5)\n",
    "seed_estimates = summary(*result, X, D, y, name='lasso/logistic')\n",
    "\n",
    "for i in range(9):\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    lassoytest = make_pipeline(transformer, StandardScaler(), LassoCV(cv=cv))\n",
    "    lgrdtest = make_pipeline(transformer, StandardScaler(), LogisticRegressionCV(cv=cv))\n",
    "    result = dr(X, D, y, lassoytest, lassoytest, lgrdtest, nfolds=5)\n",
    "    seed_estimates = pd.concat([seed_estimates, summary(*result, X, D, y, name='lasso/logistic')])\n",
    "\n",
    "med_theta = np.median(seed_estimates.values[:, 0])\n",
    "se_med = np.sqrt(np.median((seed_estimates.values[:, 1])**2 + (seed_estimates.values[:, 0] - med_theta)**2))\n",
    "tabledr = pd.DataFrame({'estimate': med_theta,\n",
    "                        'stderr': se_med,\n",
    "                        'lower': med_theta - 1.96 * se_med,\n",
    "                        'upper': med_theta + 1.96 * se_med,\n",
    "                        'rmse y': np.median(seed_estimates.values[:, 4]),\n",
    "                        'rmse D': np.median(seed_estimates.values[:, 5]),\n",
    "                        'accuracy D': np.median(seed_estimates.values[:, 6]),\n",
    "                        }, index=['lasso/logistic'])\n",
    "\n",
    "rfy = make_pipeline(transformer, RandomForestRegressor(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001))\n",
    "rfd = make_pipeline(transformer, RandomForestClassifier(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001))\n",
    "result = dr(X, D, y, rfy, rfy, rfd, nfolds=5)\n",
    "tabledr = pd.concat([tabledr, summary(*result, X, D, y, name='random forest')])\n",
    "dtry = make_pipeline(transformer, DecisionTreeRegressor(min_samples_leaf=10, ccp_alpha=.001))\n",
    "dtrd = make_pipeline(transformer, DecisionTreeClassifier(min_samples_leaf=10, ccp_alpha=.001))\n",
    "result = dr(X, D, y, dtry, dtry, dtrd, nfolds=5)\n",
    "tabledr = pd.concat([tabledr, summary(*result, X, D, y, name='decision tree')])\n",
    "gbfy = make_pipeline(transformer, GradientBoostingRegressor(max_depth=2, n_iter_no_change=5))\n",
    "gbfd = make_pipeline(transformer, GradientBoostingClassifier(max_depth=2, n_iter_no_change=5))\n",
    "result = dr(X, D, y, gbfy, gbfy, gbfd, nfolds=5)\n",
    "tabledr = pd.concat([tabledr, summary(*result, X, D, y, name='boosted forest')])\n",
    "# semi cross-fitting\n",
    "flamly0 = make_pipeline(transformer, AutoML(time_budget=60, task='regression', early_stop=True,\n",
    "                                            eval_method='cv', n_splits=3, metric='r2', verbose=0))\n",
    "flamly1 = make_pipeline(transformer, AutoML(time_budget=60, task='regression', early_stop=True,\n",
    "                                            eval_method='cv', n_splits=3, metric='r2', verbose=0))\n",
    "flamld = make_pipeline(transformer, AutoML(time_budget=60, task='classification', early_stop=True,\n",
    "                                           eval_method='cv', n_splits=3, metric='r2', verbose=0))\n",
    "\n",
    "flamly0.fit(X[D == 0], y[D == 0])\n",
    "besty0 = make_pipeline(transformer, clone(flamly0[-1].best_model_for_estimator(flamly0[-1].best_estimator)))\n",
    "flamly1.fit(X[D == 1], y[D == 1])\n",
    "besty1 = make_pipeline(transformer, clone(flamly1[-1].best_model_for_estimator(flamly1[-1].best_estimator)))\n",
    "flamld.fit(X, D)\n",
    "bestd = make_pipeline(transformer, clone(flamld[-1].best_model_for_estimator(flamld[-1].best_estimator)))\n",
    "result = dr(X, D, y, besty0, besty1, bestd, nfolds=5)\n",
    "tabledr = pd.concat([tabledr, summary(*result, X, D, y, name='automl (semi-cfit)')])\n",
    "def dr_dirty(X, D, y, modely0_list, modely1_list, modeld_list, *,\n",
    "             stacker=LinearRegression(), trimming=0.01, nfolds):\n",
    "    '''\n",
    "    DML for the Interactive Regression Model setting (Doubly Robust Learning)\n",
    "    with cross-fitting\n",
    "\n",
    "    Input\n",
    "    -----\n",
    "    X: the controls\n",
    "    D: the treatment\n",
    "    y: the outcome\n",
    "    modely_list: list of ML models for predicting the outcome y\n",
    "    modeld_list: list of ML models for predicting the treatment D\n",
    "    stacker: model used to aggregate predictions of each of the base models\n",
    "    trimming: threshold below which to trim propensities\n",
    "    nfolds: the number of folds in cross-fitting\n",
    "\n",
    "    Output\n",
    "    ------\n",
    "    point: the point estimate of the treatment effect of D on y\n",
    "    stderr: the standard error of the treatment effect\n",
    "    yhat: the cross-fitted predictions for the outcome y\n",
    "    Dhat: the cross-fitted predictions for the outcome D\n",
    "    resy: the outcome residuals\n",
    "    resD: the treatment residuals\n",
    "    drhat: the doubly robust quantity for each sample\n",
    "    '''\n",
    "    cv = KFold(n_splits=nfolds, shuffle=True, random_state=123)\n",
    "\n",
    "    # we will fit a model E[Y| D, X] by fitting a separate model for D==0\n",
    "    # and a separate model for D==1. We do that for each model type in modely_list\n",
    "    yhats0, yhats1 = np.zeros((y.shape[0], len(modely0_list))), np.zeros((y.shape[0], len(modely1_list)))\n",
    "    for train, test in cv.split(X, y):\n",
    "        for it, modely0 in enumerate(modely0_list):\n",
    "            mdl = clone(modely0).fit(X.iloc[train][D[train] == 0], y[train][D[train] == 0])\n",
    "            yhats0[test, it] = mdl.predict(X.iloc[test])\n",
    "        for it, modely1 in enumerate(modely1_list):\n",
    "            mdl = clone(modely1).fit(X.iloc[train][D[train] == 1], y[train][D[train] == 1])\n",
    "            yhats1[test, it] = mdl.predict(X.iloc[test])\n",
    "\n",
    "    # calculate stacking weights for the outcome model for each population\n",
    "    # and combine the outcome model predictions\n",
    "    yhat0 = clone(stacker).fit(yhats0[D == 0], y[D == 0]).predict(yhats0)\n",
    "    yhat1 = clone(stacker).fit(yhats1[D == 1], y[D == 1]).predict(yhats1)\n",
    "\n",
    "    # prediction for observed treatment using the stacked model\n",
    "    yhat = yhat0 * (1 - D) + yhat1 * D\n",
    "\n",
    "    # propensity scores\n",
    "    Dhats = np.array([cross_val_predict(modeld, X, D, cv=cv, method='predict_proba', n_jobs=-1)[:, 1]\n",
    "                     for modeld in modeld_list]).T\n",
    "    # construct coefficients on each model based on stacker\n",
    "    Dhat = clone(stacker).fit(Dhats, D).predict(Dhats)\n",
    "    # trim propensities\n",
    "    Dhat = np.clip(Dhat, trimming, 1 - trimming)\n",
    "\n",
    "    # doubly robust quantity for every sample\n",
    "    drhat = yhat1 - yhat0 + (y - yhat) * (D / Dhat - (1 - D) / (1 - Dhat))\n",
    "    point = np.mean(drhat)\n",
    "    var = np.var(drhat)\n",
    "    stderr = np.sqrt(var / X.shape[0])\n",
    "    return point, stderr, yhat, Dhat, y - yhat, D - Dhat, drhat\n",
    "\n",
    "result = dr_dirty(X, D, y, [lassoy, rfy, dtry, gbfy], [lassoy, rfy, dtry, gbfy], [lgrd, rfd, dtrd, gbfd], nfolds=5)\n",
    "tabledr = pd.concat([tabledr, summary(*result, X, D, y, name='stacked (semi-cfit)')])\n",
    "tabledr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Doubly Robust Results: Bottom 25% Income ===\n",
      "                        estimate       stderr        lower         upper  \\\n",
      "lasso/logistic       4168.663113  1055.478209  2099.925824   6237.400403   \n",
      "random forest        3852.236539  1048.061988  1798.035042   5906.438037   \n",
      "decision tree        6012.705255  2957.904673   215.212096  11810.198414   \n",
      "boosted forest       4947.769425  1093.701876  2804.113747   7091.425103   \n",
      "automl (semi-cfit)   4513.585430  1017.811332  2518.675220   6508.495641   \n",
      "stacked (semi-cfit)  3685.377281  1029.381455  1667.789630   5702.964932   \n",
      "\n",
      "                           rmse y    rmse D  accuracy D  \n",
      "lasso/logistic       13391.064772  0.357524    0.841001  \n",
      "random forest        13472.696995  0.345880    0.845440  \n",
      "decision tree        14583.602725  0.369917    0.810331  \n",
      "boosted forest       13492.815551  0.345680    0.843019  \n",
      "automl (semi-cfit)   13170.419927  0.346929    0.839790  \n",
      "stacked (semi-cfit)  13263.278873  0.343343    0.845036  \n",
      "\n",
      "=== Doubly Robust Results: Top 25% Income ===\n",
      "                         estimate        stderr         lower         upper  \\\n",
      "lasso/logistic       17259.513464   3916.988487   9582.216029  24936.810898   \n",
      "random forest        16165.835175   4083.132189   8162.896085  24168.774265   \n",
      "decision tree        47366.460835  26509.764120  -4592.676841  99325.598510   \n",
      "boosted forest       16130.990506   3839.700938   8605.176668  23656.804344   \n",
      "automl (semi-cfit)   17037.483404   3747.395234   9692.588744  24382.378063   \n",
      "stacked (semi-cfit)  18749.430337   3759.410647  11380.985469  26117.875205   \n",
      "\n",
      "                            rmse y    rmse D  accuracy D  \n",
      "lasso/logistic        91534.931249  0.482861    0.603066  \n",
      "random forest         96355.329560  0.485187    0.607503  \n",
      "decision tree        103739.400617  0.542458    0.539734  \n",
      "boosted forest        94838.460953  0.484382    0.600645  \n",
      "automl (semi-cfit)    94421.196487  0.484810    0.599435  \n",
      "stacked (semi-cfit)   90675.306585  0.481981    0.606696  \n"
     ]
    }
   ],
   "source": [
    "data_bottom = data.query('inc < inc.quantile(.25)').copy()\n",
    "data_top    = data.query('inc > inc.quantile(.75)').copy()\n",
    "\n",
    "def extract_XDy(df):\n",
    "    \"\"\"Given a subset of the 401k data, produce X, D, and y \n",
    "    consistent with the main DR analysis.\"\"\"\n",
    "    y_ = df['net_tfa'].values\n",
    "    D_ = df['e401'].values\n",
    "    X_ = df.drop([\n",
    "        'e401', 'p401', 'a401', 'tw', 'tfa', 'net_tfa', 'tfa_he',\n",
    "        'hval', 'hmort', 'hequity', 'nifa', 'net_nifa', 'net_n401',\n",
    "        'ira', 'dum91', 'icat', 'ecat', 'zhat', 'i1', 'i2', 'i3',\n",
    "        'i4', 'i5', 'i6', 'i7', 'a1', 'a2', 'a3', 'a4', 'a5'\n",
    "    ], axis=1)\n",
    "    return X_, D_, y_\n",
    "\n",
    "X_bottom, D_bottom, y_bottom = extract_XDy(data_bottom)\n",
    "X_top, D_top, y_top          = extract_XDy(data_top)\n",
    "\n",
    "def dr(X, D, y, modely0, modely1, modeld, *, trimming=0.01, nfolds=5):\n",
    "    '''\n",
    "    DML for the Interactive Regression Model setting (Doubly Robust Learning)\n",
    "    with cross-fitting\n",
    "    '''\n",
    "    cv = KFold(n_splits=nfolds, shuffle=True, random_state=123)\n",
    "    yhat0, yhat1 = np.zeros(y.shape), np.zeros(y.shape)\n",
    "    for train, test in cv.split(X, y):\n",
    "        # Fit E[Y|D=0,X]\n",
    "        mdl0 = clone(modely0).fit(X.iloc[train][D[train] == 0], y[train][D[train] == 0])\n",
    "        yhat0[test] = mdl0.predict(X.iloc[test])\n",
    "        # Fit E[Y|D=1,X]\n",
    "        mdl1 = clone(modely1).fit(X.iloc[train][D[train] == 1], y[train][D[train] == 1])\n",
    "        yhat1[test] = mdl1.predict(X.iloc[test])\n",
    "\n",
    "    # Combine to get E[Y|D,X] predictions for the observed D\n",
    "    yhat = yhat0 * (1 - D) + yhat1 * D\n",
    "\n",
    "    # Propensity scores\n",
    "    Dhat = cross_val_predict(modeld, X, D, cv=cv, method='predict_proba', n_jobs=-1)[:, 1]\n",
    "    Dhat = np.clip(Dhat, trimming, 1 - trimming)\n",
    "\n",
    "    # Doubly robust quantity for each sample\n",
    "    drhat = yhat1 - yhat0 + (y - yhat) * (D / Dhat - (1 - D) / (1 - Dhat))\n",
    "    point = np.mean(drhat)\n",
    "    var   = np.var(drhat)\n",
    "    stderr= np.sqrt(var / X.shape[0])\n",
    "\n",
    "    # Return:\n",
    "    #   - point: the point estimate\n",
    "    #   - stderr: standard error\n",
    "    #   - yhat: cross-fitted predictions for the observed D\n",
    "    #   - Dhat: cross-fitted propensities\n",
    "    #   - (y - yhat): outcome residual\n",
    "    #   - (D - Dhat): treatment residual\n",
    "    #   - drhat: doubly robust terms\n",
    "    return point, stderr, yhat, Dhat, (y - yhat), (D - Dhat), drhat\n",
    "\n",
    "def dr_dirty(\n",
    "    X, D, y,\n",
    "    modely0_list, modely1_list, modeld_list,\n",
    "    stacker=LinearRegression(), trimming=0.01, nfolds=5\n",
    "):\n",
    "    '''\n",
    "    DML for the Interactive Regression Model setting (Doubly Robust Learning)\n",
    "    with cross-fitting and stacking\n",
    "    '''\n",
    "    cv = KFold(n_splits=nfolds, shuffle=True, random_state=123)\n",
    "\n",
    "    # Prepare space for multiple model predictions\n",
    "    yhats0 = np.zeros((X.shape[0], len(modely0_list)))\n",
    "    yhats1 = np.zeros((X.shape[0], len(modely1_list)))\n",
    "\n",
    "    # Cross-fitting: fit each base model E[Y|D=0,X], E[Y|D=1,X]\n",
    "    for train, test in cv.split(X, y):\n",
    "        for i_m0, m0 in enumerate(modely0_list):\n",
    "            m0_cl = clone(m0).fit(X.iloc[train][D[train] == 0], y[train][D[train] == 0])\n",
    "            yhats0[test, i_m0] = m0_cl.predict(X.iloc[test])\n",
    "        for i_m1, m1 in enumerate(modely1_list):\n",
    "            m1_cl = clone(m1).fit(X.iloc[train][D[train] == 1], y[train][D[train] == 1])\n",
    "            yhats1[test, i_m1] = m1_cl.predict(X.iloc[test])\n",
    "\n",
    "    # Stack them for D=0 predictions\n",
    "    yhat0 = clone(stacker).fit(yhats0[D == 0], y[D == 0]).predict(yhats0)\n",
    "    # Stack them for D=1 predictions\n",
    "    yhat1 = clone(stacker).fit(yhats1[D == 1], y[D == 1]).predict(yhats1)\n",
    "    # Combined model prediction for Y\n",
    "    yhat = yhat0 * (1 - D) + yhat1 * D\n",
    "\n",
    "    # Now do the same for propensity models\n",
    "    Dhats_list = []\n",
    "    for md in modeld_list:\n",
    "        Dhats_list.append(cross_val_predict(md, X, D, cv=cv, method='predict_proba', n_jobs=-1)[:, 1])\n",
    "    Dhats_arr = np.vstack(Dhats_list).T  # shape: (n_samples, n_models)\n",
    "\n",
    "    # Stack the predicted propensity\n",
    "    Dhat = clone(stacker).fit(Dhats_arr, D).predict(Dhats_arr)\n",
    "    Dhat = np.clip(Dhat, trimming, 1 - trimming)\n",
    "\n",
    "    # Doubly robust quantity\n",
    "    drhat = yhat1 - yhat0 + (y - yhat) * (D / Dhat - (1 - D) / (1 - Dhat))\n",
    "    point = np.mean(drhat)\n",
    "    var   = np.var(drhat)\n",
    "    stderr= np.sqrt(var / X.shape[0])\n",
    "\n",
    "    return point, stderr, yhat, Dhat, (y - yhat), (D - Dhat), drhat\n",
    "\n",
    "\n",
    "def summary(point, stderr, yhat, Dhat, resy, resD, drhat, X, D, y, *, name=''):\n",
    "    return pd.DataFrame({\n",
    "        'estimate': point,  # point estimate\n",
    "        'stderr': stderr,   # standard error\n",
    "        'lower': point - 1.96 * stderr,\n",
    "        'upper': point + 1.96 * stderr,\n",
    "        'rmse y': np.sqrt(np.mean(resy**2)),\n",
    "        'rmse D': np.sqrt(np.mean(resD**2)),\n",
    "        # classification accuracy for D, if it's binary:\n",
    "        'accuracy D': np.mean(np.abs(resD) < 0.5),\n",
    "    }, index=[name])\n",
    "\n",
    "\n",
    "def run_dr_analysis(X, D, y):\n",
    "    \"\"\"\n",
    "    Replicates the DR analysis shown in the original code:\n",
    "      - lasso/logistic with repeated seeds, then median-based estimate\n",
    "      - random forest\n",
    "      - decision tree\n",
    "      - boosted forest\n",
    "      - automl (semi-cfit)\n",
    "      - stacking\n",
    "    Returns a single table of results.\n",
    "    \"\"\"\n",
    "    # -- 1) Lasso/logistic repeated over seeds, then median-based estimate\n",
    "    seed_estimates = None\n",
    "\n",
    "    # We'll define a base 5-fold for the model pipelines:\n",
    "    cv_5fold = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "\n",
    "    # One example run with seed=123\n",
    "    lassoy = make_pipeline(transformer, StandardScaler(), LassoCV(cv=cv_5fold))\n",
    "    lgrd   = make_pipeline(transformer, StandardScaler(), LogisticRegressionCV(cv=cv_5fold))\n",
    "\n",
    "    # DR for single seed first\n",
    "    result = dr(X, D, y, lassoy, lassoy, lgrd, nfolds=5)\n",
    "    seed_estimates = summary(*result, X, D, y, name='lasso/logistic')\n",
    "\n",
    "    # Loop over multiple seeds\n",
    "    for i in range(9):\n",
    "        cv_seed = KFold(n_splits=5, shuffle=True, random_state=i)\n",
    "        lassoy_i = make_pipeline(transformer, StandardScaler(), LassoCV(cv=cv_seed))\n",
    "        lgrd_i   = make_pipeline(transformer, StandardScaler(), LogisticRegressionCV(cv=cv_seed))\n",
    "\n",
    "        result_i = dr(X, D, y, lassoy_i, lassoy_i, lgrd_i, nfolds=5)\n",
    "        seed_estimates = pd.concat([seed_estimates, summary(*result_i, X, D, y, name='lasso/logistic')])\n",
    "\n",
    "    # Compute median-based point estimate and standard error\n",
    "    med_theta  = np.median(seed_estimates['estimate'].values)\n",
    "    # sqrt(median(var_i) + var( point_i ))\n",
    "    # but the original code basically does:\n",
    "    se_med     = np.sqrt(\n",
    "        np.median(seed_estimates['stderr'].values ** 2)\n",
    "        + np.median((seed_estimates['estimate'].values - med_theta) ** 2)\n",
    "    )\n",
    "    tabledr = pd.DataFrame({\n",
    "        'estimate': med_theta,\n",
    "        'stderr':   se_med,\n",
    "        'lower':    med_theta - 1.96 * se_med,\n",
    "        'upper':    med_theta + 1.96 * se_med,\n",
    "        'rmse y':   np.median(seed_estimates['rmse y'].values),\n",
    "        'rmse D':   np.median(seed_estimates['rmse D'].values),\n",
    "        'accuracy D': np.median(seed_estimates['accuracy D'].values)\n",
    "    }, index=['lasso/logistic'])\n",
    "\n",
    "    # -- 2) Random Forest\n",
    "    rfy = make_pipeline(transformer, \n",
    "                        RandomForestRegressor(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001))\n",
    "    rfd = make_pipeline(transformer, \n",
    "                        RandomForestClassifier(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001))\n",
    "    result = dr(X, D, y, rfy, rfy, rfd, nfolds=5)\n",
    "    tabledr = pd.concat([tabledr, summary(*result, X, D, y, name='random forest')])\n",
    "\n",
    "    # -- 3) Decision Tree\n",
    "    dtry = make_pipeline(transformer,\n",
    "                         DecisionTreeRegressor(min_samples_leaf=10, ccp_alpha=.001))\n",
    "    dtrd = make_pipeline(transformer,\n",
    "                         DecisionTreeClassifier(min_samples_leaf=10, ccp_alpha=.001))\n",
    "    result = dr(X, D, y, dtry, dtry, dtrd, nfolds=5)\n",
    "    tabledr = pd.concat([tabledr, summary(*result, X, D, y, name='decision tree')])\n",
    "\n",
    "    # -- 4) Boosted Forest\n",
    "    gbfy = make_pipeline(transformer, \n",
    "                         GradientBoostingRegressor(max_depth=2, n_iter_no_change=5))\n",
    "    gbfd = make_pipeline(transformer, \n",
    "                         GradientBoostingClassifier(max_depth=2, n_iter_no_change=5))\n",
    "    result = dr(X, D, y, gbfy, gbfy, gbfd, nfolds=5)\n",
    "    tabledr = pd.concat([tabledr, summary(*result, X, D, y, name='boosted forest')])\n",
    "\n",
    "    # -- 5) AutoML (semi cross-fitting)\n",
    "    flamly0 = make_pipeline(transformer,\n",
    "                            AutoML(time_budget=60, task='regression', early_stop=True,\n",
    "                                   eval_method='cv', n_splits=3, metric='r2', verbose=0))\n",
    "    flamly1 = make_pipeline(transformer,\n",
    "                            AutoML(time_budget=60, task='regression', early_stop=True,\n",
    "                                   eval_method='cv', n_splits=3, metric='r2', verbose=0))\n",
    "    flamld  = make_pipeline(transformer,\n",
    "                            AutoML(time_budget=60, task='classification', early_stop=True,\n",
    "                                   eval_method='cv', n_splits=3, metric='r2', verbose=0))\n",
    "\n",
    "    # Fit Y|D=0, Y|D=1 on subsets\n",
    "    flamly0.fit(X[D == 0], y[D == 0])\n",
    "    besty0_model = flamly0[-1].best_model_for_estimator(flamly0[-1].best_estimator)\n",
    "    besty0 = make_pipeline(transformer, clone(besty0_model))\n",
    "\n",
    "    flamly1.fit(X[D == 1], y[D == 1])\n",
    "    besty1_model = flamly1[-1].best_model_for_estimator(flamly1[-1].best_estimator)\n",
    "    besty1 = make_pipeline(transformer, clone(besty1_model))\n",
    "\n",
    "    # Fit propensities on the full sample\n",
    "    flamld.fit(X, D)\n",
    "    bestd_model = flamld[-1].best_model_for_estimator(flamld[-1].best_estimator)\n",
    "    bestd = make_pipeline(transformer, clone(bestd_model))\n",
    "\n",
    "    result = dr(X, D, y, besty0, besty1, bestd, nfolds=5)\n",
    "    tabledr = pd.concat([tabledr, summary(*result, X, D, y, name='automl (semi-cfit)')])\n",
    "\n",
    "    # -- 6) Stacked (semi-cfit)\n",
    "    # Prepare lists of base models for Y|D=0, Y|D=1\n",
    "    lassoy_ = make_pipeline(transformer, StandardScaler(), LassoCV(cv=cv_5fold))\n",
    "    rfy_    = make_pipeline(transformer, \n",
    "                            RandomForestRegressor(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001))\n",
    "    dtry_   = make_pipeline(transformer, \n",
    "                            DecisionTreeRegressor(min_samples_leaf=10, ccp_alpha=.001))\n",
    "    gbfy_   = make_pipeline(transformer, \n",
    "                            GradientBoostingRegressor(max_depth=2, n_iter_no_change=5))\n",
    "\n",
    "    modely0_list = [lassoy_, rfy_, dtry_, gbfy_]  # for D=0\n",
    "    modely1_list = [lassoy_, rfy_, dtry_, gbfy_]  # for D=1\n",
    "\n",
    "    lgrd_   = make_pipeline(transformer, StandardScaler(), LogisticRegressionCV(cv=cv_5fold))\n",
    "    rfd_    = make_pipeline(transformer, \n",
    "                            RandomForestClassifier(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001))\n",
    "    dtrd_   = make_pipeline(transformer, \n",
    "                            DecisionTreeClassifier(min_samples_leaf=10, ccp_alpha=.001))\n",
    "    gbfd_   = make_pipeline(transformer, \n",
    "                            GradientBoostingClassifier(max_depth=2, n_iter_no_change=5))\n",
    "    modeld_list = [lgrd_, rfd_, dtrd_, gbfd_]\n",
    "\n",
    "    result = dr_dirty(X, D, y, modely0_list, modely1_list, modeld_list, stacker=LinearRegression(), nfolds=5)\n",
    "    tabledr = pd.concat([tabledr, summary(*result, X, D, y, name='stacked (semi-cfit)')])\n",
    "\n",
    "    return tabledr\n",
    "\n",
    "table_dr_bottom = run_dr_analysis(X_bottom, D_bottom, y_bottom)\n",
    "table_dr_top    = run_dr_analysis(X_top,    D_top,    y_top   )\n",
    "\n",
    "print(\"=== Doubly Robust Results: Bottom 25% Income ===\")\n",
    "print(table_dr_bottom)\n",
    "\n",
    "print(\"\\n=== Doubly Robust Results: Top 25% Income ===\")\n",
    "print(table_dr_top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the IRM setting, we again see heterogeneity in the treatment effect with respect to income, with the bottom 25% of earners seeing estimates around 4.5k and the top 25% seeing estimates of around 17k. The different machine learning models are broadly consistent across all three income groups with the exception of the decision tree, which has far higher estimates and standard errors than other methods.  \n",
    "  \n",
    "The IRM and PLR results differ slightly in terms of the estimate, but both find heterogeneity and are broadly in the same range of values.\n",
    "\n",
    "Now we move to implemetting semi-crossfitting with best model selection, first for PLR and then for IRM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>estimate</th>\n",
       "      <th>stderr</th>\n",
       "      <th>lower</th>\n",
       "      <th>upper</th>\n",
       "      <th>rmse y</th>\n",
       "      <th>rmse D</th>\n",
       "      <th>accuracy D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>select-best (semi-cfit) PLR</th>\n",
       "      <td>8895.589396</td>\n",
       "      <td>1312.367242</td>\n",
       "      <td>6323.349602</td>\n",
       "      <td>11467.829190</td>\n",
       "      <td>54254.468883</td>\n",
       "      <td>0.443719</td>\n",
       "      <td>0.688553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>select-best (semi-cfit) PLR (bottom 25%)</th>\n",
       "      <td>3719.860298</td>\n",
       "      <td>1103.768825</td>\n",
       "      <td>1556.473401</td>\n",
       "      <td>5883.247195</td>\n",
       "      <td>13410.597432</td>\n",
       "      <td>0.345058</td>\n",
       "      <td>0.845036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>select-best (semi-cfit) PLR (top 25%)</th>\n",
       "      <td>18204.194590</td>\n",
       "      <td>3867.405524</td>\n",
       "      <td>10624.079763</td>\n",
       "      <td>25784.309418</td>\n",
       "      <td>91393.039963</td>\n",
       "      <td>0.482708</td>\n",
       "      <td>0.601049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              estimate       stderr  \\\n",
       "select-best (semi-cfit) PLR                8895.589396  1312.367242   \n",
       "select-best (semi-cfit) PLR (bottom 25%)   3719.860298  1103.768825   \n",
       "select-best (semi-cfit) PLR (top 25%)     18204.194590  3867.405524   \n",
       "\n",
       "                                                 lower         upper  \\\n",
       "select-best (semi-cfit) PLR                6323.349602  11467.829190   \n",
       "select-best (semi-cfit) PLR (bottom 25%)   1556.473401   5883.247195   \n",
       "select-best (semi-cfit) PLR (top 25%)     10624.079763  25784.309418   \n",
       "\n",
       "                                                rmse y    rmse D  accuracy D  \n",
       "select-best (semi-cfit) PLR               54254.468883  0.443719    0.688553  \n",
       "select-best (semi-cfit) PLR (bottom 25%)  13410.597432  0.345058    0.845036  \n",
       "select-best (semi-cfit) PLR (top 25%)     91393.039963  0.482708    0.601049  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# b.)\n",
    "from sklearn.model_selection import cross_val_predict, KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from copy import deepcopy\n",
    "\n",
    "def get_oof_and_mse(model, X, y, cv, classifier=False):\n",
    "    \"\"\"\n",
    "    Returns out-of-fold predictions and the MSE (or 'regression MSE' if classifier=True).\n",
    "    If classifier=True, we use model.predict_proba(...).\n",
    "    \"\"\"\n",
    "    if classifier:\n",
    "        # For binary D, treat the problem as a regression on [0,1], \n",
    "        # so we measure MSE on predicted probabilities\n",
    "        preds = cross_val_predict(model, X, y, cv=cv, method='predict_proba')[:, 1]\n",
    "    else:\n",
    "        preds = cross_val_predict(model, X, y, cv=cv)\n",
    "    mse_val = mean_squared_error(y, preds)\n",
    "    return preds, mse_val\n",
    "\n",
    "\n",
    "def dml_select_best(X, D, y, modely_list, modeld_list, *, nfolds=5, classifier=False, trimming=0.01):\n",
    "    \"\"\"\n",
    "    Semi-cross-fitting for the Partially Linear Model (PLR) by selecting\n",
    "    the single best model for Y and single best model for D among user-provided lists.\n",
    "\n",
    "    Steps:\n",
    "      1) For each candidate in modely_list, get OOF predictions vs. y. Pick the best by MSE.\n",
    "      2) For each candidate in modeld_list, get OOF predictions vs. D. Pick the best by MSE.\n",
    "         If classifier=True, each model in modeld_list is treated as a classifier,\n",
    "         but we still measure MSE on the predicted probability vs. the true D.\n",
    "      3) Use the chosen best model's OOF predictions for yhat and Dhat.\n",
    "      4) Compute partial linear estimate as in the usual DML formula:\n",
    "            theta = E[(y - yhat)*(D - Dhat)] / E[(D - Dhat)^2]\n",
    "         and the standard error formula.\n",
    "\n",
    "    Returns:\n",
    "      point, stderr, yhat, Dhat, resy, resD, epsilon\n",
    "    \"\"\"\n",
    "    cv = KFold(n_splits=nfolds, shuffle=True, random_state=123)\n",
    "\n",
    "    # --- 1) Select best model for y\n",
    "    best_mse_y = np.inf\n",
    "    best_preds_y = None\n",
    "    best_model_y = None\n",
    "\n",
    "    for candidate in modely_list:\n",
    "        # Clone so we don't pollute the original pipeline with partial fits\n",
    "        cand = deepcopy(candidate)\n",
    "        preds, mse_val = get_oof_and_mse(cand, X, y, cv, classifier=False)\n",
    "        if mse_val < best_mse_y:\n",
    "            best_mse_y = mse_val\n",
    "            best_preds_y = preds\n",
    "            best_model_y = cand\n",
    "\n",
    "    # --- 2) Select best model for D\n",
    "    best_mse_d = np.inf\n",
    "    best_preds_d = None\n",
    "    best_model_d = None\n",
    "\n",
    "    for candidate in modeld_list:\n",
    "        # If classifier=True, we measure MSE on predicted probabilities\n",
    "        cand = deepcopy(candidate)\n",
    "        preds, mse_val = get_oof_and_mse(cand, X, D, cv, classifier=classifier)\n",
    "        if mse_val < best_mse_d:\n",
    "            best_mse_d = mse_val\n",
    "            best_preds_d = preds\n",
    "            best_model_d = cand\n",
    "\n",
    "    # Residuals\n",
    "    resy = y - best_preds_y\n",
    "    resD = D - best_preds_d\n",
    "\n",
    "    # Final partial linear estimate\n",
    "    point = np.mean(resy * resD) / np.mean(resD**2)\n",
    "    epsilon = resy - point * resD\n",
    "\n",
    "    var = np.mean(epsilon**2 * resD**2) / (np.mean(resD**2)**2)\n",
    "    stderr = np.sqrt(var / X.shape[0])\n",
    "\n",
    "    return point, stderr, best_preds_y, best_preds_d, resy, resD, epsilon\n",
    "\n",
    "modely_list = [lassoy, rfy, dtry, gbfy]\n",
    "modeld_list = [lgrd, rfd, dtrd, gbfd]\n",
    "\n",
    "point, stderr, yhat, Dhat, resy, resD, epsilon = dml_select_best(\n",
    "    X, D, y, modely_list, modeld_list, nfolds=5, classifier=True\n",
    ")\n",
    "\n",
    "table_select = summary(\n",
    "    point, stderr, yhat, Dhat, resy, resD, epsilon, X, D, y,\n",
    "    name='select-best (semi-cfit) PLR'\n",
    ")\n",
    "point_bottom, stderr_bottom, yhat_bottom, Dhat_bottom, resy_bottom, resD_bottom, epsilon_bottom = dml_select_best(\n",
    "    X_bottom, D_bottom, y_bottom, modely_list, modeld_list, nfolds=5, classifier=True\n",
    ")\n",
    "table_select_bottom = summary(\n",
    "    point_bottom, stderr_bottom, yhat_bottom, Dhat_bottom, resy_bottom, resD_bottom, epsilon_bottom, X_bottom, D_bottom, y_bottom,\n",
    "    name='select-best (semi-cfit) PLR (bottom 25%)'\n",
    ")\n",
    "table_select = pd.concat([table_select, table_select_bottom])\n",
    "point_top, stderr_top, yhat_top, Dhat_top, resy_top, resD_top, epsilon_top = dml_select_best(\n",
    "    X_top, D_top, y_top, modely_list, modeld_list, nfolds=5, classifier=True\n",
    ")\n",
    "table_select_top = summary(\n",
    "    point_top, stderr_top, yhat_top, Dhat_top, resy_top, resD_top, epsilon_top, X_top, D_top, y_top,\n",
    "    name='select-best (semi-cfit) PLR (top 25%)'\n",
    ")\n",
    "table_select = pd.concat([table_select, table_select_top])\n",
    "table_select"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For IRM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>estimate</th>\n",
       "      <th>stderr</th>\n",
       "      <th>lower</th>\n",
       "      <th>upper</th>\n",
       "      <th>rmse y</th>\n",
       "      <th>rmse D</th>\n",
       "      <th>accuracy D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>select best IRM with semi cross fitting all samples</th>\n",
       "      <td>7698.712934</td>\n",
       "      <td>1118.641401</td>\n",
       "      <td>5506.175788</td>\n",
       "      <td>9891.250079</td>\n",
       "      <td>54026.928780</td>\n",
       "      <td>0.443592</td>\n",
       "      <td>0.688351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>select best IRM with semi cross fitting bottom 25\\% income</th>\n",
       "      <td>3887.273772</td>\n",
       "      <td>1044.770616</td>\n",
       "      <td>1839.523365</td>\n",
       "      <td>5935.024179</td>\n",
       "      <td>13394.262472</td>\n",
       "      <td>0.345016</td>\n",
       "      <td>0.845440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>select best IRM with semi cross fitting top 25\\% income</th>\n",
       "      <td>18393.721922</td>\n",
       "      <td>3820.594388</td>\n",
       "      <td>10905.356922</td>\n",
       "      <td>25882.086922</td>\n",
       "      <td>91533.229313</td>\n",
       "      <td>0.482708</td>\n",
       "      <td>0.601049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        estimate       stderr  \\\n",
       "select best IRM with semi cross fitting all sam...   7698.712934  1118.641401   \n",
       "select best IRM with semi cross fitting bottom ...   3887.273772  1044.770616   \n",
       "select best IRM with semi cross fitting top 25\\...  18393.721922  3820.594388   \n",
       "\n",
       "                                                           lower  \\\n",
       "select best IRM with semi cross fitting all sam...   5506.175788   \n",
       "select best IRM with semi cross fitting bottom ...   1839.523365   \n",
       "select best IRM with semi cross fitting top 25\\...  10905.356922   \n",
       "\n",
       "                                                           upper  \\\n",
       "select best IRM with semi cross fitting all sam...   9891.250079   \n",
       "select best IRM with semi cross fitting bottom ...   5935.024179   \n",
       "select best IRM with semi cross fitting top 25\\...  25882.086922   \n",
       "\n",
       "                                                          rmse y    rmse D  \\\n",
       "select best IRM with semi cross fitting all sam...  54026.928780  0.443592   \n",
       "select best IRM with semi cross fitting bottom ...  13394.262472  0.345016   \n",
       "select best IRM with semi cross fitting top 25\\...  91533.229313  0.482708   \n",
       "\n",
       "                                                    accuracy D  \n",
       "select best IRM with semi cross fitting all sam...    0.688351  \n",
       "select best IRM with semi cross fitting bottom ...    0.845440  \n",
       "select best IRM with semi cross fitting top 25\\...    0.601049  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_oof_and_mse_irm(model, X, y, D, which_d, cv):\n",
    "    \"\"\"\n",
    "    For IRM, we measure OOF performance only on the subset where D == which_d.\n",
    "    We do cross-fitting: train on all 'train' points that have D==which_d, \n",
    "    predict on the entire test fold, but compute MSE only for the test fold \n",
    "    members that also have D==which_d.\n",
    "    \n",
    "    Returns OOF predictions (full length, but only truly valid for D==which_d),\n",
    "    plus MSE measured on that subset.\n",
    "    \"\"\"\n",
    "    preds_full = np.zeros(len(y), dtype=float)\n",
    "    for train_idx, test_idx in cv.split(X, y):\n",
    "        # filter the training portion to only those with D==which_d\n",
    "        train_sub = train_idx[D[train_idx] == which_d]\n",
    "        # Fit on (X, y) for that sub-sample\n",
    "        model_cl = deepcopy(model).fit(X.iloc[train_sub], y[train_sub])\n",
    "        # Predict on the entire test fold\n",
    "        preds_full[test_idx] = model_cl.predict(X.iloc[test_idx])\n",
    "    # MSE only on the subset that has D==which_d\n",
    "    mse_val = mean_squared_error(y[D==which_d], preds_full[D==which_d])\n",
    "    return preds_full, mse_val\n",
    "\n",
    "def dr_select_best(X, D, y, modely0_list, modely1_list, modeld_list,\n",
    "                   trimming=0.01, nfolds=5):\n",
    "    \"\"\"\n",
    "    Doubly-Robust (IRM) with semi-cross-fitting:\n",
    "    Select single best model for Y|D=0 from modely0_list,\n",
    "    single best model for Y|D=1 from modely1_list,\n",
    "    single best model for the propensity from modeld_list (all data).\n",
    "    \n",
    "    Then run the standard cross-fitting formula to construct:\n",
    "      yhat0, yhat1, Dhat, drhat,\n",
    "    and output the usual IRM results.\n",
    "    \"\"\"\n",
    "    cv = KFold(n_splits=nfolds, shuffle=True, random_state=123)\n",
    "\n",
    "    # --- 1) find best model for Y|D=0\n",
    "    best_mse_y0 = np.inf\n",
    "    best_preds_y0 = None\n",
    "    best_model_y0 = None\n",
    "    for candidate in modely0_list:\n",
    "        preds0, mse0 = get_oof_and_mse_irm(candidate, X, y, D, which_d=0, cv=cv)\n",
    "        if mse0 < best_mse_y0:\n",
    "            best_mse_y0 = mse0\n",
    "            best_preds_y0 = preds0\n",
    "            best_model_y0 = candidate\n",
    "\n",
    "    # --- 2) find best model for Y|D=1\n",
    "    best_mse_y1 = np.inf\n",
    "    best_preds_y1 = None\n",
    "    best_model_y1 = None\n",
    "    for candidate in modely1_list:\n",
    "        preds1, mse1 = get_oof_and_mse_irm(candidate, X, y, D, which_d=1, cv=cv)\n",
    "        if mse1 < best_mse_y1:\n",
    "            best_mse_y1 = mse1\n",
    "            best_preds_y1 = preds1\n",
    "            best_model_y1 = candidate\n",
    "\n",
    "    # --- 3) find best model for D (propensity), measure MSE on entire sample\n",
    "    #         but we treat D as binary, using predicted probability\n",
    "    cv2 = KFold(n_splits=nfolds, shuffle=True, random_state=123)\n",
    "    best_mse_d = np.inf\n",
    "    best_preds_d = None\n",
    "    best_model_d = None\n",
    "    for candidate in modeld_list:\n",
    "        preds_d, mse_d = get_oof_and_mse(candidate, X, D, cv2, classifier=True)\n",
    "        if mse_d < best_mse_d:\n",
    "            best_mse_d = mse_d\n",
    "            best_preds_d = preds_d\n",
    "            best_model_d = candidate\n",
    "\n",
    "    # --- 4) IRM formula\n",
    "    # We already have cross-fitted yhat0, yhat1, Dhat = best_preds_d\n",
    "    yhat = best_preds_y0*(1 - D) + best_preds_y1*D\n",
    "    Dhat = np.clip(best_preds_d, trimming, 1 - trimming)\n",
    "\n",
    "    # DR score\n",
    "    drhat = best_preds_y1 - best_preds_y0 + (y - yhat) * (\n",
    "        D / Dhat - (1 - D)/(1 - Dhat)\n",
    "    )\n",
    "    point = np.mean(drhat)\n",
    "    var = np.var(drhat)\n",
    "    stderr = np.sqrt(var / X.shape[0])\n",
    "\n",
    "    return (\n",
    "        point,\n",
    "        stderr,\n",
    "        yhat,                # cross-fitted E[Y|D=observed, X]\n",
    "        Dhat,                # cross-fitted e(X)\n",
    "        (y - yhat),          # residual in Y space\n",
    "        (D - Dhat),          # residual in D space\n",
    "        drhat\n",
    "    )\n",
    "    \n",
    "modely0_list = [lassoy, rfy, dtry, gbfy]\n",
    "modely1_list = [lassoy, rfy, dtry, gbfy]\n",
    "modeld_list  = [lgrd,  rfd,  dtrd, gbfd]\n",
    "\n",
    "res_all = dr_select_best(\n",
    "    X, D, y, modely0_list, modely1_list, modeld_list, nfolds=5\n",
    ")\n",
    "res_bottom_25 = dr_select_best(\n",
    "    X_bottom, D_bottom, y_bottom, modely0_list, modely1_list, modeld_list, nfolds=5\n",
    ")\n",
    "res_top_25 = dr_select_best(\n",
    "    X_top, D_top, y_top, modely1_list, modely0_list, modeld_list, nfolds=5\n",
    ")\n",
    "table_all = summary(*res_all, X, D, y, name=\"select best IRM with semi cross fitting all samples\")\n",
    "table_bottom_25 = summary(*res_bottom_25, X_bottom, D_bottom, y_bottom, name=f\"select best IRM with semi cross fitting bottom 25\\% income\")\n",
    "table_top_25 = summary(*res_top_25, X_top, D_top, y_top, name=f\"select best IRM with semi cross fitting top 25\\% income\")\n",
    "\n",
    "table_final = pd.concat([table_all, table_bottom_25, table_top_25])\n",
    "table_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- Double Lasso in EconML for PLR------------\n",
      "Coefficient Results:  X is None, please call intercept_inference to learn the constant!\n",
      "                        CATE Intercept Results                        \n",
      "======================================================================\n",
      "               point_estimate  stderr  zstat pvalue ci_lower  ci_upper\n",
      "----------------------------------------------------------------------\n",
      "cate_intercept       8482.783 1380.934 6.143    0.0 5776.201 11189.365\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "<sub>A linear parametric conditional average treatment effect (CATE) model was fitted:\n",
      "$Y = \\Theta(X)\\cdot T + g(X, W) + \\epsilon$\n",
      "where for every outcome $i$ and treatment $j$ the CATE $\\Theta_{ij}(X)$ has the form:\n",
      "$\\Theta_{ij}(X) = X' coef_{ij} + cate\\_intercept_{ij}$\n",
      "Coefficient Results table portrays the $coef_{ij}$ parameter vector for each outcome $i$ and treatment $j$. Intercept Results table portrays the $cate\\_intercept_{ij}$ parameter.</sub>\n",
      "-----------------Random Forest in EconML for PLR------------\n",
      "Coefficient Results:  X is None, please call intercept_inference to learn the constant!\n",
      "                        CATE Intercept Results                        \n",
      "======================================================================\n",
      "               point_estimate  stderr  zstat pvalue ci_lower  ci_upper\n",
      "----------------------------------------------------------------------\n",
      "cate_intercept        8603.31 1333.541 6.451    0.0 5989.619 11217.002\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "<sub>A linear parametric conditional average treatment effect (CATE) model was fitted:\n",
      "$Y = \\Theta(X)\\cdot T + g(X, W) + \\epsilon$\n",
      "where for every outcome $i$ and treatment $j$ the CATE $\\Theta_{ij}(X)$ has the form:\n",
      "$\\Theta_{ij}(X) = X' coef_{ij} + cate\\_intercept_{ij}$\n",
      "Coefficient Results table portrays the $coef_{ij}$ parameter vector for each outcome $i$ and treatment $j$. Intercept Results table portrays the $cate\\_intercept_{ij}$ parameter.</sub>\n",
      "-----------------Gradient Boosting in EconML for PLR------------\n",
      "Coefficient Results:  X is None, please call intercept_inference to learn the constant!\n",
      "                        CATE Intercept Results                       \n",
      "=====================================================================\n",
      "               point_estimate  stderr zstat pvalue ci_lower  ci_upper\n",
      "---------------------------------------------------------------------\n",
      "cate_intercept       9129.167 1379.02  6.62    0.0 6426.337 11831.997\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "<sub>A linear parametric conditional average treatment effect (CATE) model was fitted:\n",
      "$Y = \\Theta(X)\\cdot T + g(X, W) + \\epsilon$\n",
      "where for every outcome $i$ and treatment $j$ the CATE $\\Theta_{ij}(X)$ has the form:\n",
      "$\\Theta_{ij}(X) = X' coef_{ij} + cate\\_intercept_{ij}$\n",
      "Coefficient Results table portrays the $coef_{ij}$ parameter vector for each outcome $i$ and treatment $j$. Intercept Results table portrays the $cate\\_intercept_{ij}$ parameter.</sub>\n",
      "-----------------Decision Tree in EconML for PLR------------\n",
      "Coefficient Results:  X is None, please call intercept_inference to learn the constant!\n",
      "                        CATE Intercept Results                        \n",
      "======================================================================\n",
      "               point_estimate  stderr  zstat pvalue ci_lower  ci_upper\n",
      "----------------------------------------------------------------------\n",
      "cate_intercept       8772.347 1448.641 6.056    0.0 5933.064 11611.631\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "<sub>A linear parametric conditional average treatment effect (CATE) model was fitted:\n",
      "$Y = \\Theta(X)\\cdot T + g(X, W) + \\epsilon$\n",
      "where for every outcome $i$ and treatment $j$ the CATE $\\Theta_{ij}(X)$ has the form:\n",
      "$\\Theta_{ij}(X) = X' coef_{ij} + cate\\_intercept_{ij}$\n",
      "Coefficient Results table portrays the $coef_{ij}$ parameter vector for each outcome $i$ and treatment $j$. Intercept Results table portrays the $cate\\_intercept_{ij}$ parameter.</sub>\n",
      "-----------------Double Lasso in DoubleML for PLR------------\n",
      "-----------------Random Forest in DoubleML for PLR------------\n",
      "          coef      std err         t         P>|t|        2.5 %        97.5 %\n",
      "d  8668.744728  1359.418235  6.376805  1.808207e-10  6004.333947  11333.155508\n",
      "-----------------Decision Tree in DoubleML for PLR------------\n",
      "          coef      std err         t         P>|t|       2.5 %        97.5 %\n",
      "d  9426.178388  1421.581152  6.630771  3.339391e-11  6639.93053  12212.426247\n",
      "-----------------Gradient Boosting in DoubleML for PLR------------\n",
      "          coef      std err         t         P>|t|        2.5 %        97.5 %\n",
      "d  9015.658347  1352.485526  6.665992  2.628827e-11  6364.835426  11666.481269\n",
      "-----------------Random Forest in EconML for IRM------------\n",
      "Coefficient Results:  X is None, please call intercept_inference to learn the constant!\n",
      "                        CATE Intercept Results                        \n",
      "======================================================================\n",
      "               point_estimate  stderr  zstat pvalue ci_lower  ci_upper\n",
      "----------------------------------------------------------------------\n",
      "cate_intercept       7994.934 1145.011 6.982    0.0 5750.753 10239.114\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "<sub>A linear parametric conditional average treatment effect (CATE) model was fitted:\n",
      "$Y = \\Theta(X)\\cdot T + g(X, W) + \\epsilon$\n",
      "where $T$ is the one-hot-encoding of the discrete treatment and for every outcome $i$ and treatment $j$ the CATE $\\Theta_{ij}(X)$ has the form:\n",
      "$\\Theta_{ij}(X) = \\phi(X)' coef_{ij} + cate\\_intercept_{ij}$\n",
      "where $\\phi(X)$ is the output of the `featurizer` or $X$ if `featurizer`=None. Coefficient Results table portrays the $coef_{ij}$ parameter vector for each outcome $i$ and the designated treatment $j$ passed to summary. Intercept Results table portrays the $cate\\_intercept_{ij}$ parameter.</sub>\n",
      "-----------------Random Forest in EconML for IRM (separate models)------------\n",
      "Coefficient Results:  X is None, please call intercept_inference to learn the constant!\n",
      "                        CATE Intercept Results                       \n",
      "=====================================================================\n",
      "               point_estimate  stderr  zstat pvalue ci_lower ci_upper\n",
      "---------------------------------------------------------------------\n",
      "cate_intercept       7673.317 1140.062 6.731    0.0 5438.835 9907.798\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "<sub>A linear parametric conditional average treatment effect (CATE) model was fitted:\n",
      "$Y = \\Theta(X)\\cdot T + g(X, W) + \\epsilon$\n",
      "where $T$ is the one-hot-encoding of the discrete treatment and for every outcome $i$ and treatment $j$ the CATE $\\Theta_{ij}(X)$ has the form:\n",
      "$\\Theta_{ij}(X) = \\phi(X)' coef_{ij} + cate\\_intercept_{ij}$\n",
      "where $\\phi(X)$ is the output of the `featurizer` or $X$ if `featurizer`=None. Coefficient Results table portrays the $coef_{ij}$ parameter vector for each outcome $i$ and the designated treatment $j$ passed to summary. Intercept Results table portrays the $cate\\_intercept_{ij}$ parameter.</sub>\n",
      "-----------------Decision Tree in EconML for IRM------------\n",
      "Coefficient Results:  X is None, please call intercept_inference to learn the constant!\n",
      "                        CATE Intercept Results                        \n",
      "======================================================================\n",
      "               point_estimate  stderr  zstat pvalue ci_lower  ci_upper\n",
      "----------------------------------------------------------------------\n",
      "cate_intercept       9222.086 1241.257  7.43    0.0 6789.267 11654.905\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "<sub>A linear parametric conditional average treatment effect (CATE) model was fitted:\n",
      "$Y = \\Theta(X)\\cdot T + g(X, W) + \\epsilon$\n",
      "where $T$ is the one-hot-encoding of the discrete treatment and for every outcome $i$ and treatment $j$ the CATE $\\Theta_{ij}(X)$ has the form:\n",
      "$\\Theta_{ij}(X) = \\phi(X)' coef_{ij} + cate\\_intercept_{ij}$\n",
      "where $\\phi(X)$ is the output of the `featurizer` or $X$ if `featurizer`=None. Coefficient Results table portrays the $coef_{ij}$ parameter vector for each outcome $i$ and the designated treatment $j$ passed to summary. Intercept Results table portrays the $cate\\_intercept_{ij}$ parameter.</sub>\n",
      "-----------------Decision Tree in EconML for IRM (separate models)------------\n",
      "Coefficient Results:  X is None, please call intercept_inference to learn the constant!\n",
      "                        CATE Intercept Results                        \n",
      "======================================================================\n",
      "               point_estimate  stderr  zstat pvalue ci_lower  ci_upper\n",
      "----------------------------------------------------------------------\n",
      "cate_intercept       8044.622 1189.481 6.763    0.0 5713.283 10375.961\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "<sub>A linear parametric conditional average treatment effect (CATE) model was fitted:\n",
      "$Y = \\Theta(X)\\cdot T + g(X, W) + \\epsilon$\n",
      "where $T$ is the one-hot-encoding of the discrete treatment and for every outcome $i$ and treatment $j$ the CATE $\\Theta_{ij}(X)$ has the form:\n",
      "$\\Theta_{ij}(X) = \\phi(X)' coef_{ij} + cate\\_intercept_{ij}$\n",
      "where $\\phi(X)$ is the output of the `featurizer` or $X$ if `featurizer`=None. Coefficient Results table portrays the $coef_{ij}$ parameter vector for each outcome $i$ and the designated treatment $j$ passed to summary. Intercept Results table portrays the $cate\\_intercept_{ij}$ parameter.</sub>\n",
      "-----------------Gradient Boosting in EconML for IRM------------\n",
      "Coefficient Results:  X is None, please call intercept_inference to learn the constant!\n",
      "                        CATE Intercept Results                        \n",
      "======================================================================\n",
      "               point_estimate  stderr  zstat pvalue ci_lower  ci_upper\n",
      "----------------------------------------------------------------------\n",
      "cate_intercept       8368.693 1128.212 7.418    0.0 6157.438 10579.947\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "<sub>A linear parametric conditional average treatment effect (CATE) model was fitted:\n",
      "$Y = \\Theta(X)\\cdot T + g(X, W) + \\epsilon$\n",
      "where $T$ is the one-hot-encoding of the discrete treatment and for every outcome $i$ and treatment $j$ the CATE $\\Theta_{ij}(X)$ has the form:\n",
      "$\\Theta_{ij}(X) = \\phi(X)' coef_{ij} + cate\\_intercept_{ij}$\n",
      "where $\\phi(X)$ is the output of the `featurizer` or $X$ if `featurizer`=None. Coefficient Results table portrays the $coef_{ij}$ parameter vector for each outcome $i$ and the designated treatment $j$ passed to summary. Intercept Results table portrays the $cate\\_intercept_{ij}$ parameter.</sub>\n",
      "-----------------Gradient Boosting in EconML for IRM (separate models)------------\n",
      "Coefficient Results:  X is None, please call intercept_inference to learn the constant!\n",
      "                        CATE Intercept Results                       \n",
      "=====================================================================\n",
      "               point_estimate  stderr  zstat pvalue ci_lower ci_upper\n",
      "---------------------------------------------------------------------\n",
      "cate_intercept       8191.684 1149.427 7.127    0.0 5938.849 10444.52\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "<sub>A linear parametric conditional average treatment effect (CATE) model was fitted:\n",
      "$Y = \\Theta(X)\\cdot T + g(X, W) + \\epsilon$\n",
      "where $T$ is the one-hot-encoding of the discrete treatment and for every outcome $i$ and treatment $j$ the CATE $\\Theta_{ij}(X)$ has the form:\n",
      "$\\Theta_{ij}(X) = \\phi(X)' coef_{ij} + cate\\_intercept_{ij}$\n",
      "where $\\phi(X)$ is the output of the `featurizer` or $X$ if `featurizer`=None. Coefficient Results table portrays the $coef_{ij}$ parameter vector for each outcome $i$ and the designated treatment $j$ passed to summary. Intercept Results table portrays the $cate\\_intercept_{ij}$ parameter.</sub>\n",
      "-----------------Random Forest in DoubleML for IRM------------\n",
      "          coef      std err         t         P>|t|       2.5 %       97.5 %\n",
      "d  7506.933727  1141.238795  6.577882  4.771978e-11  5270.14679  9743.720664\n",
      "-----------------Decision Tree in DoubleML for IRM------------\n",
      "          coef    std err         t         P>|t|        2.5 %       97.5 %\n",
      "d  7225.485969  1207.2451  5.985103  2.162533e-09  4859.329053  9591.642885\n",
      "-----------------Gradient Boosting in DoubleML for IRM------------\n",
      "          coef      std err         t         P>|t|        2.5 %        97.5 %\n",
      "d  8483.306715  1145.000981  7.408995  1.272600e-13  6239.146029  10727.467401\n"
     ]
    }
   ],
   "source": [
    " #c.) \n",
    "W = StandardScaler().fit_transform(transformer.fit_transform(X))\n",
    "# PLR in econml\n",
    "# ! pip install econml\n",
    "from econml.dml import LinearDML\n",
    "\n",
    "\n",
    "# double lasso in econml\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "ldml_lasso = LinearDML(\n",
    "    model_y=LassoCV(cv=cv),\n",
    "    model_t=LassoCV(cv=cv),\n",
    ").fit(y, D, W=W)\n",
    "print(\"----------------- Double Lasso in EconML for PLR------------\")\n",
    "print(ldml_lasso.summary())\n",
    "\n",
    "# random forest in econml\n",
    "ldml_rf = LinearDML(\n",
    "    model_y=RandomForestRegressor(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "    model_t=RandomForestClassifier(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "    cv=5,\n",
    "    discrete_treatment=True,\n",
    "    random_state=123\n",
    ").fit(y, D, W=W)\n",
    "print(\"-----------------Random Forest in EconML for PLR------------\")\n",
    "print(ldml_rf.summary())\n",
    "\n",
    "# gradient boosting in econml\n",
    "ldml_gb = LinearDML(\n",
    "    model_y=GradientBoostingRegressor(max_depth=2, n_iter_no_change=5, random_state=123),\n",
    "    model_t=GradientBoostingClassifier(max_depth=2, n_iter_no_change=5, random_state=123),\n",
    "    cv=5,\n",
    "    discrete_treatment=True,\n",
    "    random_state=123\n",
    ").fit(y, D, W=W)\n",
    "print(\"-----------------Gradient Boosting in EconML for PLR------------\")\n",
    "print(ldml_gb.summary())\n",
    "\n",
    "# PLR with Decision Tree\n",
    "ldml_dt = LinearDML(\n",
    "    model_y=DecisionTreeRegressor(min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "    model_t=DecisionTreeClassifier(min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "    cv=5,\n",
    "    discrete_treatment=True,\n",
    "    random_state=123\n",
    ").fit(y, D, W=W)\n",
    "print(\"-----------------Decision Tree in EconML for PLR------------\")\n",
    "print(ldml_dt.summary())\n",
    "\n",
    "# plr in double ml\n",
    "# ! pip install doubleml\n",
    "from doubleml import DoubleMLData\n",
    "import doubleml as dbml\n",
    "\n",
    "\n",
    "dml_data = DoubleMLData.from_arrays(W, y, D)\n",
    "\n",
    "try:\n",
    "    # double lasso\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "    dml_plr_lasso = dbml.DoubleMLPLR(\n",
    "        dml_data,\n",
    "        LassoCV(cv=cv),\n",
    "        LassoCV(cv=cv),\n",
    "        n_folds=5,\n",
    "    )\n",
    "    dml_plr_lasso.fit()\n",
    "    print(\"-----------------Double Lasso in DoubleML for PLR------------\")\n",
    "    # random forest\n",
    "    dml_plr_rf = dbml.DoubleMLPLR(\n",
    "        dml_data,\n",
    "        RandomForestRegressor(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "        RandomForestClassifier(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "        n_folds=5,\n",
    "    )\n",
    "    dml_plr_rf.fit()\n",
    "    print(\"-----------------Random Forest in DoubleML for PLR------------\")\n",
    "    print(dml_plr_rf.summary)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(\"DoubleML failed to run the PLR models\")\n",
    "\n",
    "# decision tree\n",
    "dml_plr_dt = dbml.DoubleMLPLR(\n",
    "    dml_data,\n",
    "    DecisionTreeRegressor(min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "    DecisionTreeClassifier(min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "    n_folds=5,\n",
    ")\n",
    "\n",
    "dml_plr_dt.fit()\n",
    "print(\"-----------------Decision Tree in DoubleML for PLR------------\")\n",
    "print(dml_plr_dt.summary)\n",
    "\n",
    "# gradient boosting\n",
    "\n",
    "dml_plr_gb = dbml.DoubleMLPLR(\n",
    "    dml_data,\n",
    "    GradientBoostingRegressor(max_depth=2, n_iter_no_change=5, random_state=123),\n",
    "    GradientBoostingClassifier(max_depth=2, n_iter_no_change=5, random_state=123),\n",
    "    n_folds=5,\n",
    ")\n",
    "\n",
    "dml_plr_gb.fit() \n",
    "print(\"-----------------Gradient Boosting in DoubleML for PLR------------\")\n",
    "print(dml_plr_gb.summary)\n",
    "\n",
    "\n",
    "# irm with econml\n",
    "\n",
    "from econml.dr import LinearDRLearner   \n",
    "from econml.utilities import SeparateModel\n",
    "\n",
    "# random forest\n",
    "dr_forest = LinearDRLearner(\n",
    "    model_regression=RandomForestRegressor(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "    model_propensity=RandomForestClassifier(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "    cv=5,\n",
    ")\n",
    "dr_forest.fit(y, D, W=W)\n",
    "print(\"-----------------Random Forest in EconML for IRM------------\")\n",
    "print(dr_forest.summary(T=1))\n",
    "\n",
    "# random forest using seperate models for model_regression\n",
    "dr_forest_sep = LinearDRLearner(\n",
    "    model_regression=SeparateModel(\n",
    "        RandomForestRegressor(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "        RandomForestRegressor(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "    ),\n",
    "    model_propensity=RandomForestClassifier(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "    cv=5,\n",
    ")\n",
    "dr_forest_sep.fit(y, D, W=W)\n",
    "print(\"-----------------Random Forest in EconML for IRM (separate models)------------\")\n",
    "print(dr_forest_sep.summary(T=1))\n",
    "\n",
    "# decision tree\n",
    "dr_tree = LinearDRLearner(\n",
    "    model_regression=DecisionTreeRegressor(min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "    model_propensity=DecisionTreeClassifier(min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "    cv=5,\n",
    ")\n",
    "dr_tree.fit(y, D, W=W)\n",
    "print(\"-----------------Decision Tree in EconML for IRM------------\")\n",
    "print(dr_tree.summary(T=1))\n",
    "\n",
    "# decision tree using seperate models for model_regression\n",
    "dr_tree_sep = LinearDRLearner(\n",
    "    model_regression=SeparateModel(\n",
    "        DecisionTreeRegressor(min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "        DecisionTreeRegressor(min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "    ),\n",
    "    model_propensity=DecisionTreeClassifier(min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "    cv=5,\n",
    ")\n",
    "dr_tree_sep.fit(y, D, W=W)\n",
    "print(\"-----------------Decision Tree in EconML for IRM (separate models)------------\")\n",
    "print(dr_tree_sep.summary(T=1))\n",
    "\n",
    "# gradient boosting\n",
    "dr_gb = LinearDRLearner(\n",
    "    model_regression=GradientBoostingRegressor(max_depth=2, n_iter_no_change=5, random_state=123),\n",
    "    model_propensity=GradientBoostingClassifier(max_depth=2, n_iter_no_change=5, random_state=123),\n",
    "    cv=5,\n",
    ")\n",
    "dr_gb.fit(y, D, W=W)\n",
    "print(\"-----------------Gradient Boosting in EconML for IRM------------\")\n",
    "print(dr_gb.summary(T=1))\n",
    "\n",
    "# gradient boosting using seperate models for model_regression\n",
    "dr_gb_sep = LinearDRLearner(\n",
    "    model_regression=SeparateModel(\n",
    "        GradientBoostingRegressor(max_depth=2, n_iter_no_change=5, random_state=123),\n",
    "        GradientBoostingRegressor(max_depth=2, n_iter_no_change=5, random_state=123),\n",
    "    ),\n",
    "    model_propensity=GradientBoostingClassifier(max_depth=2, n_iter_no_change=5, random_state=123),\n",
    "    cv=5,\n",
    ")\n",
    "dr_gb_sep.fit(y, D, W=W)\n",
    "print(\"-----------------Gradient Boosting in EconML for IRM (separate models)------------\")\n",
    "print(dr_gb_sep.summary(T=1))\n",
    "# irm with double ml\n",
    "\n",
    "# random forest\n",
    "dml_irm_rf = dbml.DoubleMLIRM(\n",
    "    dml_data,\n",
    "    RandomForestRegressor(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "    RandomForestClassifier(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "    n_folds=5,\n",
    ")\n",
    "dml_irm_rf.fit()\n",
    "print(\"-----------------Random Forest in DoubleML for IRM------------\")\n",
    "print(dml_irm_rf.summary)\n",
    "\n",
    "# decision tree\n",
    "\n",
    "dml_irm_dt = dbml.DoubleMLIRM(\n",
    "    dml_data,\n",
    "    DecisionTreeRegressor(min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "    DecisionTreeClassifier(min_samples_leaf=10, ccp_alpha=.001, random_state=123),\n",
    "    n_folds=5,\n",
    ")\n",
    "dml_irm_dt.fit()\n",
    "print(\"-----------------Decision Tree in DoubleML for IRM------------\")\n",
    "print(dml_irm_dt.summary)\n",
    "# gradient boosting\n",
    "\n",
    "dml_irm_gb = dbml.DoubleMLIRM(\n",
    "    dml_data,\n",
    "    GradientBoostingRegressor(max_depth=2, n_iter_no_change=5, random_state=123),\n",
    "    GradientBoostingClassifier(max_depth=2, n_iter_no_change=5, \n",
    "                               \n",
    "                               random_state=123),\n",
    "    n_folds=5,\n",
    ")\n",
    "dml_irm_gb.fit()\n",
    "print(\"-----------------Gradient Boosting in DoubleML for IRM------------\")\n",
    "print(dml_irm_gb.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Econml can work with most of the base learners (random forest, decision tree, boosted forest), as can doubleML. Both can work with any scikit-learn model in fact, so long as they implement fit() and predict() functions, according to the documentation. Theoretically, one could thus also build a custom class that implements the scikit api for stacking or semi crossfitting with choosing the best model, but neither library can work directly with stacking or perform the semi cross-fitting with the custom implementations we built as far as I could tell from the documentation of the packages. In theory one could write custom scikit-learn compatible interfaces for the custom implementations, but that would be a lot of work.  \n",
    "  \n",
    "In practice however, I found that double lasso did not work with the doubleML library. Upon closer examination, I found a section of the econml docs that explains that it specifically handles cases of models that do hyperparameter searches internally across folds. This is not the case for the doubleML library, which needs all folds to be treated strictly independently of each other under the hood and thus fails when LassoCV is used. This is a fundamental difference in the way the two libraries are built, and it means that doubleML cannot be used with double lasso as long as cross validation is used. One could however use a theoretically chosen penalty with lasso with scikit interface and then use that with doubleML.\n",
    "   \n",
    "As for the results (note that results may change slightly upon rerunning/exporting due to randomness. Please see output for full table, this is just for your convenience while grading. Estimate first, then standard error in parentheses, see above for other metrics): \n",
    "  \n",
    "For PLR:  \n",
    "\n",
    "double lasso previous: 9035\t(1295)\n",
    "double lasso econml: 8609 (1426)\n",
    "double lasso doubleML:  n/a\n",
    "  \n",
    "random forest previous:  8905 (1357)  \n",
    "random forest econml:  8603 (1333)   \n",
    "random forest doubleML: 8523 (1346)  \n",
    "  \n",
    "decision tree previous: 9236 (1440)  \n",
    "decision tree econml: 8772 (1449)  \n",
    "decision tree doubleML: 8734 (1455)  \n",
    "  \n",
    "boosted forest previous: 8840 (1334)  \n",
    "boosted forest econml: 9129 (1379)  \n",
    "boosted forest doubleML: 8834 (1366)  \n",
    "\n",
    "For IRM:  \n",
    "random forest previous: 7699 (1159)  \n",
    "random forest econml: 8023 (1121)  \n",
    "random forest doubleML: 7805 (1155)  \n",
    "\n",
    "decision tree previous: 7836 (1255)  \n",
    "decision tree econml: 8352 (1250)  \n",
    "decision tree doubleML: 7714 (1238) \n",
    "\n",
    "boosted forest previous: 8593 (1157)  \n",
    "boosted forest econml: 8118 (1135)  \n",
    "boosted forest doubleML: 8683 (1258)  \n",
    "\n",
    "The results are broadly consistent across the different methods and libraries, including for the decision tree. Apart from the decision tree in IRM, the results are also consistent with the estimates from the custom methods. For EconML and Double ML, the decision tree in the IRM setting is more in line with the other models than for the custom implementation, where it differs significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Semi-Synthetic Data with n=1000 ===\n",
      "True ATE in the semi-synthetic world:  7448.65797745209\n",
      "** PLR Results n = 1000 **\n",
      "True ATE in the semi-synthetic world:  7448.65797745209\n",
      "                             estimate       stderr        lower         upper  \\\n",
      "double lasso              7570.817593  2960.162817  1768.898472  13372.736715   \n",
      "lasso/logistic            7359.801520  3152.417924  1181.062390  13538.540651   \n",
      "random forest             8670.234101  3131.168538  2533.143767  14807.324434   \n",
      "decision tree             5864.766671  3204.117322  -415.303281  12144.836622   \n",
      "boosted forest            9239.795879  3039.307667  3282.752852  15196.838906   \n",
      "automl (semi-cfit)       10191.439448  2983.541924  4343.697277  16039.181619   \n",
      "stacked (semi-cfit)       7932.952213  3069.567326  1916.600254  13949.304173   \n",
      "select-best (semi-cfit)   8856.966459  3095.447524  2789.889312  14924.043605   \n",
      "\n",
      "                               rmse y    rmse D  accuracy D        error  \\\n",
      "double lasso             40373.607439  0.458513       0.672   122.159616   \n",
      "lasso/logistic           40373.607439  0.460075       0.672    88.856457   \n",
      "random forest            40557.935263  0.461674       0.660  1221.576123   \n",
      "decision tree            44235.857470  0.522950       0.601  1583.891307   \n",
      "boosted forest           40611.408415  0.459455       0.667  1791.137901   \n",
      "automl (semi-cfit)       40199.761620  0.456334       0.683  2742.781471   \n",
      "stacked (semi-cfit)      39782.417535  0.457400       0.674   484.294236   \n",
      "select-best (semi-cfit)  40373.607439  0.458222       0.668  1408.308481   \n",
      "\n",
      "                         rmse E[y|D,X]  rmse E[D|X]  covered  \n",
      "double lasso              15676.662465     0.063458        1  \n",
      "lasso/logistic            15676.662465     0.071838        1  \n",
      "random forest             15663.761077     0.081190        1  \n",
      "decision tree             23353.640896     0.263242        1  \n",
      "boosted forest            15895.012930     0.068039        1  \n",
      "automl (semi-cfit)        14239.678129     0.056597        1  \n",
      "stacked (semi-cfit)       13735.341021     0.057363        1  \n",
      "select-best (semi-cfit)   15676.662465     0.070003        1  \n",
      "** IRM Results n = 1000**\n",
      "                             estimate        stderr         lower  \\\n",
      "lasso/logistic            1620.693625   6897.991516 -11899.369746   \n",
      "random forest             7152.572748   3565.170354    164.838853   \n",
      "decision tree            31178.702869  29201.573032 -26056.380273   \n",
      "boosted forest            9340.582923   3165.138294   3136.911868   \n",
      "automl (semi-cfit)       10059.474594   2831.098627   4510.521286   \n",
      "stacked (semi-cfit)       7966.144464   3306.583064   1485.241659   \n",
      "select-best (semi-cfit)   7178.614162   3148.213911   1008.114896   \n",
      "\n",
      "                                upper        rmse y    rmse D  accuracy D  \\\n",
      "lasso/logistic           15140.756996  39909.363137  0.459594       0.680   \n",
      "random forest            14140.306643  42059.093216  0.459752       0.667   \n",
      "decision tree            88413.786011  46047.807183  0.522531       0.602   \n",
      "boosted forest           15544.253979  42130.438829  0.460049       0.669   \n",
      "automl (semi-cfit)       15608.427902  40157.905147  0.455232       0.679   \n",
      "stacked (semi-cfit)      14447.047269  39831.080727  0.455836       0.678   \n",
      "select-best (semi-cfit)  13349.113427  40039.931373  0.459116       0.665   \n",
      "\n",
      "                                error  rmse E[y|D,X]  rmse E[D|X]  covered  \n",
      "lasso/logistic            5827.964352   14579.865660     0.075939        1  \n",
      "random forest              296.085229   19091.271203     0.080773        1  \n",
      "decision tree            23730.044892   25136.861600     0.261051        1  \n",
      "boosted forest            1891.924946   18636.258998     0.075839        1  \n",
      "automl (semi-cfit)        2610.816616   14420.766540     0.053966        1  \n",
      "stacked (semi-cfit)        517.486486   14129.357730     0.059021        1  \n",
      "select-best (semi-cfit)    270.043816   14616.113115     0.069984        1  \n",
      "\n",
      "=== Semi-Synthetic Data with n=10000 ===\n",
      "True ATE in the semi-synthetic world:  7448.65797745209\n",
      "** PLR Results n = 10000 **\n",
      "True ATE in the semi-synthetic world:  7448.65797745209\n",
      "                            estimate       stderr        lower         upper  \\\n",
      "double lasso             7300.188213  1358.365332  4637.792163   9962.584264   \n",
      "lasso/logistic           7357.744199  1360.240713  4691.672402  10023.815995   \n",
      "random forest            7161.334923  1328.354387  4557.760324   9764.909522   \n",
      "decision tree            7392.153497  1407.399041  4633.651377  10150.655616   \n",
      "boosted forest           7137.731474  1335.417418  4520.313336   9755.149613   \n",
      "automl (semi-cfit)       7356.585278  1334.176848  4741.598655   9971.571901   \n",
      "stacked (semi-cfit)      7249.825940  1331.977105  4639.150813   9860.501066   \n",
      "select-best (semi-cfit)  7182.821738  1337.498892  4561.323910   9804.319566   \n",
      "\n",
      "                               rmse y    rmse D  accuracy D       error  \\\n",
      "double lasso             57986.087770  0.454712      0.6760  148.469764   \n",
      "lasso/logistic           57986.087770  0.454884      0.6731   90.913779   \n",
      "random forest            57513.461438  0.454650      0.6709  287.323055   \n",
      "decision tree            60641.275438  0.455564      0.6718   56.504481   \n",
      "boosted forest           57730.000205  0.454600      0.6720  310.926503   \n",
      "automl (semi-cfit)       57248.086414  0.454605      0.6728   92.072699   \n",
      "stacked (semi-cfit)      57241.917048  0.453979      0.6731  198.832038   \n",
      "select-best (semi-cfit)  57474.462428  0.454623      0.6732  265.836239   \n",
      "\n",
      "                         rmse E[y|D,X]  rmse E[D|X]  covered  \n",
      "double lasso              14056.375423     0.040397        1  \n",
      "lasso/logistic            14056.375423     0.040583        1  \n",
      "random forest             12619.074476     0.044488        1  \n",
      "decision tree             23910.075797     0.049067        1  \n",
      "boosted forest            12629.833122     0.037302        1  \n",
      "automl (semi-cfit)        10551.956929     0.034241        1  \n",
      "stacked (semi-cfit)       10826.092032     0.031614        1  \n",
      "select-best (semi-cfit)   11660.452989     0.036570        1  \n",
      "** IRM Results n = 10000**\n",
      "                            estimate       stderr        lower        upper  \\\n",
      "lasso/logistic           6212.259105  1341.860521  3582.212485  8842.305726   \n",
      "random forest            6056.085901  1284.036067  3539.375209  8572.796592   \n",
      "decision tree            5439.385879  1417.333202  2661.412803  8217.358954   \n",
      "boosted forest           6228.964388  1330.395371  3621.389460  8836.539316   \n",
      "automl (semi-cfit)       6125.668026  1320.334949  3537.811526  8713.524527   \n",
      "stacked (semi-cfit)      6051.714222  1314.438499  3475.414764  8628.013680   \n",
      "select-best (semi-cfit)  5898.859291  1285.935295  3378.426114  8419.292469   \n",
      "\n",
      "                               rmse y    rmse D  accuracy D        error  \\\n",
      "lasso/logistic           57917.005957  0.455028      0.6729  1236.398872   \n",
      "random forest            57202.076558  0.454923      0.6724  1392.572077   \n",
      "decision tree            60933.258805  0.455564      0.6718  2009.272099   \n",
      "boosted forest           57819.032291  0.454460      0.6721  1219.693589   \n",
      "automl (semi-cfit)       57036.038361  0.454211      0.6737  1322.989951   \n",
      "stacked (semi-cfit)      56973.336259  0.454044      0.6734  1396.943755   \n",
      "select-best (semi-cfit)  57230.557431  0.454423      0.6737  1549.798686   \n",
      "\n",
      "                         rmse E[y|D,X]  rmse E[D|X]  covered  \n",
      "lasso/logistic            13650.508219     0.042618        1  \n",
      "random forest             10356.422107     0.045094        1  \n",
      "decision tree             23500.580768     0.049067        1  \n",
      "boosted forest            13061.853165     0.035975        1  \n",
      "automl (semi-cfit)         8785.193220     0.033680        1  \n",
      "stacked (semi-cfit)        8846.660862     0.030831        1  \n",
      "select-best (semi-cfit)   10334.419324     0.036232        1  \n",
      "\n",
      "=== Semi-Synthetic Data with n=50000 ===\n",
      "True ATE in the semi-synthetic world:  7448.65797745209\n",
      "** PLR Results n = 50000 **\n",
      "True ATE in the semi-synthetic world:  7448.65797745209\n",
      "                            estimate      stderr        lower        upper  \\\n",
      "double lasso             8024.338323  550.580634  6945.200281  9103.476366   \n",
      "lasso/logistic           8010.291645  551.380263  6929.586330  9090.996961   \n",
      "random forest            8074.429501  551.120441  6994.233436  9154.625566   \n",
      "decision tree            8159.270493  571.417066  7039.293044  9279.247942   \n",
      "boosted forest           8077.055802  545.129543  7008.601897  9145.509706   \n",
      "automl (semi-cfit)       8075.584906  543.195905  7010.920933  9140.248879   \n",
      "stacked (semi-cfit)      8096.377538  543.463469  7031.189139  9161.565937   \n",
      "select-best (semi-cfit)  8107.604155  543.950319  7041.461530  9173.746781   \n",
      "\n",
      "                               rmse y    rmse D  accuracy D       error  \\\n",
      "double lasso             53502.740540  0.453518     0.67038  575.680346   \n",
      "lasso/logistic           53502.740540  0.453649     0.67156  561.633668   \n",
      "random forest            53603.902923  0.454298     0.66852  625.771523   \n",
      "decision tree            55924.904636  0.454753     0.66732  710.612515   \n",
      "boosted forest           52880.095207  0.452612     0.67156  628.397824   \n",
      "automl (semi-cfit)       52725.731485  0.453403     0.66798  626.926929   \n",
      "stacked (semi-cfit)      52774.276101  0.452520     0.67106  647.719561   \n",
      "select-best (semi-cfit)  52867.695133  0.452609     0.67122  658.946178   \n",
      "\n",
      "                         rmse E[y|D,X]  rmse E[D|X]  covered  \n",
      "double lasso              13594.020593     0.033791        1  \n",
      "lasso/logistic            13594.020593     0.033765        1  \n",
      "random forest             13857.258360     0.042927        1  \n",
      "decision tree             21030.553549     0.046215        1  \n",
      "boosted forest            10698.654339     0.022983        1  \n",
      "automl (semi-cfit)         9906.719291     0.034186        1  \n",
      "stacked (semi-cfit)       10208.825645     0.022636        1  \n",
      "select-best (semi-cfit)   10641.942067     0.022951        1  \n",
      "** IRM Results n = 50000**\n",
      "                            estimate      stderr        lower        upper  \\\n",
      "lasso/logistic           7835.778651  747.289233  6371.091753  9300.465548   \n",
      "random forest            7414.311519  513.909576  6407.048751  8421.574287   \n",
      "decision tree            7714.220988  558.121451  6620.302944  8808.139031   \n",
      "boosted forest           7306.751439  525.298708  6277.165972  8336.336906   \n",
      "automl (semi-cfit)       7507.644812  512.063887  6503.999593  8511.290032   \n",
      "stacked (semi-cfit)      7369.834290  531.769395  6327.566276  8412.102305   \n",
      "select-best (semi-cfit)  7305.412628  526.919545  6272.650320  8338.174935   \n",
      "\n",
      "                               rmse y    rmse D  accuracy D       error  \\\n",
      "lasso/logistic           53286.342273  0.453649     0.67126  387.120673   \n",
      "random forest            52842.442527  0.454257     0.66930   34.346458   \n",
      "decision tree            55665.149015  0.454753     0.66732  265.563010   \n",
      "boosted forest           52528.022059  0.452579     0.67056  141.906539   \n",
      "automl (semi-cfit)       52213.765264  0.453857     0.66836   58.986835   \n",
      "stacked (semi-cfit)      52291.068439  0.452582     0.67078   78.823687   \n",
      "select-best (semi-cfit)  52646.664341  0.452588     0.67090  143.245350   \n",
      "\n",
      "                         rmse E[y|D,X]  rmse E[D|X]  covered  \n",
      "lasso/logistic            12695.070405     0.033761        1  \n",
      "random forest             10396.243313     0.042882        1  \n",
      "decision tree             20225.489544     0.046215        1  \n",
      "boosted forest             8796.794164     0.023023        1  \n",
      "automl (semi-cfit)         6659.592326     0.036369        1  \n",
      "stacked (semi-cfit)        7254.062312     0.022226        1  \n",
      "select-best (semi-cfit)    9379.536326     0.023361        1  \n"
     ]
    }
   ],
   "source": [
    "# d.) \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_predict, KFold\n",
    "from sklearn.base import clone\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class semisynth:\n",
    "    \n",
    "    def fit(self, X, D, y, transformer, random_state=None):\n",
    "        \"\"\"\n",
    "        X, D, y: the real data\n",
    "        transformer: any sklearn-compatible Transformer for pre-processing\n",
    "        \"\"\"\n",
    "        self.X_ = X.copy()\n",
    "\n",
    "        # Model for Y|D=0\n",
    "        self.est0_ = make_pipeline(transformer,\n",
    "                                   RandomForestRegressor(min_samples_leaf=20,\n",
    "                                                         ccp_alpha=0.001,\n",
    "                                                         random_state=random_state)\n",
    "                                  ).fit(X[D==0], y[D==0])\n",
    "        self.res0_ = y[D==0] - self.est0_.predict(X[D==0])\n",
    "        # De-mean the residual distribution\n",
    "        self.res0_ -= np.mean(self.res0_)\n",
    "\n",
    "        # Model for Y|D=1\n",
    "        self.est1_ = make_pipeline(transformer,\n",
    "                                   RandomForestRegressor(min_samples_leaf=20,\n",
    "                                                         ccp_alpha=0.001,\n",
    "                                                         random_state=random_state)\n",
    "                                  ).fit(X[D==1], y[D==1])\n",
    "        self.res1_ = y[D==1] - self.est1_.predict(X[D==1])\n",
    "        self.res1_ -= np.mean(self.res1_)\n",
    "\n",
    "        # Model for D|X\n",
    "        self.prop_ = make_pipeline(transformer,\n",
    "                                   RandomForestClassifier(min_samples_leaf=20,\n",
    "                                                          ccp_alpha=0.001,\n",
    "                                                          random_state=random_state)\n",
    "                                  ).fit(X, D)\n",
    "        return self\n",
    "\n",
    "    def generate_data(self, n):\n",
    "        \"\"\"\n",
    "        Returns (X, D, Y, Y1, Y0):\n",
    "          X, D, Y: the new sample\n",
    "          Y1, Y0: potential outcomes for each row\n",
    "        \"\"\"\n",
    "        # Resample X from the empirical distribution\n",
    "        X = self.X_.iloc[np.random.choice(self.X_.shape[0], n, replace=True)]\n",
    "        \n",
    "        # Simulate D ~ Bernoulli(\\hat{p}(X))\n",
    "        pX = self.prop_.predict_proba(X)[:, 1]\n",
    "        D = np.random.binomial(1, pX)\n",
    "\n",
    "        # Construct Y0, Y1 by re-sampling from residual distribution\n",
    "        y0 = self.est0_.predict(X) + self.res0_[np.random.choice(self.res0_.shape[0], n, replace=True)]\n",
    "        y1 = self.est1_.predict(X) + self.res1_[np.random.choice(self.res1_.shape[0], n, replace=True)]\n",
    "        \n",
    "        # Observed Y\n",
    "        y = y0*(1 - D) + y1*D\n",
    "        return X, D, y, y1, y0\n",
    "    \n",
    "    def y_cef(self, X, D):\n",
    "        \"\"\"\n",
    "        Returns the 'true' E[Y|X, D] in the semi-synthetic world\n",
    "        = the random forest predictions from the original data\n",
    "        \"\"\"\n",
    "        return self.est1_.predict(X)*D + self.est0_.predict(X)*(1 - D)\n",
    "    \n",
    "    def D_cef(self, X):\n",
    "        \"\"\"\n",
    "        Returns the 'true' E[D|X] in the semi-synthetic world\n",
    "        \"\"\"\n",
    "        return self.prop_.predict_proba(X)[:, 1]\n",
    "\n",
    "    @property\n",
    "    def true_ate(self):\n",
    "        \"\"\"\n",
    "        The 'true' ATE in the semi-synthetic world, i.e. E[f1(X) - f0(X)]\n",
    "        using the entire original X_ distribution.\n",
    "        \"\"\"\n",
    "        return np.mean(self.est1_.predict(self.X_) - self.est0_.predict(self.X_))\n",
    "\n",
    "\n",
    "def summary(\n",
    "    point, stderr,\n",
    "    yhat, Dhat,    # cross-fitted predictions for y and D\n",
    "    resy, resD,    # residuals y-yhat, D-Dhat\n",
    "    final_residual, # epsilon or drhat\n",
    "    X, D, y,\n",
    "    *,\n",
    "    name,\n",
    "    synth  # the semisynth object, so we can compare to the \"true\" functions\n",
    "):\n",
    "    true_ate = synth.true_ate\n",
    "    covered = (point - 1.96*stderr <= true_ate <= point + 1.96*stderr)\n",
    "\n",
    "    # We'll compute the \"true\" E[Y|D,X], E[D|X]\n",
    "    y_cef_true = synth.y_cef(X, D)\n",
    "    d_cef_true = synth.D_cef(X)\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'estimate':    [point],\n",
    "        'stderr':      [stderr],\n",
    "        'lower':       [point - 1.96*stderr],\n",
    "        'upper':       [point + 1.96*stderr],\n",
    "        'rmse y':      [np.sqrt(np.mean(resy**2))],  # RMSE vs. *observed* Y\n",
    "        'rmse D':      [np.sqrt(np.mean(resD**2))],  # RMSE vs. *observed* D\n",
    "        'accuracy D':  [np.mean(np.abs(resD) < 0.5)],# classification accuracy\n",
    "        # New columns:\n",
    "        'error':       [abs(point - true_ate)],    # how far from true\n",
    "        'rmse E[y|D,X]':[np.sqrt(np.mean((yhat - y_cef_true)**2))],\n",
    "        'rmse E[D|X]': [np.sqrt(np.mean((Dhat - d_cef_true)**2))],\n",
    "        'covered':     [1 if covered else 0]       # did CI cover true ATE?\n",
    "    }, index=[name])\n",
    "    \n",
    "    \n",
    "from copy import deepcopy\n",
    "\n",
    "synth = semisynth().fit(X, D, y, transformer, random_state=123)\n",
    "\n",
    "\n",
    "\n",
    "def run_plr_methods(X_train, D_train, y_train, synth):\n",
    "    \"\"\"\n",
    "    X_train, D_train, y_train come from synth.generate_data(...)\n",
    "    We'll replicate your DML approach for partial linear model\n",
    "    with multiple model combos and stack them in a table.\n",
    "    \"\"\"\n",
    "    results_table = []\n",
    "\n",
    "    # 1) Double Lasso with cross-fitting\n",
    "    # (a) specify pipelines\n",
    "    lassoy_ = deepcopy(lassoy)\n",
    "    lassod_ = deepcopy(lassod)\n",
    "    # (b) run\n",
    "    point, stderr, yhat, Dhat, resy, resD, eps = dml(X_train, D_train, y_train, lassoy_, lassod_, nfolds=5)\n",
    "    # (c) summary\n",
    "    df_ = summary(point, stderr, yhat, Dhat, resy, resD, eps,\n",
    "                  X_train, D_train, y_train, name='double lasso', synth=synth)\n",
    "    results_table.append(df_)\n",
    "\n",
    "    # 2) lasso / logistic\n",
    "    lassoy_ = deepcopy(lassoy)\n",
    "    lgrd_   = deepcopy(lgrd)\n",
    "    point, stderr, yhat, Dhat, resy, resD, eps = dml(X_train, D_train, y_train,\n",
    "                                                     lassoy_, lgrd_, nfolds=5, classifier=True)\n",
    "    df_ = summary(point, stderr, yhat, Dhat, resy, resD, eps,\n",
    "                  X_train, D_train, y_train, name='lasso/logistic', synth=synth)\n",
    "    results_table.append(df_)\n",
    "\n",
    "    # 3) Random Forest\n",
    "    rfy_ = deepcopy(rfy)\n",
    "    rfd_ = deepcopy(rfd)\n",
    "    point, stderr, yhat, Dhat, resy, resD, eps = dml(X_train, D_train, y_train,\n",
    "                                                     rfy_, rfd_, nfolds=5, classifier=True)\n",
    "    df_ = summary(point, stderr, yhat, Dhat, resy, resD, eps,\n",
    "                  X_train, D_train, y_train, name='random forest', synth=synth)\n",
    "    results_table.append(df_)\n",
    "\n",
    "    # 4) Decision Tree\n",
    "    dtry_ = deepcopy(dtry)\n",
    "    dtrd_ = deepcopy(dtrd)\n",
    "    point, stderr, yhat, Dhat, resy, resD, eps = dml(X_train, D_train, y_train,\n",
    "                                                     dtry_, dtrd_, nfolds=5, classifier=True)\n",
    "    df_ = summary(point, stderr, yhat, Dhat, resy, resD, eps,\n",
    "                  X_train, D_train, y_train, name='decision tree', synth=synth)\n",
    "    results_table.append(df_)\n",
    "\n",
    "    # 5) Boosted trees\n",
    "    gbfy_ = deepcopy(gbfy)\n",
    "    gbfd_ = deepcopy(gbfd)\n",
    "    point, stderr, yhat, Dhat, resy, resD, eps = dml(X_train, D_train, y_train,\n",
    "                                                     gbfy_, gbfd_, nfolds=5, classifier=True)\n",
    "    df_ = summary(point, stderr, yhat, Dhat, resy, resD, eps,\n",
    "                  X_train, D_train, y_train, name='boosted forest', synth=synth)\n",
    "    results_table.append(df_)\n",
    "\n",
    "    # 6) automl (semi-cfit)\n",
    "    # Similarly for stacking (semi-cfit).\n",
    "    flamly_ = make_pipeline(transformer, AutoML(time_budget=50,\n",
    "                                                task='regression',\n",
    "                                                early_stop=True,\n",
    "                                                eval_method='cv',\n",
    "                                                n_splits=3,\n",
    "                                                metric='r2',\n",
    "                                                verbose=0))\n",
    "    flamld_ = make_pipeline(transformer, AutoML(time_budget=50,\n",
    "                                                task='classification',\n",
    "                                                early_stop=True,\n",
    "                                                eval_method='cv',\n",
    "                                                n_splits=3,\n",
    "                                                metric='r2',\n",
    "                                                verbose=0))\n",
    "    # Fit Y, D on entire X_train\n",
    "    flamly_.fit(X_train, y_train)\n",
    "    besty_model = flamly_[-1].best_model_for_estimator(flamly_[-1].best_estimator)\n",
    "    besty = make_pipeline(transformer, clone(besty_model))\n",
    "\n",
    "    flamld_.fit(X_train, D_train)\n",
    "    bestd_model = flamld_[-1].best_model_for_estimator(flamld_[-1].best_estimator)\n",
    "    bestd = make_pipeline(transformer, clone(bestd_model))\n",
    "\n",
    "    point, stderr, yhat, Dhat, resy, resD, eps = dml(X_train, D_train, y_train,\n",
    "                                                     besty, bestd, nfolds=5, classifier=True)\n",
    "    df_ = summary(point, stderr, yhat, Dhat, resy, resD, eps,\n",
    "                  X_train, D_train, y_train, name='automl (semi-cfit)', synth=synth)\n",
    "    results_table.append(df_)\n",
    "\n",
    "    # 7) stacking (semi-cfit)\n",
    "    point, stderr, yhat, Dhat, resy, resD, eps = dml_dirty(\n",
    "        X_train, D_train, y_train,\n",
    "        [lassoy, rfy, dtry, gbfy],\n",
    "        [lgrd, rfd, dtrd, gbfd],\n",
    "        nfolds=5, classifier=True\n",
    "    )\n",
    "    df_ = summary(point, stderr, yhat, Dhat, resy, resD, eps,\n",
    "                  X_train, D_train, y_train, name='stacked (semi-cfit)', synth=synth)\n",
    "    results_table.append(df_)\n",
    "    \n",
    "    #8 select best\n",
    "    point, stderr, yhat, Dhat, resy, resD, eps = dml_select_best(\n",
    "        X_train, D_train, y_train,\n",
    "        [lassoy, rfy, dtry, gbfy],\n",
    "        [lgrd, rfd, dtrd, gbfd],\n",
    "        nfolds=5, classifier=True\n",
    "    )\n",
    "    df_ = summary(point, stderr, yhat, Dhat, resy, resD, eps,\n",
    "                    X_train, D_train, y_train, name='select-best (semi-cfit)', synth=synth)\n",
    "    results_table.append(df_)\n",
    "\n",
    "    return pd.concat(results_table)\n",
    "\n",
    "\n",
    "def run_irm_methods(X_train, D_train, y_train, synth):\n",
    "    \"\"\"\n",
    "    X_train, D_train, y_train from synth.generate_data(...)\n",
    "    We'll replicate your IRM approach with multiple model combos and stack in a table.\n",
    "    \"\"\"\n",
    "    results_table = []\n",
    "\n",
    "    # 1) lasso-lasso, logistic repeated seeds + median aggregator\n",
    "    # We'll do a single run for demonstration:\n",
    "    lassoy_ = deepcopy(lassoytest)  # or define a pipeline as in the notebook\n",
    "    lgrd_   = deepcopy(lgrdtest)\n",
    "\n",
    "    point, stderr, yhat, Dhat, resy, resD, drhat = dr(X_train, D_train, y_train,\n",
    "                                                      lassoy_, lassoy_,\n",
    "                                                      lgrd_, nfolds=5)\n",
    "    df_ = summary(point, stderr, yhat, Dhat, resy, resD, drhat,\n",
    "                  X_train, D_train, y_train, name='lasso/logistic', synth=synth)\n",
    "    results_table.append(df_)\n",
    "\n",
    "    # 2) random forest\n",
    "    rfy_ = deepcopy(rfy)\n",
    "    rfd_ = deepcopy(rfd)\n",
    "    point, stderr, yhat, Dhat, resy, resD, drhat = dr(X_train, D_train, y_train,\n",
    "                                                      rfy_, rfy_, rfd_, nfolds=5)\n",
    "    df_ = summary(point, stderr, yhat, Dhat, resy, resD, drhat,\n",
    "                  X_train, D_train, y_train, name='random forest', synth=synth)\n",
    "    results_table.append(df_)\n",
    "\n",
    "    # 3) decision tree\n",
    "    dtry_ = deepcopy(dtry)\n",
    "    dtrd_ = deepcopy(dtrd)\n",
    "    point, stderr, yhat, Dhat, resy, resD, drhat = dr(X_train, D_train, y_train,\n",
    "                                                      dtry_, dtry_, dtrd_, nfolds=5)\n",
    "    df_ = summary(point, stderr, yhat, Dhat, resy, resD, drhat,\n",
    "                  X_train, D_train, y_train, name='decision tree', synth=synth)\n",
    "    results_table.append(df_)\n",
    "\n",
    "    # 4) boosted forest\n",
    "    gbfy_ = deepcopy(gbfy)\n",
    "    gbfd_ = deepcopy(gbfd)\n",
    "    point, stderr, yhat, Dhat, resy, resD, drhat = dr(X_train, D_train, y_train,\n",
    "                                                      gbfy_, gbfy_, gbfd_, nfolds=5)\n",
    "    df_ = summary(point, stderr, yhat, Dhat, resy, resD, drhat,\n",
    "                  X_train, D_train, y_train, name='boosted forest', synth=synth)\n",
    "    results_table.append(df_)\n",
    "\n",
    "    # 5) automl\n",
    "    flamly0_ = make_pipeline(transformer, AutoML(time_budget=30, task='regression', early_stop=True,\n",
    "                                                eval_method='cv', n_splits=3, metric='r2', verbose=0))\n",
    "    flamly1_ = make_pipeline(transformer, AutoML(time_budget=30, task='regression', early_stop=True,\n",
    "                                                eval_method='cv', n_splits=3, metric='r2', verbose=0))\n",
    "    flamld_  = make_pipeline(transformer, AutoML(time_budget=30, task='classification', early_stop=True,\n",
    "                                                eval_method='cv', n_splits=3, metric='r2', verbose=0))\n",
    "    # Fit for Y|D=0, Y|D=1\n",
    "    flamly0_.fit(X_train[D_train == 0], y_train[D_train == 0])\n",
    "    besty0_model = flamly0_[-1].best_model_for_estimator(flamly0_[-1].best_estimator)\n",
    "    besty0 = make_pipeline(transformer, clone(besty0_model))\n",
    "\n",
    "    flamly1_.fit(X_train[D_train == 1], y_train[D_train == 1])\n",
    "    besty1_model = flamly1_[-1].best_model_for_estimator(flamly1_[-1].best_estimator)\n",
    "    besty1 = make_pipeline(transformer, clone(besty1_model))\n",
    "\n",
    "    flamld_.fit(X_train, D_train)\n",
    "    bestd_model = flamld_[-1].best_model_for_estimator(flamld_[-1].best_estimator)\n",
    "    bestd = make_pipeline(transformer, clone(bestd_model))\n",
    "\n",
    "    point, stderr, yhat, Dhat, resy, resD, drhat = dr(X_train, D_train, y_train,\n",
    "                                                      besty0, besty1, bestd,\n",
    "                                                      nfolds=5)\n",
    "    df_ = summary(point, stderr, yhat, Dhat, resy, resD, drhat,\n",
    "                  X_train, D_train, y_train, name='automl (semi-cfit)', synth=synth)\n",
    "    results_table.append(df_)\n",
    "\n",
    "    # 6) stacking (semi-cfit):\n",
    "    lassoy_ = deepcopy(lassoy)\n",
    "    rfy_ = deepcopy(rfy)\n",
    "    dtry_ = deepcopy(dtry)\n",
    "    gbfy_ = deepcopy(gbfy)\n",
    "\n",
    "    lgrd_ = deepcopy(lgrd)\n",
    "    rfd_  = deepcopy(rfd)\n",
    "    dtrd_ = deepcopy(dtrd)\n",
    "    gbfd_ = deepcopy(gbfd)\n",
    "\n",
    "    point, stderr, yhat, Dhat, resy, resD, drhat = dr_dirty(\n",
    "        X_train, D_train, y_train,\n",
    "        [lassoy_, rfy_, dtry_, gbfy_],\n",
    "        [lassoy_, rfy_, dtry_, gbfy_],\n",
    "        [lgrd_, rfd_, dtrd_, gbfd_],\n",
    "        nfolds=5\n",
    "    )\n",
    "    df_ = summary(point, stderr, yhat, Dhat, resy, resD, drhat,\n",
    "                  X_train, D_train, y_train, name='stacked (semi-cfit)', synth=synth)\n",
    "    results_table.append(df_)\n",
    "    \n",
    "    # 7) select best\n",
    "    \n",
    "    point, stderr, yhat, Dhat, resy, resD, drhat = dr_select_best(\n",
    "        X_train, D_train, y_train,\n",
    "        [lassoy, rfy, dtry, gbfy],\n",
    "        [lassoy, rfy, dtry, gbfy],\n",
    "        [lgrd, rfd, dtrd, gbfd],\n",
    "        nfolds=5\n",
    "    )\n",
    "    df_ = summary(point, stderr, yhat, Dhat, resy, resD, drhat,\n",
    "                    X_train, D_train, y_train, name='select-best (semi-cfit)', synth=synth)\n",
    "    results_table.append(df_)\n",
    "\n",
    "    return pd.concat(results_table)\n",
    "\n",
    "\n",
    "for n in [1000, 10000, 50000]:\n",
    "    print(f\"\\n=== Semi-Synthetic Data with n={n} ===\")\n",
    "    print(\"True ATE in the semi-synthetic world: \", synth.true_ate)\n",
    "    X_synth, D_synth, y_synth, y1_synth, y0_synth = synth.generate_data(n)\n",
    "\n",
    "\n",
    "    print(f\"** PLR Results n = {n} **\")\n",
    "    print(\"True ATE in the semi-synthetic world: \", synth.true_ate)\n",
    "    table_plr = run_plr_methods(X_synth, D_synth, y_synth, synth)\n",
    "    print(table_plr)\n",
    "\n",
    "    print(f\"** IRM Results n = {n}**\")\n",
    "    table_irm = run_irm_methods(X_synth, D_synth, y_synth, synth)\n",
    "    print(table_irm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For n = 1000:    \n",
    "In the PLR setting, random forest outperforms all other methods in terms of the estimate. \n",
    "In the IRM setting, the random forest outperforms all other methods in terms of the estimate.   \n",
    "So neither automl nor stacking perform as well as the best model alone.  \n",
    "\n",
    "For n = 10000:  \n",
    "\n",
    "In the PLR setting, the decision tree performs best in terms of the estimate.     \n",
    "In the IRM setting, lasso / logistic performs best in terms of the estimate.   \n",
    "So neither stacking nor automl perform as well as the best model alone.   \n",
    "  \n",
    "For n = 50000:  \n",
    "  \n",
    "In the PLR setting, Decision tree performs best in terms of the estimate.   \n",
    "In the IRM setting, Decision tree performs best in terms of the estimate.  \n",
    "Neither automl nor stacking perform as well as the best model alone.  \n",
    "\n",
    "  \n",
    "Please note that running the code repeatedly showed that the results can vary due to randomness. The results above are from a single run and I saw other models perform best in some cases in other runs.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs288_alt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
