{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1276, 14)\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:           peacefactorR   R-squared:                       0.154\n",
      "Model:                            OLS   Adj. R-squared:                  0.150\n",
      "Method:                 Least Squares   F-statistic:                     25.44\n",
      "Date:                Mon, 03 Mar 2025   Prob (F-statistic):           1.23e-29\n",
      "Time:                        10:36:34   Log-Likelihood:                -5.2404\n",
      "No. Observations:                1276   AIC:                             26.48\n",
      "Df Residuals:                    1268   BIC:                             67.69\n",
      "Df Model:                           7                                         \n",
      "Covariance Type:              cluster                                         \n",
      "===================================================================================\n",
      "                      coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------\n",
      "Intercept       -8.509e-16   4.83e-15     -0.176      0.860   -1.03e-14    8.61e-15\n",
      "directlyharmedR     0.0973      0.024      4.085      0.000       0.051       0.144\n",
      "femaleR            -0.2321      0.024     -9.495      0.000      -0.280      -0.184\n",
      "ageR               -0.0021      0.001     -2.784      0.005      -0.004      -0.001\n",
      "farmerR            -0.0404      0.030     -1.368      0.171      -0.098       0.018\n",
      "herderR             0.0143      0.036      0.391      0.696      -0.057       0.086\n",
      "pastvotedR         -0.0480      0.027     -1.787      0.074      -0.101       0.005\n",
      "hhsizeR             0.0012      0.002      0.568      0.570      -0.003       0.005\n",
      "==============================================================================\n",
      "Omnibus:                       72.752   Durbin-Watson:                   2.163\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               95.464\n",
      "Skew:                           0.522   Prob(JB):                     1.86e-21\n",
      "Kurtosis:                       3.840   Cond. No.                         45.9\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors are robust to cluster correlation (cluster)\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:           peacefactorR   R-squared:                       0.135\n",
      "Model:                            OLS   Adj. R-squared:                  0.131\n",
      "Method:                 Least Squares   F-statistic:                     23.07\n",
      "Date:                Mon, 03 Mar 2025   Prob (F-statistic):           5.42e-24\n",
      "Time:                        10:36:34   Log-Likelihood:                -19.350\n",
      "No. Observations:                1276   AIC:                             52.70\n",
      "Df Residuals:                    1269   BIC:                             88.76\n",
      "Df Model:                           6                                         \n",
      "Covariance Type:              cluster                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept  -7.867e-16   4.41e-15     -0.178      0.858   -9.43e-15    7.86e-15\n",
      "femaleR       -0.2415      0.025     -9.522      0.000      -0.291      -0.192\n",
      "ageR          -0.0022      0.001     -2.934      0.003      -0.004      -0.001\n",
      "farmerR       -0.0407      0.029     -1.393      0.164      -0.098       0.017\n",
      "herderR        0.0262      0.040      0.661      0.509      -0.052       0.104\n",
      "pastvotedR    -0.0441      0.028     -1.585      0.113      -0.099       0.010\n",
      "hhsizeR        0.0013      0.002      0.628      0.530      -0.003       0.006\n",
      "==============================================================================\n",
      "Omnibus:                       78.209   Durbin-Watson:                   2.163\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              100.902\n",
      "Skew:                           0.558   Prob(JB):                     1.23e-22\n",
      "Kurtosis:                       3.807   Cond. No.                         45.7\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors are robust to cluster correlation (cluster)\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:        directlyharmedR   R-squared:                       0.018\n",
      "Model:                            OLS   Adj. R-squared:                  0.013\n",
      "Method:                 Least Squares   F-statistic:                     3.828\n",
      "Date:                Mon, 03 Mar 2025   Prob (F-statistic):           0.000970\n",
      "Time:                        10:36:34   Log-Likelihood:                -553.41\n",
      "No. Observations:                1276   AIC:                             1121.\n",
      "Df Residuals:                    1269   BIC:                             1157.\n",
      "Df Model:                           6                                         \n",
      "Covariance Type:              cluster                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept   6.102e-16   4.39e-15      0.139      0.889   -7.99e-15    9.21e-15\n",
      "femaleR       -0.0971      0.051     -1.894      0.058      -0.198       0.003\n",
      "ageR          -0.0012      0.001     -1.028      0.304      -0.003       0.001\n",
      "farmerR       -0.0028      0.043     -0.065      0.948      -0.087       0.081\n",
      "herderR        0.1228      0.051      2.425      0.015       0.024       0.222\n",
      "pastvotedR     0.0399      0.034      1.186      0.236      -0.026       0.106\n",
      "hhsizeR        0.0011      0.003      0.333      0.739      -0.005       0.008\n",
      "==============================================================================\n",
      "Omnibus:                       96.839   Durbin-Watson:                   2.274\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               54.362\n",
      "Skew:                           0.356   Prob(JB):                     1.57e-12\n",
      "Kurtosis:                       2.282   Cond. No.                         45.7\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors are robust to cluster correlation (cluster)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_predict, KFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import patsy\n",
    "import warnings\n",
    "from sklearn.base import BaseEstimator\n",
    "import statsmodels.formula.api as smf\n",
    "import scipy\n",
    "import hdmpy\n",
    "warnings.simplefilter('ignore')\n",
    "np.random.seed(1234)\n",
    "\n",
    "file = \"https://raw.githubusercontent.com/CausalAIBook/MetricsMLNotebooks/main/data/darfur.csv\"\n",
    "data = pd.read_csv(file)\n",
    "print(data.shape)\n",
    "\n",
    "# Preprocessing: take out village fixed effects and run basic linear analysis\n",
    "\n",
    "# Get rid of village fixed effects\n",
    "peacefactorR = smf.ols(formula=\"peacefactor ~ village\", data=data).fit().resid\n",
    "directlyharmedR = smf.ols(formula=\"directlyharmed ~ village\", data=data).fit().resid\n",
    "femaleR = smf.ols(formula=\"female ~ village\", data=data).fit().resid\n",
    "ageR = smf.ols(formula=\"age ~ village\", data=data).fit().resid\n",
    "farmerR = smf.ols(formula=\"farmer_dar ~ village\", data=data).fit().resid\n",
    "herderR = smf.ols(formula=\"herder_dar ~ village\", data=data).fit().resid\n",
    "pastvotedR = smf.ols(formula=\"pastvoted ~ village\", data=data).fit().resid\n",
    "hhsizeR = smf.ols(formula=\"hhsize_darfur ~ village\", data=data).fit().resid\n",
    "\n",
    "# Preliminary linear model analysis\n",
    "# Here we are clustering standard errors at the village level\n",
    "\n",
    "# Estimating the effect of the treatment, controlling linearly for other factors\n",
    "model1 = smf.ols(formula=\"peacefactorR ~ directlyharmedR + femaleR + ageR + farmerR + herderR + pastvotedR + hhsizeR\",\n",
    "                 data=data).fit(cov_type='cluster', cov_kwds={'groups': data['village']})\n",
    "print(model1.summary())\n",
    "\n",
    "# OLS model that predicts the outcome from controls\n",
    "model2 = smf.ols(formula=\"peacefactorR ~ femaleR + ageR + farmerR + herderR + pastvotedR + hhsizeR\",\n",
    "                 data=data).fit(cov_type='cluster', cov_kwds={'groups': data['village']})\n",
    "print(model2.summary())\n",
    "\n",
    "# OLS model that predicts the treatment from controls\n",
    "model3 = smf.ols(formula=\"directlyharmedR ~ femaleR + ageR + farmerR + herderR + pastvotedR + hhsizeR\",\n",
    "                 data=data).fit(cov_type='cluster', cov_kwds={'groups': data['village']})\n",
    "print(model3.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Controls explain the following fraction of variance of Outcome', 0.12510805487928756)\n",
      "('Controls explain the following fraction of variance of Treatment', 0.011984238217663812)\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                   resY   R-squared:                       0.023\n",
      "Model:                            OLS   Adj. R-squared:                  0.022\n",
      "Method:                 Least Squares   F-statistic:                     16.87\n",
      "Date:                Mon, 03 Mar 2025   Prob (F-statistic):           4.69e-05\n",
      "Time:                        10:36:34   Log-Likelihood:                -11.916\n",
      "No. Observations:                1276   AIC:                             27.83\n",
      "Df Residuals:                    1274   BIC:                             38.14\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:              cluster                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept   1.799e-10      0.000   1.13e-06      1.000      -0.000       0.000\n",
      "resD           0.1003      0.024      4.108      0.000       0.052       0.148\n",
      "==============================================================================\n",
      "Omnibus:                       77.408   Durbin-Watson:                   2.163\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               97.178\n",
      "Skew:                           0.567   Prob(JB):                     7.91e-22\n",
      "Kurtosis:                       3.736   Cond. No.                         2.67\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors are robust to cluster correlation (cluster)\n"
     ]
    }
   ],
   "source": [
    "import hdmpy\n",
    "\n",
    "\n",
    "class RLasso(BaseEstimator):\n",
    "\n",
    "    def __init__(self, *, post=True):\n",
    "        self.post = post\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.rlasso_ = hdmpy.rlasso(X, y, post=self.post)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array(X) @ np.array(self.rlasso_.est['beta']).flatten() + np.array(self.rlasso_.est['intercept'])\n",
    "\n",
    "\n",
    "def lasso_model():\n",
    "    return RLasso(post=False)\n",
    "\n",
    "Z = np.column_stack((femaleR, ageR, farmerR, herderR, pastvotedR, hhsizeR))\n",
    "Z = pd.DataFrame(Z, columns=['femaleR', 'ageR', 'farmerR', 'herderR', 'pastvotedR', 'hhsizeR'])\n",
    "# Interactions of 3 degrees\n",
    "controls = patsy.dmatrix('0 + (femaleR + ageR + farmerR + herderR + pastvotedR + hhsizeR)**3',\n",
    "                         Z, return_type='dataframe')\n",
    "\n",
    "resY = peacefactorR - lasso_model().fit(controls, peacefactorR).predict(controls)\n",
    "resD = directlyharmedR - lasso_model().fit(controls, directlyharmedR).predict(controls)\n",
    "print((\"Controls explain the following fraction of variance of Outcome\",\n",
    "       1 - np.var(resY) / np.var(peacefactorR)))\n",
    "print((\"Controls explain the following fraction of variance of Treatment\",\n",
    "       1 - np.var(resD) / np.var(directlyharmedR)))\n",
    "\n",
    "dml_data = pd.DataFrame({'resY': resY, 'resD': resD, 'village': data['village']})\n",
    "dml_model = smf.ols(formula=\"resY ~ resD\", data=dml_data).fit(cov_type='cluster',\n",
    "                                                              cov_kwds={'groups': dml_data['village']})\n",
    "print(dml_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a.)  \n",
    "\n",
    "The estimate of the causal effect of the treatment is 0.0973 and the CI is [0.051, 0.144]. The only control factors that were statistically significant at the 95% level in the second model were femaleR and ageR. The only control factor that was statistically significant at the 95% level in the third model was herderR .The estimate of the causal effect in the double lasso case is 0.1003 and the CI is [0.052, 0.148]. The result from double lasso is very similar to the one obtained from the first OLS regression, as is the confidence interval.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absolute value of the bias: 0.03244444888924153\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Main estimate\n",
    "beta = dml_model.params[1]\n",
    "\n",
    "# Hypothetical values of partial R2s\n",
    "R2_YC = 0.135\n",
    "R2_DC = 0.018\n",
    "\n",
    "# Elements of the bias equation\n",
    "kappa = (R2_YC * R2_DC) / (1 - R2_DC)\n",
    "variance_ratio = np.mean(dml_model.resid**2) / np.mean(resD**2)\n",
    "\n",
    "# Compute square bias\n",
    "BiasSq = kappa * variance_ratio\n",
    "\n",
    "# Compute absolute value of the bias\n",
    "print(\"absolute value of the bias:\", np.sqrt(BiasSq))\n",
    "\n",
    "# Plotting\n",
    "gridR2_DC = np.arange(0, 0.301, 0.001)\n",
    "gridR2_YC = kappa * (1 - gridR2_DC) / gridR2_DC\n",
    "gridR2_YC = np.where(gridR2_YC > 1, 1, gridR2_YC)\n",
    "\n",
    "# point where X= 0.1 \n",
    "plt.plot(0.1, kappa * (1 - 0.1) / 0.1, 'o', color='black')\n",
    "# put coordinates on plot\n",
    "plt.text(0.1, kappa * (1 - 0.1) / 0.1, f'({0.1}, {np.round(kappa * (1 - 0.1) / 0.1, decimals=4)})')\n",
    "# plot the curve\n",
    "plt.plot(gridR2_DC, gridR2_YC, color='red')\n",
    "plt.xlabel('Partial R2 of Treatment with Confounder')\n",
    "plt.ylabel('Partial R2 of Outcome with Confounder')\n",
    "plt.title(f'Combo of R2 such that |Bias| < {np.round(np.sqrt(BiasSq), decimals=4)}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b.) \n",
    "  \n",
    "The absolute value of the bias such a confounder may cause is 0.0324. Subtracting this from the lower bound of the CI of the double lasso model (0.052 - 0.0324) gives 0.0196, which is still positive. \n",
    "\n",
    "From the plot above, we can see that if the R-square for the treatment that the unobserved confounder further explains was 0.1, the R-square of the outcome should be approximately 0.02, so that the unobserved confounder can lead to the same amount of bias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No uncertainty: (0.06789199924405459, 0.13278089702253765)\n",
      "With uncertainty: (0.023034680259482576, 0.1775960026935741)\n",
      "Largest R2_DC: 0.05087100000003287\n",
      "Bounds with largest R2_DC: (-3.542817515467256e-07, 0.20063131319969985)\n"
     ]
    }
   ],
   "source": [
    "# packaging the bias analysis with some automated functions\n",
    "# We now automate the DML process and pass the estimates to functions to automate the sensitivity analysis. This is done in the R package sensmakr, which does not exist in python.\n",
    "def dml(X, D, y, modely, modeld, *, nfolds, classifier=False, cluster=True, clu=None):\n",
    "    '''\n",
    "    DML for the Partially Linear Model setting with cross-fitting\n",
    "\n",
    "    Input\n",
    "    -----\n",
    "    X: the controls\n",
    "    D: the treatment\n",
    "    y: the outcome\n",
    "    modely: the ML model for predicting the outcome y\n",
    "    modeld: the ML model for predicting the treatment D\n",
    "    nfolds: the number of folds in cross-fitting\n",
    "    classifier: bool, whether the modeld is a classifier or a regressor\n",
    "\n",
    "    clu: df column to cluster by\n",
    "    cluster: bool, whether to use clustered standard errors\n",
    "\n",
    "    Output\n",
    "    ------\n",
    "    point: the point estimate of the treatment effect of D on y\n",
    "    stderr: the standard error of the treatment effect\n",
    "    yhat: the cross-fitted predictions for the outcome y\n",
    "    Dhat: the cross-fitted predictions for the treatment D\n",
    "    resy: the outcome residuals\n",
    "    resD: the treatment residuals\n",
    "    epsilon: the final residual-on-residual OLS regression residual\n",
    "    '''\n",
    "\n",
    "    if nfolds > 1:\n",
    "        cv = KFold(n_splits=nfolds, shuffle=True, random_state=123)  # shuffled k-folds\n",
    "        yhat = cross_val_predict(modely, X, y, cv=cv, n_jobs=-1)  # out-of-fold predictions for y\n",
    "        # out-of-fold predictions for D\n",
    "        # use predict or predict_proba dependent on classifier or regressor for D\n",
    "        if classifier:\n",
    "            Dhat = cross_val_predict(modeld, X, D, cv=cv, method='predict_proba', n_jobs=-1)[:, 1]\n",
    "        else:\n",
    "            Dhat = cross_val_predict(modeld, X, D, cv=cv, n_jobs=-1)\n",
    "    elif nfolds == -1:\n",
    "        yhat = modely.fit(X, y).predict(X)\n",
    "        if classifier:\n",
    "            Dhat = modeld.fit(X, D).predict_proba(X)\n",
    "        else:\n",
    "            Dhat = modeld.fit(X, D).predict(X)\n",
    "\n",
    "    # calculate outcome and treatment residuals\n",
    "    resy = y - yhat\n",
    "    resD = D - Dhat\n",
    "\n",
    "    if cluster:\n",
    "        # final stage ols clustered\n",
    "        dml_data = pd.DataFrame({'resY': resY, 'resD': resD, 'cluster': clu})\n",
    "    else:\n",
    "        # final stage ols nonclustered\n",
    "        dml_data = pd.DataFrame({'resY': resY, 'resD': resD})\n",
    "\n",
    "    if cluster:\n",
    "        # clustered standard errors\n",
    "        ols_mod = smf.ols(formula='resY ~ 1 + resD', data=dml_data)\n",
    "        ols_mod = ols_mod.fit(cov_type='cluster', cov_kwds={\"groups\": dml_data['cluster']})\n",
    "    else:\n",
    "        # regular ols\n",
    "        ols_mod = smf.ols(formula='resY ~ 1 + resD', data=dml_data).fit()\n",
    "\n",
    "    point = ols_mod.params[1]\n",
    "    stderr = ols_mod.bse[1]\n",
    "    epsilon = ols_mod.resid\n",
    "\n",
    "    return point, stderr, yhat, Dhat, resy, resD, epsilon\n",
    "\n",
    "def summary(point, stderr, yhat, Dhat, resy, resD, epsilon, X, D, y, *, name):\n",
    "    '''\n",
    "    Convenience summary function that takes the results of the DML function\n",
    "    and summarizes several estimation quantities and performance metrics.\n",
    "    '''\n",
    "    return pd.DataFrame({'estimate': point,  # point estimate\n",
    "                         'stderr': stderr,  # standard error\n",
    "                         'lower': point - 1.96 * stderr,  # lower end of 95% confidence interval\n",
    "                         'upper': point + 1.96 * stderr,  # upper end of 95% confidence interval\n",
    "                         'rmse y': np.sqrt(np.mean(resy**2)),  # RMSE of model that predicts outcome y\n",
    "                         'rmse D': np.sqrt(np.mean(resD**2))  # RMSE of model that predicts treatment D\n",
    "                         }, index=[name])\n",
    "    \n",
    "def dml_sensitivity_bounds_single(res, eta_ysq, eta_asq, inds=None, return_stderr=False):\n",
    "    ''' Sensitivity analysis, specialized for the partially linear DML moment\n",
    "    E[(yres - theta * Tres) * Tres]. `est` is a `LinearDML` estimator fitted\n",
    "    with `cache_values=True` so that residuals are being stored after fitting.\n",
    "\n",
    "    Input\n",
    "    -----\n",
    "    res : tuple (yres, Tres)\n",
    "        outcome and treatment residuals from the DML procedure\n",
    "    eta_ysq : float\n",
    "        posited partial R^2 for the outcome, that the unobserved confounder explains\n",
    "    eta_asq : float\n",
    "        posited partial R^2 for the treatment, that the unobserved confounder explains\n",
    "    inds : list of int or None, optional (default=None)\n",
    "        subset of samples to use for this analysis; useful for sub-population analysis\n",
    "    return_stderr : bool, optional (default=False)\n",
    "        whether to return standard errors for the lower and upper bound\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    lower : lower bound of confidence set\n",
    "    lower_stderr : standard error for the lower bound (not returned if `return_stderr=False`)\n",
    "    upper : upper bound of confidence set\n",
    "    upper_stderr : standard error for the upper bound (not returned if `return_stderr=False`)\n",
    "    '''\n",
    "    if inds is None:\n",
    "        inds = np.arange(res[0].shape[0])\n",
    "    yres, Tres = res\n",
    "    yres, Tres = yres[inds], Tres[inds]\n",
    "    nusq = np.mean(Tres ** 2)\n",
    "    theta = np.mean(yres * Tres) / nusq\n",
    "    sigmasq = np.mean((yres - Tres * theta)**2)\n",
    "    S = np.sqrt(sigmasq / nusq)\n",
    "    Casq = eta_asq / (1 - eta_asq)\n",
    "    Cgsq = eta_ysq\n",
    "    error = S * np.sqrt(Casq * Cgsq)\n",
    "\n",
    "    if not return_stderr:\n",
    "        return theta - error, theta + error\n",
    "\n",
    "    psi_theta = (yres - Tres * theta) * Tres / nusq\n",
    "    psi_sigmasq = (yres - Tres * theta)**2 - sigmasq\n",
    "    psi_nusq = Tres**2 - nusq\n",
    "\n",
    "    phi_plus = psi_theta\n",
    "    phi_plus += (np.sqrt(Casq * Cgsq) / (2 * S)) * (-(sigmasq / (nusq**2)) * psi_nusq + (1 / nusq) * psi_sigmasq)\n",
    "    stderr_plus = np.sqrt(np.mean(phi_plus**2) / phi_plus.shape[0])\n",
    "\n",
    "    phi_minus = psi_theta\n",
    "    phi_minus -= (np.sqrt(Casq * Cgsq) / (2 * S)) * (-(sigmasq / (nusq**2)) * psi_nusq + (1 / nusq) * psi_sigmasq)\n",
    "    stderr_minus = np.sqrt(np.mean(phi_minus**2) / phi_minus.shape[0])\n",
    "    return theta - error, stderr_minus, theta + error, stderr_plus\n",
    "\n",
    "# If we want reduce the uncertainty from sample splitting, we can re-run our nfolds cross-fitting and aggregate with the median estimates. With 10 folds as above, there shouldn't be too much variance anyway, but we employ the following aggregation procedure anyway\n",
    "\n",
    "def dml_sensitivity_bounds(res_list, eta_ysq, eta_asq, alpha=None, inds=None):\n",
    "    ''' Sensitivity analysis, specialized for the partially linear DML moment\n",
    "    E[(yres - theta * Tres) * Tres]. `est` is a `LinearDML` estimator fitted\n",
    "    with `cache_values=True` so that residuals are being stored after fitting.\n",
    "\n",
    "    Input\n",
    "    -----\n",
    "    res_list : list of tuples (yres, Tres), or single tuple (yres, Tres)\n",
    "        list of (outcome and treatment residuals from the DML procedure) on different\n",
    "        random cross-fitting folds\n",
    "    eta_ysq : float\n",
    "        posited partial R^2 for the outcome, that the unobserved confounder explains\n",
    "    eta_asq : float\n",
    "        posited partial R^2 for the treatment, that the unobserved confounder explains\n",
    "    alpha : float or None, optional (default=None)\n",
    "        confidence level for confidence interval; if None, no uncertainty is incoprorated\n",
    "    inds : list of int or None, optional (default=None)\n",
    "        subset of samples to use for this analysis; useful for sub-population analysis\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    lower : lower bound of confidence set, incorporating uncertainty\n",
    "    upper : upper bound of confidence set, incorporating uncertainty\n",
    "    '''\n",
    "    if not isinstance(res_list, list):\n",
    "        res_list = [res_list]\n",
    "\n",
    "    if alpha is None:\n",
    "        lower, upper = zip(*[dml_sensitivity_bounds_single(res, eta_ysq, eta_asq, inds=inds, return_stderr=False)\n",
    "                             for res in res_list])\n",
    "        return np.median(lower), np.median(upper)\n",
    "    else:\n",
    "        lower, std_lower, upper, std_upper = zip(*[dml_sensitivity_bounds_single(res, eta_ysq, eta_asq,\n",
    "                                                                                 inds=inds, return_stderr=True)\n",
    "                                                   for res in res_list])\n",
    "        std_lower = np.array(std_lower)\n",
    "        std_upper = np.array(std_upper)\n",
    "        lower = np.median(lower)\n",
    "        lower -= scipy.stats.norm.ppf(1 - alpha / 2) * np.sqrt(np.median(std_lower**2) + np.var(lower))\n",
    "        upper = np.median(upper)\n",
    "        upper += scipy.stats.norm.ppf(1 - alpha / 2) * np.sqrt(np.median(std_upper**2) + np.var(upper))\n",
    "        return lower, upper\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "result_no_uncertainty = dml_sensitivity_bounds((resY, resD), R2_YC, R2_DC)\n",
    "print(\"No uncertainty:\", result_no_uncertainty)\n",
    "\n",
    "result_with_uncertainty = dml_sensitivity_bounds((resY, resD), R2_YC, R2_DC, alpha=0.05)\n",
    "print(\"With uncertainty:\", result_with_uncertainty)\n",
    "\n",
    "# Now we find the largest possible R2_DC that would make the bounds cross zero\n",
    "R2_DC_largest = R2_DC\n",
    "while dml_sensitivity_bounds((resY, resD), R2_YC, R2_DC_largest, alpha=0.05)[0] > 0:\n",
    "    R2_DC_largest += 0.000001\n",
    "print(\"Largest R2_DC:\", R2_DC_largest)\n",
    "print(\"Bounds with largest R2_DC:\", dml_sensitivity_bounds((resY, resD), R2_YC, R2_DC_largest, alpha=0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incorporating uncertainty does not overturn the results.  The largest Rsquared of the treatment needed to overturn the results is 0.05087100000003287. \n",
    "\n",
    "### Assignment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12959.470227125952\n",
      "---------- PLIV Table for all data ----------\n",
      "                        estimate       stderr        lower         upper  \\\n",
      "double lasso        12805.275717  1846.632406  9185.876202  16424.675232   \n",
      "lasso/logistic      12517.090908  1843.052161  8904.708673  16129.473143   \n",
      "random forest       13032.853319  1968.584550  9174.427602  16891.279037   \n",
      "decision tree       13582.990088  2140.589084  9387.435484  17778.544691   \n",
      "boosted forest      12957.719513  1944.067234  9147.347734  16768.091292   \n",
      "automl (semi-cfit)  13244.833183  1882.920533  9554.308938  16935.357428   \n",
      "\n",
      "                          rmse y    rmse D    rmse Z  accuracy D  accuracy Z  \n",
      "double lasso        54111.994234  0.414217  0.443826    0.745335    0.688048  \n",
      "lasso/logistic      54111.994234  0.414706  0.444196    0.746041    0.686334  \n",
      "random forest       55233.696123  0.415458  0.444519    0.744125    0.690469  \n",
      "decision tree       59843.870082  0.416830  0.447080    0.738477    0.678467  \n",
      "boosted forest      56016.623262  0.414945  0.444045    0.743621    0.690973  \n",
      "automl (semi-cfit)  54004.857132  0.415159  0.444057    0.745335    0.693394  \n",
      "robust region pliv: 9556.955695569557 to 16937.693769376936\n",
      "t-statistic pliv 87.57673416377116\n",
      "---------- IIV Table for all data ----------\n",
      "                        estimate       stderr        lower         upper  \\\n",
      "lasso/logistic      11161.211885  1633.059990  7960.414304  14362.009466   \n",
      "random forest       11346.068560  1674.930908  8063.203981  14628.933139   \n",
      "decision trees      10085.238705  1830.646014  6497.172518  13673.304891   \n",
      "boosted trees       12294.093912  1670.344411  9020.218867  15567.968957   \n",
      "automl (semi-cfit)  12130.623259  1615.710927  8963.829841  15297.416677   \n",
      "\n",
      "                          rmse y    rmse D    rmse Z  accuracy D  accuracy Z  \n",
      "lasso/logistic      54023.763725  0.277216  0.444196    0.890267    0.686334  \n",
      "random forest       55451.227646  0.275159  0.444789    0.889763    0.691982  \n",
      "decision trees      60264.950900  0.295399  0.447080    0.865759    0.678467  \n",
      "boosted trees       55944.725795  0.274018  0.443794    0.890166    0.693596  \n",
      "automl (semi-cfit)  55707.837230  0.278216  0.444057    0.890267    0.693394  \n",
      "robust region iiv: 8964.896489648963 to 15297.529752975297\n",
      "5817.632191564213\n",
      "---------- PLIV Table for bottom quartile of income ----------\n",
      "                       estimate       stderr        lower        upper  \\\n",
      "double lasso        5768.455193  1701.445143  2433.622713  9103.287673   \n",
      "lasso/logistic      5800.147138  1633.313737  2598.852214  9001.442062   \n",
      "random forest       6678.572922  1691.700213  3362.840505  9994.305339   \n",
      "decision tree       5813.283960  1664.505244  2550.853682  9075.714239   \n",
      "boosted forest      6096.840883  1672.116052  2819.493422  9374.188345   \n",
      "automl (semi-cfit)  5783.401566  1681.594150  2487.477032  9079.326099   \n",
      "\n",
      "                          rmse y    rmse D    rmse Z  accuracy D  accuracy Z  \n",
      "double lasso        13446.444167  0.292899  0.342700    0.901130    0.847054  \n",
      "lasso/logistic      13446.444167  0.298585  0.358910    0.901130    0.847054  \n",
      "random forest       13496.468384  0.293531  0.344563    0.901130    0.847458  \n",
      "decision tree       14288.351637  0.310102  0.364139    0.876110    0.811945  \n",
      "boosted forest      13246.353522  0.293252  0.342939    0.901130    0.846651  \n",
      "automl (semi-cfit)  13206.884784  0.296039  0.345376    0.901533    0.844633  \n",
      "robust region pliv: 2470.24702470247 to 9098.909890989098\n",
      "t-statistic pliv 25.87841474167428\n",
      "---------- IIV Table for bottom quartile of income ----------\n",
      "                       estimate       stderr        lower        upper  \\\n",
      "lasso/logistic      6326.723961  1604.227461  3182.438138  9471.009785   \n",
      "random forest       5886.598705  1620.823024  2709.785579  9063.411832   \n",
      "decision trees      1336.686235  4105.702670 -6710.490998  9383.863467   \n",
      "boosted trees       6353.205818  1605.344181  3206.731223  9499.680413   \n",
      "automl (semi-cfit)  6424.161834  1526.095257  3433.015130  9415.308538   \n",
      "\n",
      "                          rmse y    rmse D    rmse Z  accuracy D  accuracy Z  \n",
      "lasso/logistic      13428.803538  0.188321  0.358910    0.945117    0.847054  \n",
      "random forest       13444.725399  0.189482  0.344783    0.947538    0.846247  \n",
      "decision trees      14817.208028  0.217167  0.364131    0.932607    0.811945  \n",
      "boosted trees       13792.983474  0.189122  0.343134    0.944713    0.843826  \n",
      "automl (semi-cfit)  13324.476164  0.187859  0.345376    0.945924    0.844633  \n",
      "robust region iiv: 3438.343834383438 to 9434.943494349434\n",
      "23084.25263650146\n",
      "---------- PLIV Table for top quartile of income ----------\n",
      "                        estimate       stderr         lower         upper  \\\n",
      "double lasso        23577.436393  5048.340429  13682.689152  33472.183634   \n",
      "lasso/logistic      23077.897304  5048.927467  13181.999469  32973.795139   \n",
      "random forest       22926.894556  5302.655159  12533.690444  33320.098668   \n",
      "decision tree       19005.144704  6849.014274   5581.076726  32429.212681   \n",
      "boosted forest      23754.208260  5024.222963  13906.731252  33601.685267   \n",
      "automl (semi-cfit)  21960.841510  4899.310047  12358.193818  31563.489202   \n",
      "\n",
      "                           rmse y    rmse D    rmse Z  accuracy D  accuracy Z  \n",
      "double lasso         91149.933462  0.487619  0.483549    0.599435    0.591367  \n",
      "lasso/logistic       91149.933462  0.487524  0.482727    0.593788    0.602259  \n",
      "random forest        94895.977359  0.491968  0.484484    0.577249    0.601049  \n",
      "decision tree       100253.218084  0.557189  0.551253    0.524002    0.530859  \n",
      "boosted forest       94999.839983  0.490972  0.483791    0.577652    0.603873  \n",
      "automl (semi-cfit)   94456.634046  0.489290  0.484906    0.581686    0.599839  \n",
      "robust region pliv: 12357.235723572356 to 20000.0\n",
      "t-statistic pliv 66.61531863775544\n",
      "---------- IIV Table for top quartile of income ----------\n",
      "                        estimate        stderr          lower         upper  \\\n",
      "lasso/logistic      23227.812526   4996.492775   13434.686687  33020.938366   \n",
      "random forest       22883.084993   5296.939296   12501.083974  33265.086012   \n",
      "decision trees     -44271.264938  62307.793639 -166394.540471  77852.010595   \n",
      "boosted trees       22875.686982   5134.052247   12812.944579  32938.429386   \n",
      "automl (semi-cfit)  23182.446314   4851.886585   13672.748608  32692.144020   \n",
      "\n",
      "                           rmse y    rmse D    rmse Z  accuracy D  accuracy Z  \n",
      "lasso/logistic       91267.974544  0.317597  0.482727    0.868495    0.602259  \n",
      "random forest        97242.220092  0.319603  0.484865    0.868092    0.597015  \n",
      "decision trees      102480.931499  0.372686  0.551732    0.798709    0.527632  \n",
      "boosted trees        98504.088938  0.315865  0.484043    0.868899    0.601856  \n",
      "automl (semi-cfit)   95091.697374  0.318604  0.485583    0.868899    0.597822  \n",
      "robust region iiv: 13671.36713671367 to 20000.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_predict, KFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LassoCV, LogisticRegressionCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "import scipy.stats\n",
    "import warnings\n",
    "from sklearn.base import BaseEstimator, clone\n",
    "from sklearn.base import TransformerMixin\n",
    "from formulaic import Formula\n",
    "warnings.simplefilter('ignore')\n",
    "import os\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
    "np.random.seed(1234)\n",
    "\n",
    "file = \"https://raw.githubusercontent.com/CausalAIBook/MetricsMLNotebooks/main/data/401k.csv\"\n",
    "data_all = pd.read_csv(file)\n",
    "data_bottom = data_all.loc[data_all['inc'] < data_all['inc'].quantile(0.25)].copy()\n",
    "data_top = data_all.loc[data_all['inc'] > data_all['inc'].quantile(0.75)].copy()\n",
    "\n",
    "class FormulaTransformer(TransformerMixin, BaseEstimator):\n",
    "\n",
    "    def __init__(self, formula, array=False):\n",
    "        self.formula = formula\n",
    "        self.array = array\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        df = Formula(self.formula).get_model_matrix(X)\n",
    "        if self.array:\n",
    "            return df.values\n",
    "        return df\n",
    "    \n",
    "transformer = FormulaTransformer(\"0 + poly(age, degree=6, raw=True) + poly(inc, degree=8, raw=True) \"\n",
    "                                 \"+ poly(educ, degree=4, raw=True) + poly(fsize, degree=2, raw=True) \"\n",
    "                                 \"+ male + marr + twoearn + db + pira + hown\", array=True)\n",
    "\n",
    "descs = [\"all data\", \"bottom quartile of income\", \"top quartile of income\"]\n",
    "def dml(X, Z, D, y, modely, modeld, modelz, *, nfolds, classifier=False):\n",
    "    '''\n",
    "    DML for the Partially Linear Model setting with cross-fitting\n",
    "\n",
    "    Input\n",
    "    -----\n",
    "    X: the controls\n",
    "    Z: the instrument\n",
    "    D: the treatment\n",
    "    y: the outcome\n",
    "    modely: the ML model for predicting the outcome y\n",
    "    modeld: the ML model for predicting the treatment D\n",
    "    modelz: the ML model for predicting the instrument Z\n",
    "    nfolds: the number of folds in cross-fitting\n",
    "    classifier: bool, whether the modeld is a classifier or a regressor\n",
    "\n",
    "    Output\n",
    "    ------\n",
    "    point: the point estimate of the treatment effect of D on y\n",
    "    stderr: the standard error of the treatment effect\n",
    "    yhat: the cross-fitted predictions for the outcome y\n",
    "    Dhat: the cross-fitted predictions for the treatment D\n",
    "    Zhat: the cross-fitted predictions for the instrument Z\n",
    "    resy: the outcome residuals\n",
    "    resD: the treatment residuals\n",
    "    resZ: the instrument residuals\n",
    "    epsilon: the final residual-on-residual OLS regression residual\n",
    "    '''\n",
    "    cv = KFold(n_splits=nfolds, shuffle=True, random_state=123)  # shuffled k-folds\n",
    "    yhat = cross_val_predict(modely, X, y, cv=cv, n_jobs=-1)  # out-of-fold predictions for y\n",
    "    # out-of-fold predictions for D\n",
    "    # use predict or predict_proba dependent on classifier or regressor for D\n",
    "    if classifier:\n",
    "        Dhat = cross_val_predict(modeld, X, D, cv=cv, method='predict_proba', n_jobs=-1)[:, 1]\n",
    "        Zhat = cross_val_predict(modelz, X, Z, cv=cv, method='predict_proba', n_jobs=-1)[:, 1]\n",
    "    else:\n",
    "        Dhat = cross_val_predict(modeld, X, D, cv=cv, n_jobs=-1)\n",
    "        Zhat = cross_val_predict(modelz, X, Z, cv=cv, n_jobs=-1)\n",
    "    # calculate outcome and treatment residuals\n",
    "    resy = y - yhat\n",
    "    resD = D - Dhat\n",
    "    resZ = Z - Zhat\n",
    "    # final stage ols based point estimate and standard error\n",
    "    point = np.mean(resy * resZ) / np.mean(resD * resZ)\n",
    "    epsilon = resy - point * resD\n",
    "    var = np.mean(epsilon**2 * resZ**2) / np.mean(resD * resZ)**2\n",
    "    stderr = np.sqrt(var / X.shape[0])\n",
    "    return point, stderr, yhat, Dhat, Zhat, resy, resD, resZ, epsilon\n",
    "\n",
    "\n",
    "def iiv(X, Z, D, y, modely0, modely1, modeld1, modeld0, modelz, *, trimming=0.01, nfolds):\n",
    "    '''\n",
    "    DML for the Interactive IV Model setting with cross-fitting\n",
    "\n",
    "    Input\n",
    "    -----\n",
    "    X: the controls\n",
    "    D: the treatment\n",
    "    y: the outcome\n",
    "    modely0: the ML model for predicting the outcome y in the Z=0 population\n",
    "    modely1: the ML model for predicting the outcome y in the Z=1 population\n",
    "    modeld0: the ML model for predicting the treatment D in the Z=0 population\n",
    "    modeld1: the ML model for predicting the treatment D in the Z=1 population\n",
    "    modelz: the ML model for predicting the instrument Z\n",
    "    trimming: threshold below which to trim propensities\n",
    "    nfolds: the number of folds in cross-fitting\n",
    "\n",
    "    Output\n",
    "    ------\n",
    "    point: the point estimate of the treatment effect of D on y\n",
    "    stderr: the standard error of the treatment effect\n",
    "    yhat: the cross-fitted predictions for the outcome y\n",
    "    Dhat: the cross-fitted predictions for the outcome D\n",
    "    resy: the outcome residuals\n",
    "    resD: the treatment residuals\n",
    "    drhat: the doubly robust quantity for each sample\n",
    "    '''\n",
    "    cv = KFold(n_splits=nfolds, shuffle=True, random_state=123)\n",
    "\n",
    "    yhat0, yhat1 = np.zeros(y.shape), np.zeros(y.shape)\n",
    "    Dhat0, Dhat1 = np.zeros(D.shape), np.zeros(D.shape)\n",
    "\n",
    "    # we will fit a model E[Y| D, X] by fitting a separate model for D==0\n",
    "    # and a separate model for D==1.\n",
    "    for train, test in cv.split(X, y):\n",
    "        # train an outcome model on training data that received Z=0 and predict outcome on all data in test set\n",
    "        yhat0[test] = clone(modely0).fit(X.iloc[train][Z[train] == 0], y[train][Z[train] == 0]).predict(X.iloc[test])\n",
    "        # train an outcome model on training data that received Z=1 and predict outcome on all data in test set\n",
    "        yhat1[test] = clone(modely1).fit(X.iloc[train][Z[train] == 1], y[train][Z[train] == 1]).predict(X.iloc[test])\n",
    "        # train a treatment model on training data that received Z=0 and predict treatment on all data in test set\n",
    "        if np.mean(D[train][Z[train] == 0]) > 0:  # it could be that D=0, whenever Z=0 deterministically\n",
    "            modeld0_ = clone(modeld0).fit(X.iloc[train][Z[train] == 0], D[train][Z[train] == 0])\n",
    "            Dhat0[test] = modeld0_.predict_proba(X.iloc[test])[:, 1]\n",
    "        # train a treamtent model on training data that received Z=1 and predict treatment on all data in test set\n",
    "        if np.mean(D[train][Z[train] == 1]) < 1:  # it could be that D=1, whenever Z=1 deterministically\n",
    "            modeld1_ = clone(modeld1).fit(X.iloc[train][Z[train] == 1], D[train][Z[train] == 1])\n",
    "            Dhat1[test] = modeld1_.predict_proba(X.iloc[test])[:, 1]\n",
    "        else:\n",
    "            Dhat1[test] = 1\n",
    "\n",
    "    # prediction of treatment and outcome for observed instrument\n",
    "    yhat = yhat0 * (1 - Z) + yhat1 * Z\n",
    "    Dhat = Dhat0 * (1 - Z) + Dhat1 * Z\n",
    "    # propensity scores\n",
    "    Zhat = cross_val_predict(modelz, X, Z, cv=cv, method='predict_proba', n_jobs=-1)[:, 1]\n",
    "    Zhat = np.clip(Zhat, trimming, 1 - trimming)\n",
    "    # doubly robust quantity for every sample\n",
    "    HZ = Z / Zhat - (1 - Z) / (1 - Zhat)\n",
    "    drZ = yhat1 - yhat0 + (y - yhat) * HZ\n",
    "    drD = Dhat1 - Dhat0 + (D - Dhat) * HZ\n",
    "    point = np.mean(drZ) / np.mean(drD)\n",
    "    psi = drZ - point * drD\n",
    "    Jhat = np.mean(drD)\n",
    "    var = np.mean(psi**2) / Jhat**2\n",
    "    stderr = np.sqrt(var / X.shape[0])\n",
    "    return point, stderr, yhat, Dhat, Zhat, y - yhat, D - Dhat, Z - Zhat, drZ, drD\n",
    "\n",
    "\n",
    "def summary(point, stderr, yhat, Dhat, Zhat, resy, resD, resZ, epsilon, X, Z, D, y, *, name):\n",
    "    '''\n",
    "    Convenience summary function that takes the results of the DML function\n",
    "    and summarizes several estimation quantities and performance metrics.\n",
    "    '''\n",
    "    return pd.DataFrame({'estimate': point,  # point estimate\n",
    "                         'stderr': stderr,  # standard error\n",
    "                         'lower': point - 1.96 * stderr,  # lower end of 95% confidence interval\n",
    "                         'upper': point + 1.96 * stderr,  # upper end of 95% confidence interval\n",
    "                         'rmse y': np.sqrt(np.mean(resy**2)),  # RMSE of model that predicts outcome y\n",
    "                         'rmse D': np.sqrt(np.mean(resD**2)),  # RMSE of model that predicts treatment D\n",
    "                         'rmse Z': np.sqrt(np.mean(resZ**2)),  # RMSE of model that predicts treatment D\n",
    "                         'accuracy D': np.mean(np.abs(resD) < .5),  # binary classification accuracy of model for D\n",
    "                         'accuracy Z': np.mean(np.abs(resZ) < .5),  # binary classification accuracy of model for Z\n",
    "                         }, index=[name])\n",
    "    \n",
    "def summary_iiv(point, stderr, yhat, Dhat, Zhat, resy, resD, resZ, drZ, drD, X, Z, D, y, *, name):\n",
    "    '''\n",
    "    Convenience summary function that takes the results of the DML function\n",
    "    and summarizes several estimation quantities and performance metrics.\n",
    "    '''\n",
    "    return pd.DataFrame({'estimate': point,  # point estimate\n",
    "                         'stderr': stderr,  # standard error\n",
    "                         'lower': point - 1.96 * stderr,  # lower end of 95% confidence interval\n",
    "                         'upper': point + 1.96 * stderr,  # upper end of 95% confidence interval\n",
    "                         'rmse y': np.sqrt(np.mean(resy**2)),  # RMSE of model that predicts outcome y\n",
    "                         'rmse D': np.sqrt(np.mean(resD**2)),  # RMSE of model that predicts treatment D\n",
    "                         'rmse Z': np.sqrt(np.mean(resZ**2)),  # RMSE of model that predicts treatment D\n",
    "                         'accuracy D': np.mean(np.abs(resD) < .5),  # binary classification accuracy of model for D\n",
    "                         'accuracy Z': np.mean(np.abs(resZ) < .5),  # binary classification accuracy of model for Z\n",
    "                         }, index=[name])\n",
    "    \n",
    "def robust_inference(point, stderr, yhat, Dhat, Zhat, resy, resD, resZ, epsilon, X, Z, D, y, *, grid, alpha=0.05):\n",
    "    '''\n",
    "    Inference in the partially linear IV model that is robust to weak identification.\n",
    "    grid: grid of theta values to search over when trying to identify the confidence region\n",
    "    alpha: confidence level\n",
    "    '''\n",
    "    n = X.shape[0]\n",
    "    thr = scipy.stats.chi2.ppf(1 - alpha, df=1)\n",
    "    accept = []\n",
    "    for theta in grid:\n",
    "        moment = (resy - theta * resD) * resZ\n",
    "        test = n * np.mean(moment)**2 / np.var(moment)\n",
    "        if test <= thr:\n",
    "            accept.append(theta)\n",
    "    return accept\n",
    "    \n",
    "    \n",
    "def iivm_robust_inference(point, stderr, yhat, Dhat, Zhat, resy, resD, resZ, drZ, drD, X, Z, D, y, *,\n",
    "                          grid, alpha=0.05):\n",
    "    '''\n",
    "    Inference in the partially linear IV model that is robust to weak identification.\n",
    "    grid: grid of theta values to search over when trying to identify the confidence region\n",
    "    alpha: confidence level\n",
    "    '''\n",
    "    n = X.shape[0]\n",
    "    thr = scipy.stats.chi2.ppf(1 - alpha, df=1)\n",
    "    accept = []\n",
    "    for theta in grid:\n",
    "        moment = drZ - theta * drD\n",
    "        test = n * np.mean(moment)**2 / np.var(moment)\n",
    "        if test <= thr:\n",
    "            accept.append(theta)\n",
    "    return accept\n",
    "\n",
    "\n",
    "for i, dat in enumerate([data_all, data_bottom, data_top]):\n",
    "    y = dat['net_tfa'].values\n",
    "    Z = dat['e401'].values\n",
    "    D = dat['p401'].values\n",
    "    X = dat.drop(['e401', 'p401', 'a401', 'tw', 'tfa', 'net_tfa', 'tfa_he',\n",
    "                'hval', 'hmort', 'hequity',\n",
    "                'nifa', 'net_nifa', 'net_n401', 'ira',\n",
    "                'dum91', 'icat', 'ecat', 'zhat',\n",
    "                'i1', 'i2', 'i3', 'i4', 'i5', 'i6', 'i7',\n",
    "                'a1', 'a2', 'a3', 'a4', 'a5'], axis=1)\n",
    "    \n",
    "    # double ML IV under partial linearity\n",
    "    modely = make_pipeline(transformer, StandardScaler(), LassoCV())\n",
    "    modeld = make_pipeline(transformer, StandardScaler(), LassoCV())\n",
    "    modelz = make_pipeline(transformer, StandardScaler(), LassoCV())\n",
    "    resy = y - modely.fit(X, y).predict(X)\n",
    "    resZ = Z - modelz.fit(X, Z).predict(X)  # instrument is e401k (eligibility)\n",
    "    resD = D - modeld.fit(X, D).predict(X)  # treatment is p401k (participation)\n",
    "    print(np.mean(resy * resZ) / np.mean(resD * resZ))\n",
    "    \n",
    "    # DML with Non-Linear ML Models and Cross-Fitting\n",
    "    \n",
    "    # double lasso with cross-fitting\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "    lassoy = make_pipeline(transformer, StandardScaler(), LassoCV(cv=cv))\n",
    "    lassod = make_pipeline(transformer, StandardScaler(), LassoCV(cv=cv))\n",
    "    lassoz = make_pipeline(transformer, StandardScaler(), LassoCV(cv=cv))\n",
    "    result = dml(X, Z, D, y, lassoy, lassod, lassoz, nfolds=3)\n",
    "    table = summary(*result, X, Z, D, y, name='double lasso')\n",
    "    \n",
    "    # penalized logreg for D\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "    lassoy = make_pipeline(transformer, StandardScaler(), LassoCV(cv=cv))\n",
    "    lgrd = make_pipeline(transformer, StandardScaler(), LogisticRegressionCV(cv=cv))\n",
    "    lgrz = make_pipeline(transformer, StandardScaler(), LogisticRegressionCV(cv=cv))\n",
    "    result = dml(X, Z, D, y, lassoy, lgrd, lgrz, nfolds=3, classifier=True)\n",
    "    table = pd.concat([table, summary(*result, X, Z, D, y, name='lasso/logistic')])\n",
    "    \n",
    "    # Random Forests \n",
    "    rfy = make_pipeline(transformer, RandomForestRegressor(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001))\n",
    "    rfd = make_pipeline(transformer, RandomForestClassifier(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001))\n",
    "    rfz = make_pipeline(transformer, RandomForestClassifier(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001))\n",
    "    result = dml(X, Z, D, y, rfy, rfd, rfz, nfolds=3, classifier=True)\n",
    "    table = pd.concat([table, summary(*result, X, Z, D, y, name='random forest')])\n",
    "    \n",
    "    # Decision Trees\n",
    "    dtry = make_pipeline(transformer, DecisionTreeRegressor(min_samples_leaf=10, ccp_alpha=.001))\n",
    "    dtrd = make_pipeline(transformer, DecisionTreeClassifier(min_samples_leaf=10, ccp_alpha=.001))\n",
    "    dtrz = make_pipeline(transformer, DecisionTreeClassifier(min_samples_leaf=10, ccp_alpha=.001))\n",
    "    result = dml(X, Z, D, y, dtry, dtrd, dtrz, nfolds=3, classifier=True)\n",
    "    table = pd.concat([table, summary(*result, X, Z, D, y, name='decision tree')])\n",
    "    \n",
    "    # Boosted Trees\n",
    "    gbfy = make_pipeline(transformer, GradientBoostingRegressor(max_depth=2, n_iter_no_change=5))\n",
    "    gbfd = make_pipeline(transformer, GradientBoostingClassifier(max_depth=2, n_iter_no_change=5))\n",
    "    gbfz = make_pipeline(transformer, GradientBoostingClassifier(max_depth=2, n_iter_no_change=5))\n",
    "    result = dml(X, Z, D, y, gbfy, gbfd, gbfz, nfolds=3, classifier=True)\n",
    "    table = pd.concat([table, summary(*result, X, Z, D, y, name='boosted forest')])\n",
    "    \n",
    "    # semi crossfitting with AutoML\n",
    "    from flaml import AutoML\n",
    "    flamly = make_pipeline(transformer, AutoML(time_budget=100, task='regression', early_stop=True,\n",
    "                                            eval_method='cv', n_splits=3, metric='r2', verbose=0))\n",
    "    flamld = make_pipeline(transformer, AutoML(time_budget=100, task='classification', early_stop=True,\n",
    "                                            eval_method='cv', n_splits=3, metric='r2', verbose=0))\n",
    "    flamlz = make_pipeline(transformer, AutoML(time_budget=100, task='classification', early_stop=True,\n",
    "                                            eval_method='cv', n_splits=3, metric='r2', verbose=0))\n",
    "    flamly.fit(X, y)\n",
    "    besty = make_pipeline(transformer, clone(flamly[-1].best_model_for_estimator(flamly[-1].best_estimator)))\n",
    "    flamld.fit(X, D)\n",
    "    bestd = make_pipeline(transformer, clone(flamld[-1].best_model_for_estimator(flamld[-1].best_estimator)))\n",
    "    flamlz.fit(X, Z)\n",
    "    bestz = make_pipeline(transformer, clone(flamlz[-1].best_model_for_estimator(flamlz[-1].best_estimator)))\n",
    "    result = dml(X, Z, D, y, besty, bestd, bestz, nfolds=3, classifier=True)\n",
    "    table = pd.concat([table, summary(*result, X, Z, D, y, name='automl (semi-cfit)')])\n",
    "    print(f\"---------- PLIV Table for {descs[i]} ----------\")\n",
    "    print(table)\n",
    "    # inference robust to weak identification\n",
    "    region = robust_inference(*result, X, Z, D, y, grid=np.linspace(0, 20000, 10000))\n",
    "    print(f\"robust region pliv: {np.min(region)} to {np.max(region)}\")\n",
    "    beta = np.mean(resZ * resD) / np.mean(resZ**2)\n",
    "    var_beta = np.mean((resD - beta * resZ)**2 * resZ**2) / np.mean(resZ**2)**2\n",
    "    se_beta = np.sqrt(var_beta / resD.shape[0])\n",
    "    print(f\"t-statistic pliv {np.abs(beta / se_beta)}\")\n",
    "    \n",
    "    # interactive IV Model and LATE\n",
    "    \n",
    "    # lasso logistic\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "    lassoy = make_pipeline(transformer, StandardScaler(), LassoCV(cv=cv))\n",
    "    lgrd = make_pipeline(transformer, StandardScaler(), LogisticRegressionCV(cv=cv))\n",
    "    lgrz = make_pipeline(transformer, StandardScaler(), LogisticRegressionCV(cv=cv))\n",
    "    result = iiv(X, Z, D, y, lassoy, lassoy, lgrd, lgrd, lgrz, nfolds=3)\n",
    "    tableiiv = summary_iiv(*result, X, Z, D, y, name='lasso/logistic')\n",
    "    \n",
    "    # random forest\n",
    "    rfy = make_pipeline(transformer, RandomForestRegressor(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001))\n",
    "    rfd = make_pipeline(transformer, RandomForestClassifier(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001))\n",
    "    rfz = make_pipeline(transformer, RandomForestClassifier(n_estimators=100, min_samples_leaf=10, ccp_alpha=.001))\n",
    "    result = iiv(X, Z, D, y, rfy, rfy, rfd, rfd, rfz, nfolds=3)\n",
    "    tableiiv = pd.concat([tableiiv, summary_iiv(*result, X, Z, D, y, name='random forest')])\n",
    "    \n",
    "    # decision tree\n",
    "    dtry = make_pipeline(transformer, DecisionTreeRegressor(min_samples_leaf=10, ccp_alpha=.001))\n",
    "    dtrd = make_pipeline(transformer, DecisionTreeClassifier(min_samples_leaf=10, ccp_alpha=.001))\n",
    "    dtrz = make_pipeline(transformer, DecisionTreeClassifier(min_samples_leaf=10, ccp_alpha=.001))\n",
    "    result = iiv(X, Z, D, y, dtry, dtry, dtrd, dtrd, dtrz, nfolds=3)\n",
    "    tableiiv = pd.concat([tableiiv, summary_iiv(*result, X, Z, D, y, name='decision trees')])\n",
    "    \n",
    "    # boosted trees\n",
    "    gbfy = make_pipeline(transformer, GradientBoostingRegressor(max_depth=2, n_iter_no_change=5))\n",
    "    gbfd = make_pipeline(transformer, GradientBoostingClassifier(max_depth=2, n_iter_no_change=5))\n",
    "    gbfz = make_pipeline(transformer, GradientBoostingClassifier(max_depth=2, n_iter_no_change=5))\n",
    "    result = iiv(X, Z, D, y, gbfy, gbfy, gbfd, gbfd, gbfz, nfolds=3)\n",
    "    tableiiv = pd.concat([tableiiv, summary_iiv(*result, X, Z, D, y, name='boosted trees')])\n",
    "    \n",
    "    # semi crossfitting with AutoML\n",
    "    flamly0 = make_pipeline(transformer, AutoML(time_budget=60, task='regression', early_stop=True,\n",
    "                                            eval_method='cv', n_splits=3, metric='r2', verbose=0))\n",
    "    flamly1 = make_pipeline(transformer, AutoML(time_budget=60, task='regression', early_stop=True,\n",
    "                                                eval_method='cv', n_splits=3, metric='r2', verbose=0))\n",
    "    flamld1 = make_pipeline(transformer, AutoML(time_budget=60, task='classification', early_stop=True,\n",
    "                                                eval_method='cv', n_splits=3, metric='r2', verbose=0))\n",
    "    flamlz = make_pipeline(transformer, AutoML(time_budget=60, task='classification', early_stop=True,\n",
    "                                            eval_method='cv', n_splits=3, metric='r2', verbose=0))\n",
    "    flamly0.fit(X[Z == 0], y[Z == 0])\n",
    "    besty0 = make_pipeline(transformer, clone(flamly0[-1].best_model_for_estimator(flamly0[-1].best_estimator)))\n",
    "    flamly1.fit(X[Z == 1], y[Z == 1])\n",
    "    besty1 = make_pipeline(transformer, clone(flamly1[-1].best_model_for_estimator(flamly1[-1].best_estimator)))\n",
    "    from sklearn.dummy import DummyClassifier\n",
    "    bestd0 = DummyClassifier()  # since D=0 whenever Z=0\n",
    "    flamld1.fit(X[Z == 1], D[Z == 1])\n",
    "    bestd1 = make_pipeline(transformer, clone(flamld1[-1].best_model_for_estimator(flamld1[-1].best_estimator)))\n",
    "    flamlz.fit(X, Z)\n",
    "    bestz = make_pipeline(transformer, clone(flamlz[-1].best_model_for_estimator(flamlz[-1].best_estimator)))\n",
    "    result = iiv(X, Z, D, y, besty0, besty1, bestd0, bestd1, bestz, nfolds=3)\n",
    "    tableiiv = pd.concat([tableiiv, summary_iiv(*result, X, Z, D, y, name='automl (semi-cfit)')])\n",
    "    print(f\"---------- IIV Table for {descs[i]} ----------\")\n",
    "    print(tableiiv)\n",
    "    # inference robust to weak identification\n",
    "    region = iivm_robust_inference(*result, X, Z, D, y, grid=np.linspace(0, 20000, 10000))\n",
    "    print(f\"robust region iiv: {np.min(region)} to {np.max(region)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bottom quartile of income:  \n",
    "\n",
    "The different ML models are broadly consistent for PLIV. The estimate for all models is roughly between 5.5k and 6.5k. \n",
    "The different ML models are broadly consistent for IIV apart from the decision tree, which gives an estimate of 1.3k, whilst all other models are between 6k and 6.5k.  \n",
    "\n",
    "Top quartile of income:\n",
    "The different models are broadly consistent for PLIV. The estimate for all models are around 22k - 23.5k with one outlier being the decision tree (19k).  \n",
    "The different models are broadly consistent for IIV with the exception of the decision tree. The other models are all between 22.5k and 23.5k while the decision tree is at -44k (with huge standard error).  \n",
    "\n",
    "As can be seen by the results, there is treatment effect heterogeneity across income groups, with the effect broadly estimated to be higher for the top quartile of income than the bottom quartile of income. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        estimate       stderr         lower        upper  \\\n",
      "automl (semi-cfit)  24056.451175  4971.012625  14313.266431  33799.63592   \n",
      "\n",
      "                          rmse y    rmse D    rmse Z  accuracy D  accuracy Z  \n",
      "automl (semi-cfit)  90417.164623  0.487063  0.482006    0.587334    0.601856  \n",
      "robust region pliv: 14309.430943094309 to 33807.38073807381\n",
      "                    estimate       stderr         lower        upper  \\\n",
      "stacking (IIV)  24530.572484  4903.588217  14919.539579  34141.60539   \n",
      "\n",
      "                      rmse y    rmse D    rmse Z  accuracy D  accuracy Z  \n",
      "stacking (IIV)  90754.126429  0.315845  0.482029    0.868899    0.607906  \n",
      "robust region iiv: 14921.49214921492 to 34147.414741474146\n"
     ]
    }
   ],
   "source": [
    "# b.) semi cross-fitting with stacking for PLIV and IIV\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def dml_pliv_stacking(\n",
    "    X, Z, D, y,\n",
    "    modely_list,    # list of base regressors for outcome\n",
    "    modeld_list,    # list of base regressors/classifiers for treatment\n",
    "    modelz_list,    # list of base regressors/classifiers for instrument\n",
    "    stacker_y=LinearRegression(),\n",
    "    stacker_d=LinearRegression(),\n",
    "    stacker_z=LinearRegression(),\n",
    "    nfolds=3,\n",
    "    classifier_d=False,\n",
    "    classifier_z=False,\n",
    "    random_state=123\n",
    "):\n",
    "    \"\"\"\n",
    "    Semi-cross-fitting with stacking for the Partially Linear IV Model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : pd.DataFrame or np.ndarray\n",
    "        Covariates.\n",
    "    Z : array-like\n",
    "        Instrument (binary or continuous).\n",
    "    D : array-like\n",
    "        Treatment (binary or continuous).\n",
    "    y : array-like\n",
    "        Outcome.\n",
    "    modely_list : list\n",
    "        List of base learners for outcome Y.\n",
    "    modeld_list : list\n",
    "        List of base learners for treatment D.\n",
    "    modelz_list : list\n",
    "        List of base learners for instrument Z.\n",
    "    stacker_y : estimator\n",
    "        Final stacker for combining outcome base learners.\n",
    "    stacker_d : estimator\n",
    "        Final stacker for combining treatment base learners.\n",
    "    stacker_z : estimator\n",
    "        Final stacker for combining instrument base learners.\n",
    "    nfolds : int\n",
    "        Number of cross-fitting folds.\n",
    "    classifier_d : bool\n",
    "        If True, use predict_proba(..., )[:,1] for each base learner in D.\n",
    "    classifier_z : bool\n",
    "        If True, use predict_proba(..., )[:,1] for each base learner in Z.\n",
    "    random_state : int\n",
    "        Random state for reproducible CV.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    point : float\n",
    "        PLIV estimate of the treatment effect.\n",
    "    stderr : float\n",
    "        Standard error of the estimate.\n",
    "    yhat : np.ndarray\n",
    "        Final stacked predictions for the outcome Y.\n",
    "    Dhat : np.ndarray\n",
    "        Final stacked predictions for the treatment D.\n",
    "    Zhat : np.ndarray\n",
    "        Final stacked predictions for the instrument Z.\n",
    "    resy : np.ndarray\n",
    "        Residuals Y - Yhat.\n",
    "    resD : np.ndarray\n",
    "        Residuals D - Dhat.\n",
    "    resZ : np.ndarray\n",
    "        Residuals Z - Zhat.\n",
    "    epsilon : np.ndarray\n",
    "        Final residual (resy - theta * resD).\n",
    "    \"\"\"\n",
    "\n",
    "    cv = KFold(n_splits=nfolds, shuffle=True, random_state=random_state)\n",
    "\n",
    "    # ----- 1) Build out-of-fold predictions for Y using each base model ------\n",
    "    yhats_base = np.zeros((X.shape[0], len(modely_list)))\n",
    "    for j, base_model in enumerate(modely_list):\n",
    "        yhats_base[:, j] = cross_val_predict(base_model, X, y, cv=cv, n_jobs=-1)\n",
    "    # Fit the stacker on these base predictions\n",
    "    stacker_y.fit(yhats_base, y)\n",
    "    # Final stacked prediction for Y\n",
    "    yhat = stacker_y.predict(yhats_base)\n",
    "\n",
    "    # ----- 2) Build out-of-fold predictions for D using each base model ------\n",
    "    Dhats_base = np.zeros((X.shape[0], len(modeld_list)))\n",
    "    for j, base_model in enumerate(modeld_list):\n",
    "        if classifier_d:\n",
    "            # predicted probability\n",
    "            Dhats_base[:, j] = cross_val_predict(base_model, X, D, cv=cv,\n",
    "                                                 method='predict_proba', n_jobs=-1)[:, 1]\n",
    "        else:\n",
    "            Dhats_base[:, j] = cross_val_predict(base_model, X, D, cv=cv, n_jobs=-1)\n",
    "    # Fit the stacker on these base predictions\n",
    "    stacker_d.fit(Dhats_base, D)\n",
    "    # Final stacked prediction for D\n",
    "    Dhat = stacker_d.predict(Dhats_base)\n",
    "\n",
    "    # ----- 3) Build out-of-fold predictions for Z using each base model ------\n",
    "    Zhats_base = np.zeros((X.shape[0], len(modelz_list)))\n",
    "    for j, base_model in enumerate(modelz_list):\n",
    "        if classifier_z:\n",
    "            Zhats_base[:, j] = cross_val_predict(base_model, X, Z, cv=cv,\n",
    "                                                 method='predict_proba', n_jobs=-1)[:, 1]\n",
    "        else:\n",
    "            Zhats_base[:, j] = cross_val_predict(base_model, X, Z, cv=cv, n_jobs=-1)\n",
    "    # Fit the stacker on these base predictions\n",
    "    stacker_z.fit(Zhats_base, Z)\n",
    "    # Final stacked prediction for Z\n",
    "    Zhat = stacker_z.predict(Zhats_base)\n",
    "\n",
    "    # ----- 4) Residuals and final partial linear IV formula ------\n",
    "    resy = y - yhat\n",
    "    resD = D - Dhat\n",
    "    resZ = Z - Zhat\n",
    "    point = np.mean(resy * resZ) / np.mean(resD * resZ)\n",
    "    epsilon = resy - point * resD\n",
    "\n",
    "    # ----- 5) Variance, standard error ------\n",
    "    var = np.mean(epsilon**2 * (resZ**2)) / (np.mean(resD * resZ)**2)\n",
    "    stderr = np.sqrt(var / X.shape[0])\n",
    "\n",
    "    return point, stderr, yhat, Dhat, Zhat, resy, resD, resZ, epsilon\n",
    "\n",
    "\n",
    "def iiv_stacking(\n",
    "    X, Z, D, y,\n",
    "    modely0_list, modely1_list,   # base models for Y|Z=0 and Y|Z=1\n",
    "    modeld0_list, modeld1_list,   # base models for D|Z=0 and D|Z=1\n",
    "    modelz_list,                  # base models for instrument Z\n",
    "    stacker_y0=LinearRegression(),\n",
    "    stacker_y1=LinearRegression(),\n",
    "    stacker_d0=LinearRegression(),\n",
    "    stacker_d1=LinearRegression(),\n",
    "    stacker_z=LinearRegression(),\n",
    "    trimming=0.01,\n",
    "    nfolds=3,\n",
    "    random_state=123\n",
    "):\n",
    "    \"\"\"\n",
    "    Semi-cross-fitting with stacking for the Interactive IV (IIV) Model.\n",
    "\n",
    "    We fit separate models in the Z=0 and Z=1 subsamples for Y and D. \n",
    "    Then we stack their out-of-fold predictions. \n",
    "    After that, we do cross_val_predict for the instrument Z with a separate stacker.\n",
    "\n",
    "    The final doubly robust (DR) or interactive formula is used to compute the LATE.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    point : float\n",
    "        The LATE (or interactive IV) estimate.\n",
    "    stderr : float\n",
    "        Standard error of the estimate.\n",
    "    yhat : np.ndarray\n",
    "        Combined Y-hat across Z=0 and Z=1.\n",
    "    Dhat : np.ndarray\n",
    "        Combined D-hat across Z=0 and Z=1.\n",
    "    Zhat : np.ndarray\n",
    "        Predicted instrument from the stacked model.\n",
    "    resy : np.ndarray\n",
    "        Y - yhat.\n",
    "    resD : np.ndarray\n",
    "        D - Dhat.\n",
    "    resZ : np.ndarray\n",
    "        Z - Zhat.\n",
    "    drZ : np.ndarray\n",
    "        The doubly robust transformation for Y.\n",
    "    drD : np.ndarray\n",
    "        The doubly robust transformation for D.\n",
    "    \"\"\"\n",
    "    n = X.shape[0]\n",
    "    cv = KFold(n_splits=nfolds, shuffle=True, random_state=random_state)\n",
    "\n",
    "    # We'll store base predictions for y0, y1, d0, d1:\n",
    "    y0_base_preds = np.zeros((n, len(modely0_list)))\n",
    "    y1_base_preds = np.zeros((n, len(modely1_list)))\n",
    "    d0_base_preds = np.zeros((n, len(modeld0_list)))\n",
    "    d1_base_preds = np.zeros((n, len(modeld1_list)))\n",
    "\n",
    "    # We will fill them by explicit loop over folds:\n",
    "    # This approach is consistent with iiv code that\n",
    "    # only trains model on sub-sample with Z=0 or Z=1 in train fold, \n",
    "    # and then predicts on the entire test fold.\n",
    "    for train_index, test_index in cv.split(X, y):\n",
    "\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        Z_train, Z_test = Z[train_index], Z[test_index]\n",
    "        D_train, D_test = D[train_index], D[test_index]\n",
    "\n",
    "        # 1) Y0 sub-sample (Z=0 in training)\n",
    "        X_train_z0 = X_train[Z_train == 0]\n",
    "        y_train_z0 = y_train[Z_train == 0]\n",
    "        # fit each base learner\n",
    "        for j, base_model in enumerate(modely0_list):\n",
    "            mdl = base_model.fit(X_train_z0, y_train_z0)\n",
    "            # predict on the test fold (full test fold, not only Z=0 in test)\n",
    "            y0_base_preds[test_index, j] = mdl.predict(X_test)\n",
    "\n",
    "        # 2) Y1 sub-sample (Z=1 in training)\n",
    "        X_train_z1 = X_train[Z_train == 1]\n",
    "        y_train_z1 = y_train[Z_train == 1]\n",
    "        for j, base_model in enumerate(modely1_list):\n",
    "            mdl = base_model.fit(X_train_z1, y_train_z1)\n",
    "            y1_base_preds[test_index, j] = mdl.predict(X_test)\n",
    "\n",
    "        # 3) D0 sub-sample (Z=0 in training)\n",
    "        #    watch out for corner case: if D=0 always for Z=0, or similarly if no variation\n",
    "        if np.mean(D_train_z0 := D_train[Z_train == 0]) in (0,1):\n",
    "            # no variation in D for Z=0\n",
    "            d0_base_preds[test_index, :] = np.mean(D_train_z0)\n",
    "        else:\n",
    "            X_train_z0D = X_train[Z_train == 0]\n",
    "            for j, base_model in enumerate(modeld0_list):\n",
    "                mdl = base_model.fit(X_train_z0D, D_train_z0)\n",
    "                # treat D as a classification => predict_proba if desired\n",
    "                if hasattr(mdl, \"predict_proba\"):\n",
    "                    d0_base_preds[test_index, j] = mdl.predict_proba(X_test)[:,1]\n",
    "                else:\n",
    "                    d0_base_preds[test_index, j] = mdl.predict(X_test)\n",
    "\n",
    "        # 4) D1 sub-sample (Z=1 in training)\n",
    "        if np.mean(D_train_z1 := D_train[Z_train == 1]) in (0,1):\n",
    "            # no variation in D for Z=1\n",
    "            d1_base_preds[test_index, :] = np.mean(D_train_z1)\n",
    "        else:\n",
    "            X_train_z1D = X_train[Z_train == 1]\n",
    "            for j, base_model in enumerate(modeld1_list):\n",
    "                mdl = base_model.fit(X_train_z1D, D_train_z1)\n",
    "                if hasattr(mdl, \"predict_proba\"):\n",
    "                    d1_base_preds[test_index, j] = mdl.predict_proba(X_test)[:,1]\n",
    "                else:\n",
    "                    d1_base_preds[test_index, j] = mdl.predict(X_test)\n",
    "\n",
    "    # Now we have out-of-fold base predictions for each sub-group.\n",
    "    # Next, we stack them using the Z=0 and Z=1 subsamples:\n",
    "\n",
    "    # y0\n",
    "    idx_z0 = (Z == 0)\n",
    "    stacker_y0.fit(y0_base_preds[idx_z0], y[idx_z0])\n",
    "    final_y0 = stacker_y0.predict(y0_base_preds)\n",
    "\n",
    "    # y1\n",
    "    idx_z1 = (Z == 1)\n",
    "    stacker_y1.fit(y1_base_preds[idx_z1], y[idx_z1])\n",
    "    final_y1 = stacker_y1.predict(y1_base_preds)\n",
    "\n",
    "    # d0\n",
    "    if np.var(D[idx_z0]) > 1e-10:\n",
    "        stacker_d0.fit(d0_base_preds[idx_z0], D[idx_z0])\n",
    "        final_d0 = stacker_d0.predict(d0_base_preds)\n",
    "    else:\n",
    "        # no variation => always 0 or always 1\n",
    "        final_d0 = np.mean(D[idx_z0]) * np.ones_like(D)\n",
    "    # d1\n",
    "    if np.var(D[idx_z1]) > 1e-10:\n",
    "        stacker_d1.fit(d1_base_preds[idx_z1], D[idx_z1])\n",
    "        final_d1 = stacker_d1.predict(d1_base_preds)\n",
    "    else:\n",
    "        final_d1 = np.mean(D[idx_z1]) * np.ones_like(D)\n",
    "\n",
    "    # Combine them into final predictions for each individual's observed Z\n",
    "    yhat = final_y0*(1 - Z) + final_y1*Z\n",
    "    Dhat = final_d0*(1 - Z) + final_d1*Z\n",
    "\n",
    "    # Now handle the instrument Z with stacking\n",
    "    # cross_val_predict to get base predictions for Z\n",
    "    cv = KFold(n_splits=nfolds, shuffle=True, random_state=random_state)\n",
    "    z_base_preds = np.zeros((n, len(modelz_list)))\n",
    "    for j, base_model in enumerate(modelz_list):\n",
    "        # if Z is classification => use predict_proba\n",
    "        if set(Z) == {0,1} and hasattr(base_model, \"predict_proba\"):\n",
    "            z_base_preds[:, j] = cross_val_predict(\n",
    "                base_model, X, Z, cv=cv, method='predict_proba', n_jobs=-1\n",
    "            )[:,1]\n",
    "        else:\n",
    "            z_base_preds[:, j] = cross_val_predict(base_model, X, Z, cv=cv, n_jobs=-1)\n",
    "    # Fit the stacker\n",
    "    stacker_z.fit(z_base_preds, Z)\n",
    "    Zhat = stacker_z.predict(z_base_preds)\n",
    "\n",
    "    # clip the propensities\n",
    "    Zhat = np.clip(Zhat, trimming, 1 - trimming)\n",
    "\n",
    "    # ----- DR transformations -----\n",
    "    HZ = (Z / Zhat) - ((1 - Z) / (1 - Zhat))\n",
    "    drZ = final_y1 - final_y0 + (y - yhat)*HZ\n",
    "    drD = final_d1 - final_d0 + (D - Dhat)*HZ\n",
    "    point = np.mean(drZ) / np.mean(drD)\n",
    "    psi = drZ - point*drD\n",
    "    Jhat = np.mean(drD)\n",
    "    var = np.mean(psi**2) / (Jhat**2)\n",
    "    stderr = np.sqrt(var / n)\n",
    "\n",
    "    resy = y - yhat\n",
    "    resD = D - Dhat\n",
    "    resZ = Z - Zhat\n",
    "\n",
    "    return point, stderr, yhat, Dhat, Zhat, resy, resD, resZ, drZ, drD\n",
    "\n",
    "\n",
    "lasso_y = make_pipeline(transformer, StandardScaler(), LassoCV(cv=5))\n",
    "rf_y    = make_pipeline(transformer, RandomForestRegressor(\n",
    "               n_estimators=100, min_samples_leaf=10, ccp_alpha=.001))\n",
    "gbf_y   = make_pipeline(transformer, GradientBoostingRegressor(\n",
    "               max_depth=2, n_iter_no_change=5))\n",
    "modely_list = [lasso_y, rf_y, gbf_y]\n",
    "\n",
    "# For treatment D (binary => classifier)\n",
    "lgr_d = make_pipeline(transformer, StandardScaler(), LogisticRegressionCV(cv=5))\n",
    "rf_d  = make_pipeline(transformer, RandomForestClassifier(\n",
    "               n_estimators=100, min_samples_leaf=10, ccp_alpha=.001))\n",
    "gbf_d = make_pipeline(transformer, GradientBoostingClassifier(\n",
    "               max_depth=2, n_iter_no_change=5))\n",
    "modeld_list = [lgr_d, rf_d, gbf_d]\n",
    "\n",
    "# For instrument Z (binary => classifier)\n",
    "lgr_z = make_pipeline(transformer, StandardScaler(), LogisticRegressionCV(cv=5))\n",
    "rf_z  = make_pipeline(transformer, RandomForestClassifier(\n",
    "               n_estimators=100, min_samples_leaf=10, ccp_alpha=.001))\n",
    "gbf_z = make_pipeline(transformer, GradientBoostingClassifier(\n",
    "               max_depth=2, n_iter_no_change=5))\n",
    "modelz_list = [lgr_z, rf_z, gbf_z]\n",
    "\n",
    "# 2) Run PLIV with stacking\n",
    "pliv_res = dml_pliv_stacking(\n",
    "    X, Z, D, y,\n",
    "    modely_list=modely_list,\n",
    "    modeld_list=modeld_list,\n",
    "    modelz_list=modelz_list,\n",
    "    stacker_y=LinearRegression(), \n",
    "    stacker_d=LinearRegression(),\n",
    "    stacker_z=LinearRegression(),\n",
    "    nfolds=3,\n",
    "    classifier_d=True,\n",
    "    classifier_z=True\n",
    ")\n",
    "\n",
    "result = pliv_res\n",
    "\n",
    "# 3) Summarize:\n",
    "table_stack = summary(*result, X, Z, D, y, name='automl (semi-cfit)')\n",
    "print(table_stack)\n",
    "\n",
    "def robust_inference(point, stderr, yhat, Dhat, Zhat, resy, resD, resZ, epsilon, X, Z, D, y, *, grid, alpha=0.05):\n",
    "    '''\n",
    "    Inference in the partially linear IV model that is robust to weak identification.\n",
    "    grid: grid of theta values to search over when trying to identify the confidence region\n",
    "    alpha: confidence level\n",
    "    '''\n",
    "    n = X.shape[0]\n",
    "    thr = scipy.stats.chi2.ppf(1 - alpha, df=1)\n",
    "    accept = []\n",
    "    for theta in grid:\n",
    "        moment = (resy - theta * resD) * resZ\n",
    "        test = n * np.mean(moment)**2 / np.var(moment)\n",
    "        if test <= thr:\n",
    "            accept.append(theta)\n",
    "    return accept\n",
    "\n",
    "region = robust_inference(*result, X, Z, D, y, grid=np.linspace(0, 40000, 10000))\n",
    "\n",
    "print(f\"robust region pliv: {np.min(region)} to {np.max(region)}\")\n",
    "\n",
    "modely0_list = [clone(m) for m in modely_list]\n",
    "modely1_list = [clone(m) for m in modely_list]\n",
    "modeld0_list = [clone(m) for m in modeld_list]\n",
    "modeld1_list = [clone(m) for m in modeld_list]\n",
    "modelz_list  = [clone(m) for m in modelz_list]  # same instrument models\n",
    "\n",
    "iiv_res = iiv_stacking(\n",
    "    X, Z, D, y,\n",
    "    modely0_list, modely1_list,\n",
    "    modeld0_list, modeld1_list,\n",
    "    modelz_list,\n",
    "    stacker_y0=LinearRegression(),\n",
    "    stacker_y1=LinearRegression(),\n",
    "    stacker_d0=LinearRegression(),\n",
    "    stacker_d1=LinearRegression(),\n",
    "    stacker_z=LinearRegression(),\n",
    "    trimming=0.01,\n",
    "    nfolds=3\n",
    ")\n",
    "\n",
    "result = iiv_res\n",
    "\n",
    "table_iiv = summary_iiv(*result,X, Z, D, y, name='stacking (IIV)')\n",
    "print(table_iiv)\n",
    "    \n",
    "def iivm_robust_inference(point, stderr, yhat, Dhat, Zhat, resy, resD, resZ, drZ, drD, X, Z, D, y, *,\n",
    "                          grid, alpha=0.05):\n",
    "    '''\n",
    "    Inference in the partially linear IV model that is robust to weak identification.\n",
    "    grid: grid of theta values to search over when trying to identify the confidence region\n",
    "    alpha: confidence level\n",
    "    '''\n",
    "    n = X.shape[0]\n",
    "    thr = scipy.stats.chi2.ppf(1 - alpha, df=1)\n",
    "    accept = []\n",
    "    for theta in grid:\n",
    "        moment = drZ - theta * drD\n",
    "        test = n * np.mean(moment)**2 / np.var(moment)\n",
    "        if test <= thr:\n",
    "            accept.append(theta)\n",
    "    return accept\n",
    "\n",
    "region = iivm_robust_inference(*result, X, Z, D, y, grid=np.linspace(0, 40000, 10000))\n",
    "\n",
    "print(f\"robust region iiv: {np.min(region)} to {np.max(region)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs288_alt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
